/home/jw4199/miniconda3/envs/ouro_vllm/lib/python3.10/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
2026-02-15 04:53:22,898 - INFO - ============================================================
2026-02-15 04:53:22,898 - INFO - GRPO Training with verl + vLLM Acceleration
2026-02-15 04:53:22,898 - INFO - ============================================================
2026-02-15 04:53:22,898 - INFO - 
2026-02-15 04:53:22,898 - INFO - *** TRAINING MODE: FULL PARAMETER FINE-TUNING ***
2026-02-15 04:53:22,898 - INFO - *** All model parameters will be updated during training ***
2026-02-15 04:53:22,898 - INFO - 
2026-02-15 04:53:22,898 - INFO - Model: /scratch/gpfs/OLGARUS/jw4199/model_weights_path/Ouro-2.6B-Thinking
2026-02-15 04:53:22,898 - INFO - Output: ./grpo_output/4753492
2026-02-15 04:53:22,898 - INFO - Beta (KL coef): 0.001
2026-02-15 04:53:22,898 - INFO - Num generations: 8
2026-02-15 04:53:22,898 - INFO - Num prompts per batch: 32
2026-02-15 04:53:22,898 - INFO - Gradient accumulation steps: 2
2026-02-15 04:53:22,898 - INFO - Effective prompts per update: 64 (32 x 2)
2026-02-15 04:53:22,898 - INFO - Learning rate: 1e-06
2026-02-15 04:53:22,898 - INFO - Temperature: 1.0
2026-02-15 04:53:22,898 - INFO - Max prompt length: 1024
2026-02-15 04:53:22,898 - INFO - Max completion length: 2048
2026-02-15 04:53:22,898 - INFO - PPO max token len: 16384
2026-02-15 04:53:22,898 - INFO - Log-prob max token len: 16384
2026-02-15 04:53:22,898 - INFO - Epochs: 1
2026-02-15 04:53:22,898 - INFO - vLLM enabled: True
2026-02-15 04:53:22,898 - INFO - SFT checkpoint loading disabled - starting from fresh model weights
2026-02-15 04:53:22,902 - INFO - Saved configuration to: ./grpo_output/4753492/config.json
2026-02-15 04:53:22,902 - INFO - Using verl for 4-GPU training
/home/jw4199/miniconda3/envs/ouro_vllm/lib/python3.10/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
2026-02-15 04:53:44,111 - INFO - Using custom GRPOActorRolloutRefWorker with parameter logging
2026-02-15 04:53:44,263 - INFO - Setting up verl training...
2026-02-15 04:53:44,263 - INFO - ============================================================
2026-02-15 04:53:44,263 - INFO - FULL PARAMETER FINE-TUNING MODE
2026-02-15 04:53:44,263 - INFO - All model parameters will be trained (no LoRA adapters)
2026-02-15 04:53:44,263 - INFO - ============================================================
2026-02-15 04:53:44,377 - INFO - Loading JSONL from /scratch/gpfs/OLGARUS/jw4199/datasets/MATH/math_train.jsonl
2026-02-15 04:53:44,454 - INFO - Loaded 7500 examples
2026-02-15 04:53:44,534 - INFO - Saved 7500 examples to /scratch/gpfs/OLGARUS/jw4199/distillation/LLOPSD/grpo_experiments/grpo_output/4753492/data/train.parquet
2026-02-15 04:53:44,537 - INFO - LR Schedule:
2026-02-15 04:53:44,537 - INFO -   Scheduler type: constant (with warmup)
2026-02-15 04:53:44,537 - INFO -   Total steps (batches): 3
2026-02-15 04:53:44,538 - INFO -   Warmup steps (batches): 0 (10%)
2026-02-15 04:53:44,538 - INFO -   Prompts per batch: 32
2026-02-15 04:53:44,538 - INFO - Saved verl config to: ./grpo_output/4753492/verl_config.json
2026-02-15 04:53:44,538 - INFO - Starting verl training with custom GRPOActorRolloutRefWorker...
2026-02-15 04:53:44,545 - INFO - Set GRPO_OUTPUT_DIR=/scratch/gpfs/OLGARUS/jw4199/distillation/LLOPSD/grpo_experiments/grpo_output/4753492
[2026-02-15 04:53:44,612 W 1978456 1978456] network_util.cc:296: Failed to determine local IP via external connectivity to: 8.8.8.8:53, [2001:4860:4860::8888]:53, falling back to hostname resolution
2026-02-15 04:53:50,797	INFO worker.py:2014 -- Started a local Ray instance. View the dashboard at [1m[32m127.0.0.1:8266 [39m[22m
/home/jw4199/miniconda3/envs/ouro_vllm/lib/python3.10/site-packages/ray/_private/worker.py:2062: FutureWarning: Tip: In future versions of Ray, Ray will no longer override accelerator visible devices env var if num_gpus=0 or num_gpus=None (default). To enable this behavior and turn off this error message, set RAY_ACCEL_ENV_VAR_OVERRIDE_ON_ZERO=0
  warnings.warn(
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 7500 examples [00:00, 507973.58 examples/s]
Filtering prompts longer than 1024 tokens (num_proc=16):   0%|          | 0/7500 [00:00<?, ? examples/s]Filtering prompts longer than 1024 tokens (num_proc=16):   6%|â–‹         | 469/7500 [00:00<00:06, 1017.74 examples/s]Filtering prompts longer than 1024 tokens (num_proc=16):  19%|â–ˆâ–‰        | 1407/7500 [00:00<00:03, 1894.40 examples/s]Filtering prompts longer than 1024 tokens (num_proc=16):  25%|â–ˆâ–ˆâ–Œ       | 1876/7500 [00:00<00:02, 2385.05 examples/s]Filtering prompts longer than 1024 tokens (num_proc=16):  31%|â–ˆâ–ˆâ–ˆâ–      | 2345/7500 [00:01<00:01, 2825.79 examples/s]Filtering prompts longer than 1024 tokens (num_proc=16):  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 3282/7500 [00:01<00:01, 2930.86 examples/s]Filtering prompts longer than 1024 tokens (num_proc=16):  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 5158/7500 [00:01<00:00, 5332.73 examples/s]Filtering prompts longer than 1024 tokens (num_proc=16):  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 7032/7500 [00:01<00:00, 7820.74 examples/s]Filtering prompts longer than 1024 tokens (num_proc=16): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7500/7500 [00:01<00:00, 4270.88 examples/s]
Parameter 'function'=<function RLHFDataset.maybe_filter_out_long_prompts.<locals>.<lambda> at 0x148f024423b0> of the transform datasets.arrow_dataset.Dataset.filter@2.0.1 couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only shown once. Subsequent hashing failures won't be shown.
2026-02-15 04:53:58,382 - WARNING - Parameter 'function'=<function RLHFDataset.maybe_filter_out_long_prompts.<locals>.<lambda> at 0x148f024423b0> of the transform datasets.arrow_dataset.Dataset.filter@2.0.1 couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only shown once. Subsequent hashing failures won't be shown.
Filtering prompts longer than 1024 tokens (num_proc=16):   0%|          | 0/7500 [00:00<?, ? examples/s]Filtering prompts longer than 1024 tokens (num_proc=16):   6%|â–‹         | 469/7500 [00:00<00:07, 908.90 examples/s]Filtering prompts longer than 1024 tokens (num_proc=16):  19%|â–ˆâ–‰        | 1407/7500 [00:00<00:02, 2413.02 examples/s]Filtering prompts longer than 1024 tokens (num_proc=16):  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 3283/7500 [00:00<00:00, 4616.42 examples/s]Filtering prompts longer than 1024 tokens (num_proc=16):  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4221/7500 [00:01<00:00, 4927.67 examples/s]Filtering prompts longer than 1024 tokens (num_proc=16):  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 5158/7500 [00:01<00:00, 5256.38 examples/s]Filtering prompts longer than 1024 tokens (num_proc=16):  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 7032/7500 [00:01<00:00, 7760.73 examples/s]Filtering prompts longer than 1024 tokens (num_proc=16): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7500/7500 [00:01<00:00, 4859.58 examples/s]
[36m(pid=1983552)[0m /home/jw4199/miniconda3/envs/ouro_vllm/lib/python3.10/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
[36m(pid=1983552)[0m   warnings.warn(
[36m(pid=1983555)[0m /home/jw4199/miniconda3/envs/ouro_vllm/lib/python3.10/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
[36m(pid=1983555)[0m   warnings.warn(
[36m(pid=1983554)[0m /home/jw4199/miniconda3/envs/ouro_vllm/lib/python3.10/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
[36m(pid=1983554)[0m   warnings.warn(
[36m(pid=1983553)[0m /home/jw4199/miniconda3/envs/ouro_vllm/lib/python3.10/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
[36m(pid=1983553)[0m   warnings.warn(
[36m(WorkerDict pid=1983552)[0m /home/jw4199/miniconda3/envs/ouro_vllm/lib/python3.10/site-packages/verl/utils/tokenizer.py:30: UserWarning: tokenizer.pad_token_id is None. Now set to 0
[36m(WorkerDict pid=1983552)[0m   warnings.warn(f"tokenizer.pad_token_id is None. Now set to {tokenizer.eos_token_id}", stacklevel=1)
[36m(WorkerDict pid=1983555)[0m /home/jw4199/miniconda3/envs/ouro_vllm/lib/python3.10/site-packages/verl/utils/tokenizer.py:30: UserWarning: tokenizer.pad_token_id is None. Now set to 0
[36m(WorkerDict pid=1983555)[0m   warnings.warn(f"tokenizer.pad_token_id is None. Now set to {tokenizer.eos_token_id}", stacklevel=1)
[36m(WorkerDict pid=1983554)[0m /home/jw4199/miniconda3/envs/ouro_vllm/lib/python3.10/site-packages/verl/utils/tokenizer.py:30: UserWarning: tokenizer.pad_token_id is None. Now set to 0
[36m(WorkerDict pid=1983554)[0m   warnings.warn(f"tokenizer.pad_token_id is None. Now set to {tokenizer.eos_token_id}", stacklevel=1)
[36m(WorkerDict pid=1983553)[0m /home/jw4199/miniconda3/envs/ouro_vllm/lib/python3.10/site-packages/verl/utils/tokenizer.py:30: UserWarning: tokenizer.pad_token_id is None. Now set to 0
[36m(WorkerDict pid=1983553)[0m   warnings.warn(f"tokenizer.pad_token_id is None. Now set to {tokenizer.eos_token_id}", stacklevel=1)
[36m(WorkerDict pid=1983552)[0m `torch_dtype` is deprecated! Use `dtype` instead!
[36m(WorkerDict pid=1983552)[0m Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in OuroForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", dtype=torch.float16)`
[36m(WorkerDict pid=1983555)[0m `torch_dtype` is deprecated! Use `dtype` instead!
[36m(WorkerDict pid=1983555)[0m Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in OuroForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", dtype=torch.float16)`
[36m(WorkerDict pid=1983554)[0m `torch_dtype` is deprecated! Use `dtype` instead!
[36m(WorkerDict pid=1983554)[0m Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in OuroForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", dtype=torch.float16)`
[36m(WorkerDict pid=1983553)[0m `torch_dtype` is deprecated! Use `dtype` instead!
[36m(WorkerDict pid=1983553)[0m Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in OuroForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", dtype=torch.float16)`
[36m(WorkerDict pid=1983552)[0m Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in OuroModel is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", dtype=torch.float16)`
[36m(WorkerDict pid=1983555)[0m Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in OuroModel is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", dtype=torch.float16)`
[36m(WorkerDict pid=1983554)[0m Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in OuroModel is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", dtype=torch.float16)`
[36m(WorkerDict pid=1983553)[0m Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in OuroModel is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", dtype=torch.float16)`
[36m(WorkerDict pid=1983554)[0m /home/jw4199/miniconda3/envs/ouro_vllm/lib/python3.10/site-packages/verl/utils/tokenizer.py:30: UserWarning: tokenizer.pad_token_id is None. Now set to 0
[36m(WorkerDict pid=1983554)[0m   warnings.warn(f"tokenizer.pad_token_id is None. Now set to {tokenizer.eos_token_id}", stacklevel=1)
[36m(WorkerDict pid=1983553)[0m /home/jw4199/miniconda3/envs/ouro_vllm/lib/python3.10/site-packages/verl/utils/tokenizer.py:30: UserWarning: tokenizer.pad_token_id is None. Now set to 0
[36m(WorkerDict pid=1983553)[0m   warnings.warn(f"tokenizer.pad_token_id is None. Now set to {tokenizer.eos_token_id}", stacklevel=1)
[36m(WorkerDict pid=1983552)[0m /home/jw4199/miniconda3/envs/ouro_vllm/lib/python3.10/site-packages/verl/utils/tokenizer.py:30: UserWarning: tokenizer.pad_token_id is None. Now set to 0
[36m(WorkerDict pid=1983552)[0m   warnings.warn(f"tokenizer.pad_token_id is None. Now set to {tokenizer.eos_token_id}", stacklevel=1)
[36m(WorkerDict pid=1983555)[0m /home/jw4199/miniconda3/envs/ouro_vllm/lib/python3.10/site-packages/verl/utils/tokenizer.py:30: UserWarning: tokenizer.pad_token_id is None. Now set to 0
[36m(WorkerDict pid=1983555)[0m   warnings.warn(f"tokenizer.pad_token_id is None. Now set to {tokenizer.eos_token_id}", stacklevel=1)
[36m(WorkerDict pid=1983552)[0m /home/jw4199/miniconda3/envs/ouro_vllm/lib/python3.10/site-packages/verl/utils/tokenizer.py:30: UserWarning: tokenizer.pad_token_id is None. Now set to 0
[36m(WorkerDict pid=1983552)[0m   warnings.warn(f"tokenizer.pad_token_id is None. Now set to {tokenizer.eos_token_id}", stacklevel=1)
[36m(WorkerDict pid=1983555)[0m /home/jw4199/miniconda3/envs/ouro_vllm/lib/python3.10/site-packages/verl/utils/tokenizer.py:30: UserWarning: tokenizer.pad_token_id is None. Now set to 0
[36m(WorkerDict pid=1983555)[0m   warnings.warn(f"tokenizer.pad_token_id is None. Now set to {tokenizer.eos_token_id}", stacklevel=1)
[36m(WorkerDict pid=1983554)[0m /home/jw4199/miniconda3/envs/ouro_vllm/lib/python3.10/site-packages/verl/utils/tokenizer.py:30: UserWarning: tokenizer.pad_token_id is None. Now set to 0
[36m(WorkerDict pid=1983554)[0m   warnings.warn(f"tokenizer.pad_token_id is None. Now set to {tokenizer.eos_token_id}", stacklevel=1)
[36m(WorkerDict pid=1983553)[0m /home/jw4199/miniconda3/envs/ouro_vllm/lib/python3.10/site-packages/verl/utils/tokenizer.py:30: UserWarning: tokenizer.pad_token_id is None. Now set to 0
[36m(WorkerDict pid=1983553)[0m   warnings.warn(f"tokenizer.pad_token_id is None. Now set to {tokenizer.eos_token_id}", stacklevel=1)
[36m(WorkerDict pid=1983552)[0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[36m(WorkerDict pid=1983555)[0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[36m(WorkerDict pid=1983554)[0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[36m(WorkerDict pid=1983553)[0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[36m(WorkerDict pid=1983552)[0m Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
[36m(WorkerDict pid=1983552)[0m Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:02<00:00,  2.70s/it]
[36m(WorkerDict pid=1983552)[0m Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:02<00:00,  2.70s/it]
[36m(WorkerDict pid=1983552)[0m 
[36m(WorkerDict pid=1983555)[0m [rank3]:W0215 04:55:41.610000 1983555 site-packages/torch/_inductor/codecache.py:1021] [0/0] fx graph cache unable to load compiled graph
[36m(WorkerDict pid=1983555)[0m [rank3]:W0215 04:55:41.610000 1983555 site-packages/torch/_inductor/codecache.py:1021] [0/0] Traceback (most recent call last):
[36m(WorkerDict pid=1983555)[0m [rank3]:W0215 04:55:41.610000 1983555 site-packages/torch/_inductor/codecache.py:1021] [0/0]   File "/home/jw4199/miniconda3/envs/ouro_vllm/lib/python3.10/site-packages/torch/_inductor/codecache.py", line 1019, in iterate_over_candidates
[36m(WorkerDict pid=1983555)[0m [rank3]:W0215 04:55:41.610000 1983555 site-packages/torch/_inductor/codecache.py:1021] [0/0]     yield pickle.loads(content), content
[36m(WorkerDict pid=1983555)[0m [rank3]:W0215 04:55:41.610000 1983555 site-packages/torch/_inductor/codecache.py:1021] [0/0] EOFError: Ran out of input
[36m(WorkerDict pid=1983554)[0m [rank2]:W0215 04:55:41.613000 1983554 site-packages/torch/_inductor/codecache.py:1021] [0/0] fx graph cache unable to load compiled graph
[36m(WorkerDict pid=1983554)[0m [rank2]:W0215 04:55:41.613000 1983554 site-packages/torch/_inductor/codecache.py:1021] [0/0] Traceback (most recent call last):
[36m(WorkerDict pid=1983554)[0m [rank2]:W0215 04:55:41.613000 1983554 site-packages/torch/_inductor/codecache.py:1021] [0/0]   File "/home/jw4199/miniconda3/envs/ouro_vllm/lib/python3.10/site-packages/torch/_inductor/codecache.py", line 1019, in iterate_over_candidates
[36m(WorkerDict pid=1983554)[0m [rank2]:W0215 04:55:41.613000 1983554 site-packages/torch/_inductor/codecache.py:1021] [0/0]     yield pickle.loads(content), content
[36m(WorkerDict pid=1983554)[0m [rank2]:W0215 04:55:41.613000 1983554 site-packages/torch/_inductor/codecache.py:1021] [0/0] EOFError: Ran out of input
[36m(WorkerDict pid=1983554)[0m [rank2]:W0215 04:55:44.616000 1983554 site-packages/torch/_inductor/codecache.py:1021] [0/0] fx graph cache unable to load compiled graph
[36m(WorkerDict pid=1983554)[0m [rank2]:W0215 04:55:44.616000 1983554 site-packages/torch/_inductor/codecache.py:1021] [0/0] Traceback (most recent call last):
[36m(WorkerDict pid=1983554)[0m [rank2]:W0215 04:55:44.616000 1983554 site-packages/torch/_inductor/codecache.py:1021] [0/0]   File "/home/jw4199/miniconda3/envs/ouro_vllm/lib/python3.10/site-packages/torch/_inductor/codecache.py", line 1019, in iterate_over_candidates
[36m(WorkerDict pid=1983554)[0m [rank2]:W0215 04:55:44.616000 1983554 site-packages/torch/_inductor/codecache.py:1021] [0/0]     yield pickle.loads(content), content
[36m(WorkerDict pid=1983554)[0m [rank2]:W0215 04:55:44.616000 1983554 site-packages/torch/_inductor/codecache.py:1021] [0/0] EOFError: Ran out of input
[36m(WorkerDict pid=1983554)[0m [rank2]:W0215 04:55:45.320000 1983554 site-packages/torch/_inductor/codecache.py:1021] [0/0] fx graph cache unable to load compiled graph
[36m(WorkerDict pid=1983554)[0m [rank2]:W0215 04:55:45.320000 1983554 site-packages/torch/_inductor/codecache.py:1021] [0/0] Traceback (most recent call last):
[36m(WorkerDict pid=1983554)[0m [rank2]:W0215 04:55:45.320000 1983554 site-packages/torch/_inductor/codecache.py:1021] [0/0]   File "/home/jw4199/miniconda3/envs/ouro_vllm/lib/python3.10/site-packages/torch/_inductor/codecache.py", line 1019, in iterate_over_candidates
[36m(WorkerDict pid=1983554)[0m [rank2]:W0215 04:55:45.320000 1983554 site-packages/torch/_inductor/codecache.py:1021] [0/0]     yield pickle.loads(content), content
[36m(WorkerDict pid=1983554)[0m [rank2]:W0215 04:55:45.320000 1983554 site-packages/torch/_inductor/codecache.py:1021] [0/0] EOFError: Ran out of input
[36m(WorkerDict pid=1983554)[0m [rank2]:W0215 04:55:45.743000 1983554 site-packages/torch/_inductor/codecache.py:1021] [0/0] fx graph cache unable to load compiled graph
[36m(WorkerDict pid=1983554)[0m [rank2]:W0215 04:55:45.743000 1983554 site-packages/torch/_inductor/codecache.py:1021] [0/0] Traceback (most recent call last):
[36m(WorkerDict pid=1983554)[0m [rank2]:W0215 04:55:45.743000 1983554 site-packages/torch/_inductor/codecache.py:1021] [0/0]   File "/home/jw4199/miniconda3/envs/ouro_vllm/lib/python3.10/site-packages/torch/_inductor/codecache.py", line 1019, in iterate_over_candidates
[36m(WorkerDict pid=1983554)[0m [rank2]:W0215 04:55:45.743000 1983554 site-packages/torch/_inductor/codecache.py:1021] [0/0]     yield pickle.loads(content), content
[36m(WorkerDict pid=1983554)[0m [rank2]:W0215 04:55:45.743000 1983554 site-packages/torch/_inductor/codecache.py:1021] [0/0] EOFError: Ran out of input
[36m(WorkerDict pid=1983554)[0m [rank2]:W0215 04:55:45.830000 1983554 site-packages/torch/_inductor/codecache.py:1021] [0/0] fx graph cache unable to load compiled graph
[36m(WorkerDict pid=1983554)[0m [rank2]:W0215 04:55:45.830000 1983554 site-packages/torch/_inductor/codecache.py:1021] [0/0] Traceback (most recent call last):
[36m(WorkerDict pid=1983554)[0m [rank2]:W0215 04:55:45.830000 1983554 site-packages/torch/_inductor/codecache.py:1021] [0/0]   File "/home/jw4199/miniconda3/envs/ouro_vllm/lib/python3.10/site-packages/torch/_inductor/codecache.py", line 1019, in iterate_over_candidates
[36m(WorkerDict pid=1983554)[0m [rank2]:W0215 04:55:45.830000 1983554 site-packages/torch/_inductor/codecache.py:1021] [0/0]     yield pickle.loads(content), content
[36m(WorkerDict pid=1983554)[0m [rank2]:W0215 04:55:45.830000 1983554 site-packages/torch/_inductor/codecache.py:1021] [0/0] EOFError: Ran out of input
[36m(WorkerDict pid=1983555)[0m [rank3]:W0215 04:55:46.455000 1983555 site-packages/torch/_inductor/codecache.py:1021] [0/0] fx graph cache unable to load compiled graph
[36m(WorkerDict pid=1983555)[0m [rank3]:W0215 04:55:46.455000 1983555 site-packages/torch/_inductor/codecache.py:1021] [0/0] Traceback (most recent call last):
[36m(WorkerDict pid=1983555)[0m [rank3]:W0215 04:55:46.455000 1983555 site-packages/torch/_inductor/codecache.py:1021] [0/0]   File "/home/jw4199/miniconda3/envs/ouro_vllm/lib/python3.10/site-packages/torch/_inductor/codecache.py", line 1019, in iterate_over_candidates
[36m(WorkerDict pid=1983555)[0m [rank3]:W0215 04:55:46.455000 1983555 site-packages/torch/_inductor/codecache.py:1021] [0/0]     yield pickle.loads(content), content
[36m(WorkerDict pid=1983555)[0m [rank3]:W0215 04:55:46.455000 1983555 site-packages/torch/_inductor/codecache.py:1021] [0/0] EOFError: Ran out of input
[36m(WorkerDict pid=1983554)[0m [rank2]:W0215 04:55:49.574000 1983554 site-packages/torch/_inductor/codecache.py:1021] [0/0] fx graph cache unable to load compiled graph
[36m(WorkerDict pid=1983554)[0m [rank2]:W0215 04:55:49.574000 1983554 site-packages/torch/_inductor/codecache.py:1021] [0/0] Traceback (most recent call last):
[36m(WorkerDict pid=1983554)[0m [rank2]:W0215 04:55:49.574000 1983554 site-packages/torch/_inductor/codecache.py:1021] [0/0]   File "/home/jw4199/miniconda3/envs/ouro_vllm/lib/python3.10/site-packages/torch/_inductor/codecache.py", line 1019, in iterate_over_candidates
[36m(WorkerDict pid=1983554)[0m [rank2]:W0215 04:55:49.574000 1983554 site-packages/torch/_inductor/codecache.py:1021] [0/0]     yield pickle.loads(content), content
[36m(WorkerDict pid=1983554)[0m [rank2]:W0215 04:55:49.574000 1983554 site-packages/torch/_inductor/codecache.py:1021] [0/0] EOFError: Ran out of input
[36m(WorkerDict pid=1983554)[0m [rank2]:W0215 04:55:50.761000 1983554 site-packages/torch/_inductor/codecache.py:1021] [0/0] fx graph cache unable to load compiled graph
[36m(WorkerDict pid=1983554)[0m [rank2]:W0215 04:55:50.761000 1983554 site-packages/torch/_inductor/codecache.py:1021] [0/0] Traceback (most recent call last):
[36m(WorkerDict pid=1983554)[0m [rank2]:W0215 04:55:50.761000 1983554 site-packages/torch/_inductor/codecache.py:1021] [0/0]   File "/home/jw4199/miniconda3/envs/ouro_vllm/lib/python3.10/site-packages/torch/_inductor/codecache.py", line 1019, in iterate_over_candidates
[36m(WorkerDict pid=1983554)[0m [rank2]:W0215 04:55:50.761000 1983554 site-packages/torch/_inductor/codecache.py:1021] [0/0]     yield pickle.loads(content), content
[36m(WorkerDict pid=1983554)[0m [rank2]:W0215 04:55:50.761000 1983554 site-packages/torch/_inductor/codecache.py:1021] [0/0] EOFError: Ran out of input
[36m(WorkerDict pid=1983555)[0m [rank3]:W0215 04:55:51.751000 1983555 site-packages/torch/_inductor/codecache.py:1021] [0/0] fx graph cache unable to load compiled graph
[36m(WorkerDict pid=1983555)[0m [rank3]:W0215 04:55:51.751000 1983555 site-packages/torch/_inductor/codecache.py:1021] [0/0] Traceback (most recent call last):
[36m(WorkerDict pid=1983555)[0m [rank3]:W0215 04:55:51.751000 1983555 site-packages/torch/_inductor/codecache.py:1021] [0/0]   File "/home/jw4199/miniconda3/envs/ouro_vllm/lib/python3.10/site-packages/torch/_inductor/codecache.py", line 1019, in iterate_over_candidates
[36m(WorkerDict pid=1983555)[0m [rank3]:W0215 04:55:51.751000 1983555 site-packages/torch/_inductor/codecache.py:1021] [0/0]     yield pickle.loads(content), content
[36m(WorkerDict pid=1983555)[0m [rank3]:W0215 04:55:51.751000 1983555 site-packages/torch/_inductor/codecache.py:1021] [0/0] EOFError: Ran out of input
[36m(WorkerDict pid=1983552)[0m [rank0]:W0215 04:55:51.933000 1983552 site-packages/torch/_inductor/codecache.py:1021] [0/0] fx graph cache unable to load compiled graph
[36m(WorkerDict pid=1983552)[0m [rank0]:W0215 04:55:51.933000 1983552 site-packages/torch/_inductor/codecache.py:1021] [0/0] Traceback (most recent call last):
[36m(WorkerDict pid=1983552)[0m [rank0]:W0215 04:55:51.933000 1983552 site-packages/torch/_inductor/codecache.py:1021] [0/0]   File "/home/jw4199/miniconda3/envs/ouro_vllm/lib/python3.10/site-packages/torch/_inductor/codecache.py", line 1019, in iterate_over_candidates
[36m(WorkerDict pid=1983552)[0m [rank0]:W0215 04:55:51.933000 1983552 site-packages/torch/_inductor/codecache.py:1021] [0/0]     yield pickle.loads(content), content
[36m(WorkerDict pid=1983552)[0m [rank0]:W0215 04:55:51.933000 1983552 site-packages/torch/_inductor/codecache.py:1021] [0/0] EOFError: Ran out of input
[36m(WorkerDict pid=1983553)[0m [rank1]:W0215 04:55:56.370000 1983553 site-packages/torch/_inductor/codecache.py:1021] [0/0] fx graph cache unable to load compiled graph
[36m(WorkerDict pid=1983553)[0m [rank1]:W0215 04:55:56.370000 1983553 site-packages/torch/_inductor/codecache.py:1021] [0/0] Traceback (most recent call last):
[36m(WorkerDict pid=1983553)[0m [rank1]:W0215 04:55:56.370000 1983553 site-packages/torch/_inductor/codecache.py:1021] [0/0]   File "/home/jw4199/miniconda3/envs/ouro_vllm/lib/python3.10/site-packages/torch/_inductor/codecache.py", line 1019, in iterate_over_candidates
[36m(WorkerDict pid=1983553)[0m [rank1]:W0215 04:55:56.370000 1983553 site-packages/torch/_inductor/codecache.py:1021] [0/0]     yield pickle.loads(content), content
[36m(WorkerDict pid=1983553)[0m [rank1]:W0215 04:55:56.370000 1983553 site-packages/torch/_inductor/codecache.py:1021] [0/0] EOFError: Ran out of input
[36m(WorkerDict pid=1983555)[0m [rank3]:W0215 04:55:58.428000 1983555 site-packages/torch/_inductor/codecache.py:1021] [0/0] fx graph cache unable to load compiled graph
[36m(WorkerDict pid=1983555)[0m [rank3]:W0215 04:55:58.428000 1983555 site-packages/torch/_inductor/codecache.py:1021] [0/0] Traceback (most recent call last):
[36m(WorkerDict pid=1983555)[0m [rank3]:W0215 04:55:58.428000 1983555 site-packages/torch/_inductor/codecache.py:1021] [0/0]   File "/home/jw4199/miniconda3/envs/ouro_vllm/lib/python3.10/site-packages/torch/_inductor/codecache.py", line 1019, in iterate_over_candidates
[36m(WorkerDict pid=1983555)[0m [rank3]:W0215 04:55:58.428000 1983555 site-packages/torch/_inductor/codecache.py:1021] [0/0]     yield pickle.loads(content), content
[36m(WorkerDict pid=1983555)[0m [rank3]:W0215 04:55:58.428000 1983555 site-packages/torch/_inductor/codecache.py:1021] [0/0] EOFError: Ran out of input
[36m(WorkerDict pid=1983552)[0m 2026-02-15 04:56:09,460 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
[36m(WorkerDict pid=1983555)[0m 2026-02-15 04:56:09,462 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
[36m(WorkerDict pid=1983554)[0m 2026-02-15 04:56:09,463 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
[36m(WorkerDict pid=1983553)[0m 2026-02-15 04:56:09,464 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
[36m(WorkerDict pid=1983552)[0m 2026-02-15 04:56:09,626 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
[36m(WorkerDict pid=1983555)[0m 2026-02-15 04:56:09,627 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
[36m(WorkerDict pid=1983554)[0m 2026-02-15 04:56:09,626 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
[36m(WorkerDict pid=1983553)[0m 2026-02-15 04:56:09,625 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
[36m(WorkerDict pid=1983552)[0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/51 [00:00<?, ?it/s]
[36m(WorkerDict pid=1983552)[0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   2%|â–         | 1/51 [00:00<00:07,  6.47it/s]
[36m(WorkerDict pid=1983552)[0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   4%|â–         | 2/51 [00:00<00:07,  6.70it/s]
[36m(WorkerDict pid=1983552)[0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|â–Œ         | 3/51 [00:00<00:07,  6.85it/s]
[36m(WorkerDict pid=1983552)[0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   8%|â–Š         | 4/51 [00:00<00:06,  6.93it/s]
[36m(WorkerDict pid=1983552)[0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  10%|â–‰         | 5/51 [00:00<00:06,  6.85it/s]
[36m(WorkerDict pid=1983552)[0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  12%|â–ˆâ–        | 6/51 [00:00<00:06,  6.77it/s]
[36m(WorkerDict pid=1983552)[0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|â–ˆâ–Ž        | 7/51 [00:01<00:06,  6.81it/s]
[36m(WorkerDict pid=1983552)[0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  16%|â–ˆâ–Œ        | 8/51 [00:01<00:06,  6.78it/s]
[36m(WorkerDict pid=1983552)[0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|â–ˆâ–Š        | 9/51 [00:01<00:06,  6.65it/s]
[36m(WorkerDict pid=1983552)[0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|â–ˆâ–‰        | 10/51 [00:01<00:06,  6.64it/s]
[36m(WorkerDict pid=1983552)[0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  22%|â–ˆâ–ˆâ–       | 11/51 [00:01<00:06,  6.64it/s]
[36m(WorkerDict pid=1983552)[0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  24%|â–ˆâ–ˆâ–Ž       | 12/51 [00:01<00:05,  6.55it/s]
[36m(WorkerDict pid=1983552)[0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|â–ˆâ–ˆâ–Œ       | 13/51 [00:01<00:05,  6.46it/s]
[36m(WorkerDict pid=1983552)[0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|â–ˆâ–ˆâ–‹       | 14/51 [00:02<00:05,  6.50it/s]
[36m(WorkerDict pid=1983552)[0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|â–ˆâ–ˆâ–‰       | 15/51 [00:02<00:05,  6.31it/s]
[36m(WorkerDict pid=1983552)[0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|â–ˆâ–ˆâ–ˆâ–      | 16/51 [00:02<00:05,  6.26it/s]
[36m(WorkerDict pid=1983552)[0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 17/51 [00:02<00:05,  6.18it/s]
[36m(WorkerDict pid=1983552)[0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 18/51 [00:02<00:05,  6.17it/s]
[36m(WorkerDict pid=1983552)[0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|â–ˆâ–ˆâ–ˆâ–‹      | 19/51 [00:02<00:05,  6.10it/s]
[36m(WorkerDict pid=1983552)[0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  39%|â–ˆâ–ˆâ–ˆâ–‰      | 20/51 [00:03<00:05,  6.16it/s]
[36m(WorkerDict pid=1983552)[0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 21/51 [00:03<00:04,  6.07it/s]
[36m(WorkerDict pid=1983552)[0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 22/51 [00:03<00:04,  5.97it/s]
[36m(WorkerDict pid=1983552)[0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 23/51 [00:03<00:05,  5.25it/s]
[36m(WorkerDict pid=1983552)[0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 24/51 [00:03<00:05,  5.37it/s]
[36m(WorkerDict pid=1983552)[0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 25/51 [00:04<00:04,  5.36it/s]
[36m(WorkerDict pid=1983552)[0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 26/51 [00:04<00:04,  5.46it/s]
[36m(WorkerDict pid=1983552)[0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 27/51 [00:04<00:04,  5.40it/s]
[36m(WorkerDict pid=1983552)[0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 28/51 [00:04<00:04,  5.47it/s]
[36m(WorkerDict pid=1983552)[0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 29/51 [00:04<00:04,  5.29it/s]
[36m(WorkerDict pid=1983552)[0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 30/51 [00:04<00:04,  5.18it/s]
[36m(WorkerDict pid=1983552)[0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 31/51 [00:05<00:03,  5.11it/s]
[36m(WorkerDict pid=1983552)[0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 32/51 [00:05<00:03,  5.11it/s]
[36m(WorkerDict pid=1983552)[0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 33/51 [00:05<00:03,  5.18it/s]
[36m(WorkerDict pid=1983552)[0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 34/51 [00:05<00:03,  5.23it/s]
[36m(WorkerDict pid=1983552)[0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 35/51 [00:05<00:03,  5.21it/s]
[36m(WorkerDict pid=1983552)[0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 36/51 [00:06<00:02,  5.16it/s]
[36m(WorkerDict pid=1983552)[0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 37/51 [00:06<00:02,  5.12it/s]
[36m(WorkerDict pid=1983552)[0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 38/51 [00:06<00:02,  5.14it/s]
[36m(WorkerDict pid=1983552)[0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 39/51 [00:06<00:02,  5.19it/s]
[36m(WorkerDict pid=1983552)[0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 40/51 [00:06<00:02,  5.21it/s]
[36m(WorkerDict pid=1983552)[0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 41/51 [00:07<00:01,  5.29it/s]
[36m(WorkerDict pid=1983552)[0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 42/51 [00:07<00:01,  5.30it/s]
[36m(WorkerDict pid=1983552)[0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 43/51 [00:07<00:01,  5.28it/s]
[36m(WorkerDict pid=1983552)[0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 44/51 [00:07<00:01,  5.26it/s]
[36m(WorkerDict pid=1983552)[0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 45/51 [00:07<00:01,  5.27it/s]
[36m(WorkerDict pid=1983552)[0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 46/51 [00:08<00:00,  5.27it/s]
[36m(WorkerDict pid=1983552)[0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 47/51 [00:08<00:00,  5.28it/s]
[36m(WorkerDict pid=1983552)[0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 48/51 [00:08<00:00,  5.28it/s]
[36m(WorkerDict pid=1983552)[0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 49/51 [00:08<00:00,  5.30it/s]
[36m(WorkerDict pid=1983552)[0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 50/51 [00:08<00:00,  5.29it/s]
[36m(WorkerDict pid=1983552)[0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 51/51 [00:09<00:00,  4.66it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 51/51 [00:09<00:00,  5.61it/s]
[36m(WorkerDict pid=1983552)[0m Capturing CUDA graphs (decode, FULL):   0%|          | 0/51 [00:00<?, ?it/s]
[36m(WorkerDict pid=1983552)[0m Capturing CUDA graphs (decode, FULL):   2%|â–         | 1/51 [00:00<00:21,  2.38it/s]
[36m(WorkerDict pid=1983552)[0m Capturing CUDA graphs (decode, FULL):   4%|â–         | 2/51 [00:00<00:12,  3.88it/s]
[36m(WorkerDict pid=1983552)[0m Capturing CUDA graphs (decode, FULL):   6%|â–Œ         | 3/51 [00:00<00:10,  4.77it/s]
[36m(WorkerDict pid=1983552)[0m Capturing CUDA graphs (decode, FULL):   8%|â–Š         | 4/51 [00:00<00:08,  5.31it/s]
[36m(WorkerDict pid=1983552)[0m Capturing CUDA graphs (decode, FULL):  10%|â–‰         | 5/51 [00:01<00:08,  5.64it/s]
[36m(WorkerDict pid=1983552)[0m Capturing CUDA graphs (decode, FULL):  12%|â–ˆâ–        | 6/51 [00:01<00:07,  5.93it/s]
[36m(WorkerDict pid=1983552)[0m Capturing CUDA graphs (decode, FULL):  14%|â–ˆâ–Ž        | 7/51 [00:01<00:07,  6.16it/s]
[36m(WorkerDict pid=1983552)[0m Capturing CUDA graphs (decode, FULL):  16%|â–ˆâ–Œ        | 8/51 [00:01<00:06,  6.31it/s]
[36m(WorkerDict pid=1983552)[0m Capturing CUDA graphs (decode, FULL):  18%|â–ˆâ–Š        | 9/51 [00:01<00:06,  6.42it/s]
[36m(WorkerDict pid=1983552)[0m Capturing CUDA graphs (decode, FULL):  20%|â–ˆâ–‰        | 10/51 [00:01<00:05,  6.83it/s]
[36m(WorkerDict pid=1983552)[0m Capturing CUDA graphs (decode, FULL):  22%|â–ˆâ–ˆâ–       | 11/51 [00:01<00:05,  7.17it/s]
[36m(WorkerDict pid=1983552)[0m Capturing CUDA graphs (decode, FULL):  24%|â–ˆâ–ˆâ–Ž       | 12/51 [00:02<00:05,  7.45it/s]
[36m(WorkerDict pid=1983552)[0m Capturing CUDA graphs (decode, FULL):  25%|â–ˆâ–ˆâ–Œ       | 13/51 [00:02<00:05,  7.45it/s]
[36m(WorkerDict pid=1983552)[0m Capturing CUDA graphs (decode, FULL):  27%|â–ˆâ–ˆâ–‹       | 14/51 [00:02<00:04,  7.58it/s]
[36m(WorkerDict pid=1983552)[0m Capturing CUDA graphs (decode, FULL):  29%|â–ˆâ–ˆâ–‰       | 15/51 [00:02<00:04,  7.76it/s]
[36m(WorkerDict pid=1983552)[0m Capturing CUDA graphs (decode, FULL):  31%|â–ˆâ–ˆâ–ˆâ–      | 16/51 [00:02<00:04,  7.74it/s]
[36m(WorkerDict pid=1983552)[0m Capturing CUDA graphs (decode, FULL):  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 17/51 [00:02<00:04,  7.69it/s]
[36m(WorkerDict pid=1983552)[0m Capturing CUDA graphs (decode, FULL):  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 18/51 [00:02<00:04,  7.80it/s]
[36m(WorkerDict pid=1983552)[0m Capturing CUDA graphs (decode, FULL):  37%|â–ˆâ–ˆâ–ˆâ–‹      | 19/51 [00:02<00:04,  7.95it/s]
[36m(WorkerDict pid=1983552)[0m Capturing CUDA graphs (decode, FULL):  39%|â–ˆâ–ˆâ–ˆâ–‰      | 20/51 [00:03<00:03,  8.08it/s]
[36m(WorkerDict pid=1983552)[0m Capturing CUDA graphs (decode, FULL):  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 21/51 [00:03<00:03,  8.19it/s]
[36m(WorkerDict pid=1983552)[0m Capturing CUDA graphs (decode, FULL):  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 22/51 [00:03<00:03,  8.20it/s]
[36m(WorkerDict pid=1983552)[0m Capturing CUDA graphs (decode, FULL):  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 23/51 [00:03<00:03,  8.18it/s]
[36m(WorkerDict pid=1983552)[0m Capturing CUDA graphs (decode, FULL):  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 24/51 [00:03<00:03,  8.29it/s]
[36m(WorkerDict pid=1983552)[0m Capturing CUDA graphs (decode, FULL):  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 25/51 [00:03<00:03,  8.34it/s]
[36m(WorkerDict pid=1983552)[0m Capturing CUDA graphs (decode, FULL):  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 26/51 [00:03<00:03,  8.31it/s]
[36m(WorkerDict pid=1983552)[0m Capturing CUDA graphs (decode, FULL):  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 27/51 [00:03<00:02,  8.38it/s]
[36m(WorkerDict pid=1983552)[0m Capturing CUDA graphs (decode, FULL):  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 28/51 [00:03<00:02,  8.42it/s]
[36m(WorkerDict pid=1983552)[0m Capturing CUDA graphs (decode, FULL):  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 29/51 [00:04<00:02,  8.48it/s]
[36m(WorkerDict pid=1983552)[0m Capturing CUDA graphs (decode, FULL):  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 30/51 [00:04<00:02,  8.41it/s]
[36m(WorkerDict pid=1983552)[0m Capturing CUDA graphs (decode, FULL):  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 31/51 [00:04<00:02,  8.37it/s]
[36m(WorkerDict pid=1983552)[0m Capturing CUDA graphs (decode, FULL):  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 32/51 [00:04<00:02,  8.39it/s]
[36m(WorkerDict pid=1983552)[0m Capturing CUDA graphs (decode, FULL):  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 33/51 [00:04<00:02,  8.32it/s]
[36m(WorkerDict pid=1983552)[0m Capturing CUDA graphs (decode, FULL):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 34/51 [00:04<00:02,  8.40it/s]
[36m(WorkerDict pid=1983552)[0m Capturing CUDA graphs (decode, FULL):  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 35/51 [00:04<00:01,  8.39it/s]
[36m(WorkerDict pid=1983552)[0m Capturing CUDA graphs (decode, FULL):  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 36/51 [00:04<00:01,  8.41it/s]
[36m(WorkerDict pid=1983552)[0m Capturing CUDA graphs (decode, FULL):  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 37/51 [00:05<00:01,  8.43it/s]
[36m(WorkerDict pid=1983552)[0m Capturing CUDA graphs (decode, FULL):  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 38/51 [00:05<00:01,  8.34it/s]
[36m(WorkerDict pid=1983552)[0m Capturing CUDA graphs (decode, FULL):  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 39/51 [00:05<00:01,  8.01it/s]
[36m(WorkerDict pid=1983552)[0m Capturing CUDA graphs (decode, FULL):  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 40/51 [00:05<00:01,  8.03it/s]
[36m(WorkerDict pid=1983552)[0m Capturing CUDA graphs (decode, FULL):  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 41/51 [00:05<00:01,  8.01it/s]
[36m(WorkerDict pid=1983552)[0m Capturing CUDA graphs (decode, FULL):  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 42/51 [00:05<00:01,  7.18it/s]
[36m(WorkerDict pid=1983552)[0m Capturing CUDA graphs (decode, FULL):  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 43/51 [00:05<00:01,  7.44it/s]
[36m(WorkerDict pid=1983552)[0m Capturing CUDA graphs (decode, FULL):  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 44/51 [00:05<00:00,  7.68it/s]
[36m(WorkerDict pid=1983552)[0m Capturing CUDA graphs (decode, FULL):  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 45/51 [00:06<00:00,  7.52it/s]
[36m(WorkerDict pid=1983552)[0m Capturing CUDA graphs (decode, FULL):  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 46/51 [00:06<00:00,  7.31it/s]
[36m(WorkerDict pid=1983552)[0m Capturing CUDA graphs (decode, FULL):  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 47/51 [00:06<00:00,  7.42it/s]
[36m(WorkerDict pid=1983552)[0m Capturing CUDA graphs (decode, FULL):  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 48/51 [00:06<00:00,  7.58it/s]
[36m(WorkerDict pid=1983552)[0m Capturing CUDA graphs (decode, FULL):  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 49/51 [00:06<00:00,  7.71it/s]
[36m(WorkerDict pid=1983552)[0m Capturing CUDA graphs (decode, FULL):  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 50/51 [00:06<00:00,  7.46it/s]
[36m(WorkerDict pid=1983552)[0m Capturing CUDA graphs (decode, FULL): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 51/51 [00:06<00:00,  7.38it/s]Capturing CUDA graphs (decode, FULL): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 51/51 [00:06<00:00,  7.38it/s]
[36m(WorkerDict pid=1983552)[0m /home/jw4199/miniconda3/envs/ouro_vllm/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:675: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
[36m(WorkerDict pid=1983552)[0m   warnings.warn(
[36m(WorkerDict pid=1983555)[0m /home/jw4199/miniconda3/envs/ouro_vllm/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:675: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
[36m(WorkerDict pid=1983555)[0m   warnings.warn(
[36m(WorkerDict pid=1983554)[0m /home/jw4199/miniconda3/envs/ouro_vllm/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:675: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
[36m(WorkerDict pid=1983554)[0m   warnings.warn(
[36m(WorkerDict pid=1983553)[0m /home/jw4199/miniconda3/envs/ouro_vllm/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:675: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
[36m(WorkerDict pid=1983553)[0m   warnings.warn(
wandb: Tracking run with wandb version 0.23.1
wandb: W&B syncing is set to `offline` in this directory. Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
wandb: Run data is saved locally in /scratch/gpfs/OLGARUS/jw4199/model_weights_path/wandb/offline-run-20260215_045633-nb6dspky
2026-02-15 04:56:36,114 - INFO - Gradient accumulation enabled: accumulating 2 rollout batches per update
Training Progress:   0%|          | 0/3 [00:00<?, ?it/s]2026-02-15 04:58:49,844 - INFO - Step 1 (accum 1/2): rollout done in 133.4s
2026-02-15 04:59:22,955 - INFO - Truncated 126/256 responses at first \boxed{}. Mean tokens dropped: 146.6
[36m(WorkerDict pid=1983552)[0m INFO:2026-02-15 04:59:26,130:[Rank 0] Saved model to /scratch/gpfs/OLGARUS/jw4199/distillation/LLOPSD/grpo_experiments/grpo_output/4753492/global_step_1/actor/model_world_size_4_rank_0.pt
[36m(WorkerDict pid=1983552)[0m INFO:2026-02-15 04:59:26,134:[Rank 0] Saved optim to /scratch/gpfs/OLGARUS/jw4199/distillation/LLOPSD/grpo_experiments/grpo_output/4753492/global_step_1/actor/optim_world_size_4_rank_0.pt
[36m(WorkerDict pid=1983552)[0m INFO:2026-02-15 04:59:26,135:[Rank 0] Saved extra_state to /scratch/gpfs/OLGARUS/jw4199/distillation/LLOPSD/grpo_experiments/grpo_output/4753492/global_step_1/actor/extra_state_world_size_4_rank_0.pt
[36m(WorkerDict pid=1983555)[0m INFO:2026-02-15 04:59:26,168:[Rank 3] Saved model to /scratch/gpfs/OLGARUS/jw4199/distillation/LLOPSD/grpo_experiments/grpo_output/4753492/global_step_1/actor/model_world_size_4_rank_3.pt
[36m(WorkerDict pid=1983555)[0m INFO:2026-02-15 04:59:26,171:[Rank 3] Saved optim to /scratch/gpfs/OLGARUS/jw4199/distillation/LLOPSD/grpo_experiments/grpo_output/4753492/global_step_1/actor/optim_world_size_4_rank_3.pt
[36m(WorkerDict pid=1983555)[0m INFO:2026-02-15 04:59:26,172:[Rank 3] Saved extra_state to /scratch/gpfs/OLGARUS/jw4199/distillation/LLOPSD/grpo_experiments/grpo_output/4753492/global_step_1/actor/extra_state_world_size_4_rank_3.pt
[36m(WorkerDict pid=1983554)[0m INFO:2026-02-15 04:59:26,152:[Rank 2] Saved model to /scratch/gpfs/OLGARUS/jw4199/distillation/LLOPSD/grpo_experiments/grpo_output/4753492/global_step_1/actor/model_world_size_4_rank_2.pt
[36m(WorkerDict pid=1983554)[0m INFO:2026-02-15 04:59:26,156:[Rank 2] Saved optim to /scratch/gpfs/OLGARUS/jw4199/distillation/LLOPSD/grpo_experiments/grpo_output/4753492/global_step_1/actor/optim_world_size_4_rank_2.pt
[36m(WorkerDict pid=1983554)[0m INFO:2026-02-15 04:59:26,159:[Rank 2] Saved extra_state to /scratch/gpfs/OLGARUS/jw4199/distillation/LLOPSD/grpo_experiments/grpo_output/4753492/global_step_1/actor/extra_state_world_size_4_rank_2.pt
[36m(WorkerDict pid=1983553)[0m INFO:2026-02-15 04:59:26,143:[Rank 1] Saved model to /scratch/gpfs/OLGARUS/jw4199/distillation/LLOPSD/grpo_experiments/grpo_output/4753492/global_step_1/actor/model_world_size_4_rank_1.pt
[36m(WorkerDict pid=1983553)[0m INFO:2026-02-15 04:59:26,147:[Rank 1] Saved optim to /scratch/gpfs/OLGARUS/jw4199/distillation/LLOPSD/grpo_experiments/grpo_output/4753492/global_step_1/actor/optim_world_size_4_rank_1.pt
[36m(WorkerDict pid=1983553)[0m INFO:2026-02-15 04:59:26,149:[Rank 1] Saved extra_state to /scratch/gpfs/OLGARUS/jw4199/distillation/LLOPSD/grpo_experiments/grpo_output/4753492/global_step_1/actor/extra_state_world_size_4_rank_1.pt
Training Progress:   0%|          | 0/3 [02:50<?, ?it/s, rollout=133.4s, backprop=0.0s, accum=1/2]Training Progress:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [02:50<05:40, 170.11s/it, rollout=133.4s, backprop=0.0s, accum=1/2][36m(WorkerDict pid=1983552)[0m INFO:2026-02-15 04:59:26,184:[Rank 0] Saved model config and tokenizer class to /scratch/gpfs/OLGARUS/jw4199/distillation/LLOPSD/grpo_experiments/grpo_output/4753492/global_step_1/actor/huggingface
2026-02-15 05:01:32,284 - INFO - Step 2 (accum 2/2): rollout done in 126.1s
2026-02-15 05:02:01,693 - INFO - Truncated 134/256 responses at first \boxed{}. Mean tokens dropped: 156.5
2026-02-15 05:03:58,676 - INFO - Step 2: backprop done in 116.9s (accumulated 2 batches, 512 samples, total rollout time: 259.4s)
[36m(WorkerDict pid=1983552)[0m INFO:2026-02-15 05:04:01,332:[Rank 0] Saved model to /scratch/gpfs/OLGARUS/jw4199/distillation/LLOPSD/grpo_experiments/grpo_output/4753492/global_step_2/actor/model_world_size_4_rank_0.pt
[36m(WorkerDict pid=1983555)[0m INFO:2026-02-15 05:04:01,436:[Rank 3] Saved model to /scratch/gpfs/OLGARUS/jw4199/distillation/LLOPSD/grpo_experiments/grpo_output/4753492/global_step_2/actor/model_world_size_4_rank_3.pt
[36m(WorkerDict pid=1983554)[0m INFO:2026-02-15 05:04:01,398:[Rank 2] Saved model to /scratch/gpfs/OLGARUS/jw4199/distillation/LLOPSD/grpo_experiments/grpo_output/4753492/global_step_2/actor/model_world_size_4_rank_2.pt
[36m(WorkerDict pid=1983553)[0m INFO:2026-02-15 05:04:01,476:[Rank 1] Saved model to /scratch/gpfs/OLGARUS/jw4199/distillation/LLOPSD/grpo_experiments/grpo_output/4753492/global_step_2/actor/model_world_size_4_rank_1.pt
[36m(WorkerDict pid=1983555)[0m INFO:2026-02-15 05:04:02,500:[Rank 3] Saved optim to /scratch/gpfs/OLGARUS/jw4199/distillation/LLOPSD/grpo_experiments/grpo_output/4753492/global_step_2/actor/optim_world_size_4_rank_3.pt
[36m(WorkerDict pid=1983555)[0m INFO:2026-02-15 05:04:02,502:[Rank 3] Saved extra_state to /scratch/gpfs/OLGARUS/jw4199/distillation/LLOPSD/grpo_experiments/grpo_output/4753492/global_step_2/actor/extra_state_world_size_4_rank_3.pt
[36m(WorkerDict pid=1983554)[0m INFO:2026-02-15 05:04:02,494:[Rank 2] Saved optim to /scratch/gpfs/OLGARUS/jw4199/distillation/LLOPSD/grpo_experiments/grpo_output/4753492/global_step_2/actor/optim_world_size_4_rank_2.pt
[36m(WorkerDict pid=1983554)[0m INFO:2026-02-15 05:04:02,495:[Rank 2] Saved extra_state to /scratch/gpfs/OLGARUS/jw4199/distillation/LLOPSD/grpo_experiments/grpo_output/4753492/global_step_2/actor/extra_state_world_size_4_rank_2.pt
[36m(WorkerDict pid=1983553)[0m INFO:2026-02-15 05:04:02,523:[Rank 1] Saved optim to /scratch/gpfs/OLGARUS/jw4199/distillation/LLOPSD/grpo_experiments/grpo_output/4753492/global_step_2/actor/optim_world_size_4_rank_1.pt
[36m(WorkerDict pid=1983553)[0m INFO:2026-02-15 05:04:02,524:[Rank 1] Saved extra_state to /scratch/gpfs/OLGARUS/jw4199/distillation/LLOPSD/grpo_experiments/grpo_output/4753492/global_step_2/actor/extra_state_world_size_4_rank_1.pt
Training Progress:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [07:26<05:40, 170.11s/it, rollout=126.1s, backprop=116.9s, accum=0/2]Training Progress:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [07:26<03:52, 232.72s/it, rollout=126.1s, backprop=116.9s, accum=0/2][36m(WorkerDict pid=1983552)[0m INFO:2026-02-15 05:04:02,705:[Rank 0] Saved optim to /scratch/gpfs/OLGARUS/jw4199/distillation/LLOPSD/grpo_experiments/grpo_output/4753492/global_step_2/actor/optim_world_size_4_rank_0.pt
[36m(WorkerDict pid=1983552)[0m INFO:2026-02-15 05:04:02,706:[Rank 0] Saved extra_state to /scratch/gpfs/OLGARUS/jw4199/distillation/LLOPSD/grpo_experiments/grpo_output/4753492/global_step_2/actor/extra_state_world_size_4_rank_0.pt
[36m(WorkerDict pid=1983552)[0m INFO:2026-02-15 05:04:02,754:[Rank 0] Saved model config and tokenizer class to /scratch/gpfs/OLGARUS/jw4199/distillation/LLOPSD/grpo_experiments/grpo_output/4753492/global_step_2/actor/huggingface
2026-02-15 05:05:56,823 - INFO - Step 3 (accum 1/2): rollout done in 114.0s
2026-02-15 05:06:23,996 - INFO - Truncated 148/256 responses at first \boxed{}. Mean tokens dropped: 94.0
2026-02-15 05:07:22,314 - INFO - Step 3: backprop done in 58.3s (accumulated 1 batches, 256 samples, total rollout time: 114.0s)
[36m(WorkerDict pid=1983552)[0m INFO:2026-02-15 05:07:24,908:[Rank 0] Saved model to /scratch/gpfs/OLGARUS/jw4199/distillation/LLOPSD/grpo_experiments/grpo_output/4753492/global_step_3/actor/model_world_size_4_rank_0.pt
[36m(WorkerDict pid=1983555)[0m INFO:2026-02-15 05:07:24,970:[Rank 3] Saved model to /scratch/gpfs/OLGARUS/jw4199/distillation/LLOPSD/grpo_experiments/grpo_output/4753492/global_step_3/actor/model_world_size_4_rank_3.pt
[36m(WorkerDict pid=1983554)[0m INFO:2026-02-15 05:07:24,974:[Rank 2] Saved model to /scratch/gpfs/OLGARUS/jw4199/distillation/LLOPSD/grpo_experiments/grpo_output/4753492/global_step_3/actor/model_world_size_4_rank_2.pt
[36m(WorkerDict pid=1983553)[0m INFO:2026-02-15 05:07:24,962:[Rank 1] Saved model to /scratch/gpfs/OLGARUS/jw4199/distillation/LLOPSD/grpo_experiments/grpo_output/4753492/global_step_3/actor/model_world_size_4_rank_1.pt
[36m(WorkerDict pid=1983555)[0m INFO:2026-02-15 05:07:26,020:[Rank 3] Saved optim to /scratch/gpfs/OLGARUS/jw4199/distillation/LLOPSD/grpo_experiments/grpo_output/4753492/global_step_3/actor/optim_world_size_4_rank_3.pt
[36m(WorkerDict pid=1983555)[0m INFO:2026-02-15 05:07:26,021:[Rank 3] Saved extra_state to /scratch/gpfs/OLGARUS/jw4199/distillation/LLOPSD/grpo_experiments/grpo_output/4753492/global_step_3/actor/extra_state_world_size_4_rank_3.pt
[36m(WorkerDict pid=1983554)[0m INFO:2026-02-15 05:07:26,007:[Rank 2] Saved optim to /scratch/gpfs/OLGARUS/jw4199/distillation/LLOPSD/grpo_experiments/grpo_output/4753492/global_step_3/actor/optim_world_size_4_rank_2.pt
[36m(WorkerDict pid=1983554)[0m INFO:2026-02-15 05:07:26,008:[Rank 2] Saved extra_state to /scratch/gpfs/OLGARUS/jw4199/distillation/LLOPSD/grpo_experiments/grpo_output/4753492/global_step_3/actor/extra_state_world_size_4_rank_2.pt
[36m(WorkerDict pid=1983553)[0m INFO:2026-02-15 05:07:25,998:[Rank 1] Saved optim to /scratch/gpfs/OLGARUS/jw4199/distillation/LLOPSD/grpo_experiments/grpo_output/4753492/global_step_3/actor/optim_world_size_4_rank_1.pt
[36m(WorkerDict pid=1983553)[0m INFO:2026-02-15 05:07:26,000:[Rank 1] Saved extra_state to /scratch/gpfs/OLGARUS/jw4199/distillation/LLOPSD/grpo_experiments/grpo_output/4753492/global_step_3/actor/extra_state_world_size_4_rank_1.pt
[36m(WorkerDict pid=1983552)[0m INFO:2026-02-15 05:07:26,242:[Rank 0] Saved optim to /scratch/gpfs/OLGARUS/jw4199/distillation/LLOPSD/grpo_experiments/grpo_output/4753492/global_step_3/actor/optim_world_size_4_rank_0.pt
[36m(WorkerDict pid=1983552)[0m INFO:2026-02-15 05:07:26,243:[Rank 0] Saved extra_state to /scratch/gpfs/OLGARUS/jw4199/distillation/LLOPSD/grpo_experiments/grpo_output/4753492/global_step_3/actor/extra_state_world_size_4_rank_0.pt
[36m(WorkerDict pid=1983552)[0m INFO:2026-02-15 05:07:26,293:[Rank 0] Saved model config and tokenizer class to /scratch/gpfs/OLGARUS/jw4199/distillation/LLOPSD/grpo_experiments/grpo_output/4753492/global_step_3/actor/huggingface
Training Progress:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [10:50<03:52, 232.72s/it, rollout=114.0s, backprop=58.3s, accum=0/2] Training Progress: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [10:50<00:00, 219.42s/it, rollout=114.0s, backprop=58.3s, accum=0/2]2026-02-15 05:07:26,367 - INFO - Epoch 1 Timing Summary:
2026-02-15 05:07:26,368 - INFO -   Total rollout time: 373.5s
2026-02-15 05:07:26,368 - INFO -   Total backprop time: 175.2s
2026-02-15 05:07:26,368 - INFO - Final validation metrics: None
Training Progress: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [10:50<00:00, 216.75s/it, rollout=114.0s, backprop=58.3s, accum=0/2]
2026-02-15 05:07:26,369 - INFO - GRPO verl training completed successfully!
2026-02-15 05:07:26,369 - INFO - No validation outputs found to analyze
Exception ignored in: <function Tracking.__del__ at 0x148f02ce4820>
2026-02-15 05:07:26,663 - WARNING - socket.send() raised exception.
Traceback (most recent call last):
  File "/home/jw4199/miniconda3/envs/ouro_vllm/lib/python3.10/site-packages/verl/utils/tracking.py", line 160, in __del__
2026-02-15 05:07:26,663 - WARNING - socket.send() raised exception.
  File "/home/jw4199/miniconda3/envs/ouro_vllm/lib/python3.10/site-packages/wandb/sdk/wandb_run.py", line 4097, in finish
  File "/home/jw4199/miniconda3/envs/ouro_vllm/lib/python3.10/site-packages/wandb/sdk/wandb_run.py", line 400, in wrapper
  File "/home/jw4199/miniconda3/envs/ouro_vllm/lib/python3.10/site-packages/wandb/sdk/wandb_run.py", line 445, in wrapper
  File "/home/jw4199/miniconda3/envs/ouro_vllm/lib/python3.10/site-packages/wandb/sdk/wandb_run.py", line 2275, in finish
  File "/home/jw4199/miniconda3/envs/ouro_vllm/lib/python3.10/site-packages/wandb/sdk/wandb_run.py", line 400, in wrapper
  File "/home/jw4199/miniconda3/envs/ouro_vllm/lib/python3.10/site-packages/wandb/sdk/wandb_run.py", line 2288, in _finish
  File "/home/jw4199/miniconda3/envs/ouro_vllm/lib/python3.10/site-packages/wandb/sdk/lib/telemetry.py", line 42, in __exit__
  File "/home/jw4199/miniconda3/envs/ouro_vllm/lib/python3.10/site-packages/wandb/sdk/wandb_run.py", line 752, in _telemetry_callback
  File "/home/jw4199/miniconda3/envs/ouro_vllm/lib/python3.10/site-packages/wandb/sdk/wandb_run.py", line 765, in _telemetry_flush
  File "/home/jw4199/miniconda3/envs/ouro_vllm/lib/python3.10/site-packages/wandb/sdk/interface/interface_shared.py", line 103, in _publish_telemetry
  File "/home/jw4199/miniconda3/envs/ouro_vllm/lib/python3.10/site-packages/wandb/sdk/interface/interface_sock.py", line 46, in _publish
  File "/home/jw4199/miniconda3/envs/ouro_vllm/lib/python3.10/site-packages/wandb/sdk/lib/asyncio_manager.py", line 136, in run
  File "/home/jw4199/miniconda3/envs/ouro_vllm/lib/python3.10/concurrent/futures/_base.py", line 458, in result
  File "/home/jw4199/miniconda3/envs/ouro_vllm/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
  File "/home/jw4199/miniconda3/envs/ouro_vllm/lib/python3.10/site-packages/wandb/sdk/lib/asyncio_manager.py", line 219, in _wrap
  File "/home/jw4199/miniconda3/envs/ouro_vllm/lib/python3.10/site-packages/wandb/sdk/lib/service/service_client.py", line 38, in publish
  File "/home/jw4199/miniconda3/envs/ouro_vllm/lib/python3.10/site-packages/wandb/sdk/lib/service/service_client.py", line 64, in _send_server_request
  File "/home/jw4199/miniconda3/envs/ouro_vllm/lib/python3.10/asyncio/streams.py", line 371, in drain
  File "/home/jw4199/miniconda3/envs/ouro_vllm/lib/python3.10/asyncio/streams.py", line 167, in _drain_helper
ConnectionResetError: Connection lost
2026-02-15 05:07:26,683 - WARNING - socket.send() raised exception.
2026-02-15 05:07:26,690 - WARNING - socket.send() raised exception.
2026-02-15 05:07:26,690 - WARNING - socket.send() raised exception.
2026-02-15 05:07:26,690 - WARNING - socket.send() raised exception.
2026-02-15 05:07:26,690 - WARNING - socket.send() raised exception.
2026-02-15 05:07:26,690 - WARNING - socket.send() raised exception.
2026-02-15 05:07:26,690 - WARNING - socket.send() raised exception.
2026-02-15 05:07:26,690 - WARNING - socket.send() raised exception.
2026-02-15 05:07:26,690 - WARNING - socket.send() raised exception.
2026-02-15 05:07:26,690 - WARNING - socket.send() raised exception.
2026-02-15 05:07:26,690 - WARNING - socket.send() raised exception.
2026-02-15 05:07:26,690 - WARNING - socket.send() raised exception.
2026-02-15 05:07:26,690 - WARNING - socket.send() raised exception.
2026-02-15 05:07:26,690 - WARNING - socket.send() raised exception.
2026-02-15 05:07:26,690 - WARNING - socket.send() raised exception.
2026-02-15 05:07:26,690 - WARNING - socket.send() raised exception.
2026-02-15 05:07:26,690 - WARNING - socket.send() raised exception.
2026-02-15 05:07:26,690 - WARNING - socket.send() raised exception.
2026-02-15 05:07:26,690 - WARNING - socket.send() raised exception.
2026-02-15 05:07:26,690 - WARNING - socket.send() raised exception.
2026-02-15 05:07:26,690 - WARNING - socket.send() raised exception.
2026-02-15 05:07:26,691 - WARNING - socket.send() raised exception.
2026-02-15 05:07:26,691 - WARNING - socket.send() raised exception.
2026-02-15 05:07:26,691 - WARNING - socket.send() raised exception.
2026-02-15 05:07:26,691 - WARNING - socket.send() raised exception.
2026-02-15 05:07:26,691 - WARNING - socket.send() raised exception.
2026-02-15 05:07:26,691 - WARNING - socket.send() raised exception.
2026-02-15 05:07:26,691 - WARNING - socket.send() raised exception.
2026-02-15 05:07:26,691 - WARNING - socket.send() raised exception.
2026-02-15 05:07:26,691 - WARNING - socket.send() raised exception.
2026-02-15 05:07:26,691 - WARNING - socket.send() raised exception.
2026-02-15 05:07:26,691 - WARNING - socket.send() raised exception.
2026-02-15 05:07:26,691 - WARNING - socket.send() raised exception.
2026-02-15 05:07:26,691 - WARNING - socket.send() raised exception.
2026-02-15 05:07:26,691 - WARNING - socket.send() raised exception.
2026-02-15 05:07:26,691 - WARNING - socket.send() raised exception.
2026-02-15 05:07:26,691 - WARNING - socket.send() raised exception.
2026-02-15 05:07:26,691 - WARNING - socket.send() raised exception.
2026-02-15 05:07:26,691 - WARNING - socket.send() raised exception.
2026-02-15 05:07:26,691 - WARNING - socket.send() raised exception.
2026-02-15 05:07:26,691 - WARNING - socket.send() raised exception.
2026-02-15 05:07:26,691 - WARNING - socket.send() raised exception.
2026-02-15 05:07:26,691 - WARNING - socket.send() raised exception.
2026-02-15 05:07:26,691 - WARNING - socket.send() raised exception.
2026-02-15 05:07:26,691 - WARNING - socket.send() raised exception.
2026-02-15 05:07:26,691 - WARNING - socket.send() raised exception.
2026-02-15 05:07:26,691 - WARNING - socket.send() raised exception.
2026-02-15 05:07:26,691 - WARNING - socket.send() raised exception.
2026-02-15 05:07:26,691 - WARNING - socket.send() raised exception.
2026-02-15 05:07:26,691 - WARNING - socket.send() raised exception.
2026-02-15 05:07:26,691 - WARNING - socket.send() raised exception.
2026-02-15 05:07:26,691 - WARNING - socket.send() raised exception.
