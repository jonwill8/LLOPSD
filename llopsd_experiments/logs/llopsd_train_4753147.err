/home/jw4199/miniconda3/envs/ouro_vllm/lib/python3.10/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
2026-02-15 03:25:53,893 - INFO - ============================================================
2026-02-15 03:25:53,893 - INFO - LLOPSD Training - Latent-Loop On-Policy Self-Distillation
2026-02-15 03:25:53,893 - INFO - ============================================================
2026-02-15 03:25:53,893 - INFO - 
2026-02-15 03:25:53,893 - INFO - *** TRAINING MODE: FULL PARAMETER FINE-TUNING ***
2026-02-15 03:25:53,893 - INFO - *** All model parameters will be updated during training ***
2026-02-15 03:25:53,893 - INFO - 
2026-02-15 03:25:53,893 - INFO - Model: /scratch/gpfs/OLGARUS/jw4199/model_weights_path/Ouro-2.6B-Thinking
2026-02-15 03:25:53,893 - INFO - Output: ./llopsd_output/4753147
2026-02-15 03:25:53,893 - INFO - Num generations: 1
2026-02-15 03:25:53,893 - INFO - Num prompts per batch: 32
2026-02-15 03:25:53,893 - INFO - Gradient accumulation steps: 2
2026-02-15 03:25:53,893 - INFO - Effective prompts per update: 64 (32 x 2)
2026-02-15 03:25:53,893 - INFO - Learning rate: 1e-06
2026-02-15 03:25:53,893 - INFO - Temperature: 1.0
2026-02-15 03:25:53,893 - INFO - Max prompt length: 1024
2026-02-15 03:25:53,893 - INFO - Max completion length: 2048
2026-02-15 03:25:53,893 - INFO - Epochs: 1
2026-02-15 03:25:53,893 - INFO - vLLM enabled: True
2026-02-15 03:25:53,894 - INFO - PPO max token len: 8192
2026-02-15 03:25:53,894 - INFO - Log-prob max token len: 8192
2026-02-15 03:25:53,894 - INFO - Ref model offload: False
2026-02-15 03:25:53,894 - INFO - 
2026-02-15 03:25:53,894 - INFO - LLOPSD-specific settings:
2026-02-15 03:25:53,894 - INFO -   Teacher loops: 4
2026-02-15 03:25:53,894 - INFO -   Student loops: 2
2026-02-15 03:25:53,894 - INFO -   Loop mapping: shift
2026-02-15 03:25:53,894 - INFO -   Weight schedule: uniform
2026-02-15 03:25:53,894 - INFO -   Weight gamma: 1.0
2026-02-15 03:25:53,894 - INFO -   Divergence: jsd
2026-02-15 03:25:53,894 - INFO -   Teacher context: opsd
2026-02-15 03:25:53,894 - INFO -   Teacher mode: same
2026-02-15 03:25:53,894 - INFO -   Total UT steps: 4
2026-02-15 03:25:53,894 - INFO - SFT checkpoint loading disabled - starting from fresh model weights
2026-02-15 03:25:53,895 - INFO - Saved configuration to: ./llopsd_output/4753147/config.json
2026-02-15 03:25:53,895 - INFO - Using verl for 4-GPU training with LLOPSD distillation objective
2026-02-15 03:25:53,895 - INFO - LLOPSDActorRolloutRefWorker enables loop-aware teacher-student distillation in verl
/home/jw4199/miniconda3/envs/ouro_vllm/lib/python3.10/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
2026-02-15 03:26:05,597 - INFO - Setting up verl training with LLOPSD distillation objective...
2026-02-15 03:26:05,597 - INFO - Using LLOPSDActorRolloutRefWorker for loop-aware teacher-student distillation
2026-02-15 03:26:05,597 - INFO - ============================================================
2026-02-15 03:26:05,597 - INFO - FULL PARAMETER FINE-TUNING MODE
2026-02-15 03:26:05,597 - INFO - All model parameters will be trained (no LoRA adapters)
2026-02-15 03:26:05,597 - INFO - ============================================================
2026-02-15 03:26:05,692 - INFO - Loading JSONL from /scratch/gpfs/OLGARUS/jw4199/datasets/MATH/math_train.jsonl
2026-02-15 03:26:05,783 - INFO - Loaded 7500 examples
2026-02-15 03:26:05,844 - INFO - Saved 7500 examples to /scratch/gpfs/OLGARUS/jw4199/distillation/LLOPSD/llopsd_experiments/llopsd_output/4753147/data/train.parquet
2026-02-15 03:26:05,847 - INFO - LR Schedule:
2026-02-15 03:26:05,848 - INFO -   Scheduler type: constant
2026-02-15 03:26:05,848 - INFO -   Total steps (batches): 3
2026-02-15 03:26:05,848 - INFO -   Warmup steps (batches): 0 (10%)
2026-02-15 03:26:05,848 - INFO -   Prompts per batch: 32
2026-02-15 03:26:05,850 - INFO - Saved verl config to: ./llopsd_output/4753147/verl_config.json
2026-02-15 03:26:05,850 - INFO - LLOPSD Configuration:
2026-02-15 03:26:05,850 - INFO -   Teacher loops: 4
2026-02-15 03:26:05,850 - INFO -   Student loops: 2
2026-02-15 03:26:05,850 - INFO -   Loop mapping: shift
2026-02-15 03:26:05,850 - INFO -   Weight schedule: uniform
2026-02-15 03:26:05,850 - INFO -   Weight gamma: 1.0
2026-02-15 03:26:05,850 - INFO -   Divergence: jsd
2026-02-15 03:26:05,850 - INFO -   Teacher context: opsd
2026-02-15 03:26:05,850 - INFO -   Teacher mode: same
2026-02-15 03:26:05,857 - INFO - Starting verl training with LLOPSD worker...
[2026-02-15 03:26:05,919 W 2578667 2578667] network_util.cc:296: Failed to determine local IP via external connectivity to: 8.8.8.8:53, [2001:4860:4860::8888]:53, falling back to hostname resolution
2026-02-15 03:26:09,975	INFO worker.py:2014 -- Started a local Ray instance. View the dashboard at [1m[32m127.0.0.1:8266 [39m[22m
/home/jw4199/miniconda3/envs/ouro_vllm/lib/python3.10/site-packages/ray/_private/worker.py:2062: FutureWarning: Tip: In future versions of Ray, Ray will no longer override accelerator visible devices env var if num_gpus=0 or num_gpus=None (default). To enable this behavior and turn off this error message, set RAY_ACCEL_ENV_VAR_OVERRIDE_ON_ZERO=0
  warnings.warn(
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 7500 examples [00:00, 506901.29 examples/s]
Filtering prompts longer than 1024 tokens (num_proc=16):   0%|          | 0/7500 [00:00<?, ? examples/s]Filtering prompts longer than 1024 tokens (num_proc=16):   6%|â–‹         | 469/7500 [00:00<00:08, 819.57 examples/s]Filtering prompts longer than 1024 tokens (num_proc=16):  19%|â–ˆâ–‰        | 1407/7500 [00:00<00:02, 2240.20 examples/s]Filtering prompts longer than 1024 tokens (num_proc=16):  38%|â–ˆâ–ˆâ–ˆâ–Š      | 2814/7500 [00:00<00:01, 4598.20 examples/s]Filtering prompts longer than 1024 tokens (num_proc=16):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 3752/7500 [00:01<00:00, 4932.73 examples/s]Filtering prompts longer than 1024 tokens (num_proc=16):  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 4690/7500 [00:01<00:00, 5147.18 examples/s]Filtering prompts longer than 1024 tokens (num_proc=16):  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 5627/7500 [00:01<00:00, 5757.20 examples/s]Filtering prompts longer than 1024 tokens (num_proc=16):  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 7032/7500 [00:01<00:00, 6645.77 examples/s]Filtering prompts longer than 1024 tokens (num_proc=16): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7500/7500 [00:01<00:00, 4462.59 examples/s]
Filtering prompts longer than 1024 tokens (num_proc=16):   0%|          | 0/7500 [00:00<?, ? examples/s]Filtering prompts longer than 1024 tokens (num_proc=16):   6%|â–‹         | 469/7500 [00:00<00:07, 958.36 examples/s]Filtering prompts longer than 1024 tokens (num_proc=16):  19%|â–ˆâ–‰        | 1407/7500 [00:00<00:02, 2832.79 examples/s]Filtering prompts longer than 1024 tokens (num_proc=16):  31%|â–ˆâ–ˆâ–ˆâ–      | 2345/7500 [00:00<00:01, 3655.10 examples/s]Filtering prompts longer than 1024 tokens (num_proc=16):  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 3283/7500 [00:00<00:01, 4052.69 examples/s]Filtering prompts longer than 1024 tokens (num_proc=16):  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4221/7500 [00:01<00:00, 5067.44 examples/s]Filtering prompts longer than 1024 tokens (num_proc=16):  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 5159/7500 [00:01<00:00, 5925.67 examples/s]Filtering prompts longer than 1024 tokens (num_proc=16):  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 6096/7500 [00:01<00:00, 6724.26 examples/s]Filtering prompts longer than 1024 tokens (num_proc=16):  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 7032/7500 [00:01<00:00, 6913.50 examples/s]Filtering prompts longer than 1024 tokens (num_proc=16): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7500/7500 [00:01<00:00, 4642.26 examples/s]
[36m(pid=2584104)[0m /home/jw4199/miniconda3/envs/ouro_vllm/lib/python3.10/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
[36m(pid=2584104)[0m   warnings.warn(
[36m(pid=2584103)[0m /home/jw4199/miniconda3/envs/ouro_vllm/lib/python3.10/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
[36m(pid=2584103)[0m   warnings.warn(
[36m(pid=2584106)[0m /home/jw4199/miniconda3/envs/ouro_vllm/lib/python3.10/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
[36m(pid=2584106)[0m   warnings.warn(
[36m(pid=2584105)[0m /home/jw4199/miniconda3/envs/ouro_vllm/lib/python3.10/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
[36m(pid=2584105)[0m   warnings.warn(
[36m(WorkerDict pid=2584104)[0m /home/jw4199/miniconda3/envs/ouro_vllm/lib/python3.10/site-packages/verl/utils/tokenizer.py:30: UserWarning: tokenizer.pad_token_id is None. Now set to 0
[36m(WorkerDict pid=2584104)[0m   warnings.warn(f"tokenizer.pad_token_id is None. Now set to {tokenizer.eos_token_id}", stacklevel=1)
[36m(WorkerDict pid=2584103)[0m /home/jw4199/miniconda3/envs/ouro_vllm/lib/python3.10/site-packages/verl/utils/tokenizer.py:30: UserWarning: tokenizer.pad_token_id is None. Now set to 0
[36m(WorkerDict pid=2584103)[0m   warnings.warn(f"tokenizer.pad_token_id is None. Now set to {tokenizer.eos_token_id}", stacklevel=1)
[36m(WorkerDict pid=2584106)[0m /home/jw4199/miniconda3/envs/ouro_vllm/lib/python3.10/site-packages/verl/utils/tokenizer.py:30: UserWarning: tokenizer.pad_token_id is None. Now set to 0
[36m(WorkerDict pid=2584106)[0m   warnings.warn(f"tokenizer.pad_token_id is None. Now set to {tokenizer.eos_token_id}", stacklevel=1)
[36m(WorkerDict pid=2584105)[0m /home/jw4199/miniconda3/envs/ouro_vllm/lib/python3.10/site-packages/verl/utils/tokenizer.py:30: UserWarning: tokenizer.pad_token_id is None. Now set to 0
[36m(WorkerDict pid=2584105)[0m   warnings.warn(f"tokenizer.pad_token_id is None. Now set to {tokenizer.eos_token_id}", stacklevel=1)
[36m(WorkerDict pid=2584104)[0m `torch_dtype` is deprecated! Use `dtype` instead!
[36m(WorkerDict pid=2584104)[0m Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in OuroForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", dtype=torch.float16)`
[36m(WorkerDict pid=2584103)[0m `torch_dtype` is deprecated! Use `dtype` instead!
[36m(WorkerDict pid=2584103)[0m Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in OuroForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", dtype=torch.float16)`
[36m(WorkerDict pid=2584106)[0m `torch_dtype` is deprecated! Use `dtype` instead!
[36m(WorkerDict pid=2584106)[0m Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in OuroForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", dtype=torch.float16)`
[36m(WorkerDict pid=2584105)[0m `torch_dtype` is deprecated! Use `dtype` instead!
[36m(WorkerDict pid=2584105)[0m Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in OuroForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", dtype=torch.float16)`
[36m(WorkerDict pid=2584104)[0m Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in OuroModel is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", dtype=torch.float16)`
[36m(WorkerDict pid=2584103)[0m Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in OuroModel is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", dtype=torch.float16)`
[36m(WorkerDict pid=2584106)[0m Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in OuroModel is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", dtype=torch.float16)`
[36m(WorkerDict pid=2584105)[0m Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in OuroModel is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", dtype=torch.float16)`
[36m(WorkerDict pid=2584104)[0m /home/jw4199/miniconda3/envs/ouro_vllm/lib/python3.10/site-packages/verl/utils/tokenizer.py:30: UserWarning: tokenizer.pad_token_id is None. Now set to 0
[36m(WorkerDict pid=2584104)[0m   warnings.warn(f"tokenizer.pad_token_id is None. Now set to {tokenizer.eos_token_id}", stacklevel=1)
[36m(WorkerDict pid=2584103)[0m /home/jw4199/miniconda3/envs/ouro_vllm/lib/python3.10/site-packages/verl/utils/tokenizer.py:30: UserWarning: tokenizer.pad_token_id is None. Now set to 0
[36m(WorkerDict pid=2584103)[0m   warnings.warn(f"tokenizer.pad_token_id is None. Now set to {tokenizer.eos_token_id}", stacklevel=1)
[36m(WorkerDict pid=2584106)[0m /home/jw4199/miniconda3/envs/ouro_vllm/lib/python3.10/site-packages/verl/utils/tokenizer.py:30: UserWarning: tokenizer.pad_token_id is None. Now set to 0
[36m(WorkerDict pid=2584106)[0m   warnings.warn(f"tokenizer.pad_token_id is None. Now set to {tokenizer.eos_token_id}", stacklevel=1)
[36m(WorkerDict pid=2584105)[0m /home/jw4199/miniconda3/envs/ouro_vllm/lib/python3.10/site-packages/verl/utils/tokenizer.py:30: UserWarning: tokenizer.pad_token_id is None. Now set to 0
[36m(WorkerDict pid=2584105)[0m   warnings.warn(f"tokenizer.pad_token_id is None. Now set to {tokenizer.eos_token_id}", stacklevel=1)
[36m(WorkerDict pid=2584104)[0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[36m(WorkerDict pid=2584103)[0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[36m(WorkerDict pid=2584106)[0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[36m(WorkerDict pid=2584105)[0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[36m(WorkerDict pid=2584103)[0m Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
[36m(WorkerDict pid=2584103)[0m Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.19s/it]
[36m(WorkerDict pid=2584103)[0m Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.19s/it]
[36m(WorkerDict pid=2584103)[0m 
[36m(WorkerDict pid=2584103)[0m [rank0]:W0215 03:27:37.003000 2584103 site-packages/torch/_inductor/codecache.py:1021] [0/0] fx graph cache unable to load compiled graph
[36m(WorkerDict pid=2584103)[0m [rank0]:W0215 03:27:37.003000 2584103 site-packages/torch/_inductor/codecache.py:1021] [0/0] Traceback (most recent call last):
[36m(WorkerDict pid=2584103)[0m [rank0]:W0215 03:27:37.003000 2584103 site-packages/torch/_inductor/codecache.py:1021] [0/0]   File "/home/jw4199/miniconda3/envs/ouro_vllm/lib/python3.10/site-packages/torch/_inductor/codecache.py", line 1019, in iterate_over_candidates
[36m(WorkerDict pid=2584103)[0m [rank0]:W0215 03:27:37.003000 2584103 site-packages/torch/_inductor/codecache.py:1021] [0/0]     yield pickle.loads(content), content
[36m(WorkerDict pid=2584103)[0m [rank0]:W0215 03:27:37.003000 2584103 site-packages/torch/_inductor/codecache.py:1021] [0/0] EOFError: Ran out of input
[36m(WorkerDict pid=2584103)[0m [rank0]:W0215 03:27:37.066000 2584103 site-packages/torch/_inductor/codecache.py:1021] [0/0] fx graph cache unable to load compiled graph
[36m(WorkerDict pid=2584103)[0m [rank0]:W0215 03:27:37.066000 2584103 site-packages/torch/_inductor/codecache.py:1021] [0/0] Traceback (most recent call last):
[36m(WorkerDict pid=2584103)[0m [rank0]:W0215 03:27:37.066000 2584103 site-packages/torch/_inductor/codecache.py:1021] [0/0]   File "/home/jw4199/miniconda3/envs/ouro_vllm/lib/python3.10/site-packages/torch/_inductor/codecache.py", line 1019, in iterate_over_candidates
[36m(WorkerDict pid=2584103)[0m [rank0]:W0215 03:27:37.066000 2584103 site-packages/torch/_inductor/codecache.py:1021] [0/0]     yield pickle.loads(content), content
[36m(WorkerDict pid=2584103)[0m [rank0]:W0215 03:27:37.066000 2584103 site-packages/torch/_inductor/codecache.py:1021] [0/0] EOFError: Ran out of input
[36m(WorkerDict pid=2584105)[0m [rank2]:W0215 03:27:37.741000 2584105 site-packages/torch/_inductor/codecache.py:1021] [0/0] fx graph cache unable to load compiled graph
[36m(WorkerDict pid=2584105)[0m [rank2]:W0215 03:27:37.741000 2584105 site-packages/torch/_inductor/codecache.py:1021] [0/0] Traceback (most recent call last):
[36m(WorkerDict pid=2584105)[0m [rank2]:W0215 03:27:37.741000 2584105 site-packages/torch/_inductor/codecache.py:1021] [0/0]   File "/home/jw4199/miniconda3/envs/ouro_vllm/lib/python3.10/site-packages/torch/_inductor/codecache.py", line 1019, in iterate_over_candidates
[36m(WorkerDict pid=2584105)[0m [rank2]:W0215 03:27:37.741000 2584105 site-packages/torch/_inductor/codecache.py:1021] [0/0]     yield pickle.loads(content), content
[36m(WorkerDict pid=2584105)[0m [rank2]:W0215 03:27:37.741000 2584105 site-packages/torch/_inductor/codecache.py:1021] [0/0] EOFError: Ran out of input
[36m(WorkerDict pid=2584104)[0m [rank1]:W0215 03:27:37.898000 2584104 site-packages/torch/_inductor/codecache.py:1021] [0/0] fx graph cache unable to load compiled graph
[36m(WorkerDict pid=2584104)[0m [rank1]:W0215 03:27:37.898000 2584104 site-packages/torch/_inductor/codecache.py:1021] [0/0] Traceback (most recent call last):
[36m(WorkerDict pid=2584104)[0m [rank1]:W0215 03:27:37.898000 2584104 site-packages/torch/_inductor/codecache.py:1021] [0/0]   File "/home/jw4199/miniconda3/envs/ouro_vllm/lib/python3.10/site-packages/torch/_inductor/codecache.py", line 1019, in iterate_over_candidates
[36m(WorkerDict pid=2584104)[0m [rank1]:W0215 03:27:37.898000 2584104 site-packages/torch/_inductor/codecache.py:1021] [0/0]     yield pickle.loads(content), content
[36m(WorkerDict pid=2584104)[0m [rank1]:W0215 03:27:37.898000 2584104 site-packages/torch/_inductor/codecache.py:1021] [0/0] EOFError: Ran out of input
[36m(WorkerDict pid=2584104)[0m [rank1]:W0215 03:27:39.036000 2584104 site-packages/torch/_inductor/codecache.py:1021] [0/0] fx graph cache unable to load compiled graph
[36m(WorkerDict pid=2584104)[0m [rank1]:W0215 03:27:39.036000 2584104 site-packages/torch/_inductor/codecache.py:1021] [0/0] Traceback (most recent call last):
[36m(WorkerDict pid=2584104)[0m [rank1]:W0215 03:27:39.036000 2584104 site-packages/torch/_inductor/codecache.py:1021] [0/0]   File "/home/jw4199/miniconda3/envs/ouro_vllm/lib/python3.10/site-packages/torch/_inductor/codecache.py", line 1019, in iterate_over_candidates
[36m(WorkerDict pid=2584104)[0m [rank1]:W0215 03:27:39.036000 2584104 site-packages/torch/_inductor/codecache.py:1021] [0/0]     yield pickle.loads(content), content
[36m(WorkerDict pid=2584104)[0m [rank1]:W0215 03:27:39.036000 2584104 site-packages/torch/_inductor/codecache.py:1021] [0/0] EOFError: Ran out of input
[36m(WorkerDict pid=2584105)[0m [rank2]:W0215 03:27:41.102000 2584105 site-packages/torch/_inductor/codecache.py:1021] [0/0] fx graph cache unable to load compiled graph
[36m(WorkerDict pid=2584105)[0m [rank2]:W0215 03:27:41.102000 2584105 site-packages/torch/_inductor/codecache.py:1021] [0/0] Traceback (most recent call last):
[36m(WorkerDict pid=2584105)[0m [rank2]:W0215 03:27:41.102000 2584105 site-packages/torch/_inductor/codecache.py:1021] [0/0]   File "/home/jw4199/miniconda3/envs/ouro_vllm/lib/python3.10/site-packages/torch/_inductor/codecache.py", line 1019, in iterate_over_candidates
[36m(WorkerDict pid=2584105)[0m [rank2]:W0215 03:27:41.102000 2584105 site-packages/torch/_inductor/codecache.py:1021] [0/0]     yield pickle.loads(content), content
[36m(WorkerDict pid=2584105)[0m [rank2]:W0215 03:27:41.102000 2584105 site-packages/torch/_inductor/codecache.py:1021] [0/0] EOFError: Ran out of input
[36m(WorkerDict pid=2584105)[0m [rank2]:W0215 03:27:42.424000 2584105 site-packages/torch/_inductor/codecache.py:1021] [0/0] fx graph cache unable to load compiled graph
[36m(WorkerDict pid=2584105)[0m [rank2]:W0215 03:27:42.424000 2584105 site-packages/torch/_inductor/codecache.py:1021] [0/0] Traceback (most recent call last):
[36m(WorkerDict pid=2584105)[0m [rank2]:W0215 03:27:42.424000 2584105 site-packages/torch/_inductor/codecache.py:1021] [0/0]   File "/home/jw4199/miniconda3/envs/ouro_vllm/lib/python3.10/site-packages/torch/_inductor/codecache.py", line 1019, in iterate_over_candidates
[36m(WorkerDict pid=2584105)[0m [rank2]:W0215 03:27:42.424000 2584105 site-packages/torch/_inductor/codecache.py:1021] [0/0]     yield pickle.loads(content), content
[36m(WorkerDict pid=2584105)[0m [rank2]:W0215 03:27:42.424000 2584105 site-packages/torch/_inductor/codecache.py:1021] [0/0] EOFError: Ran out of input
[36m(WorkerDict pid=2584104)[0m [rank1]:W0215 03:27:42.603000 2584104 site-packages/torch/_inductor/codecache.py:1021] [0/0] fx graph cache unable to load compiled graph
[36m(WorkerDict pid=2584104)[0m [rank1]:W0215 03:27:42.603000 2584104 site-packages/torch/_inductor/codecache.py:1021] [0/0] Traceback (most recent call last):
[36m(WorkerDict pid=2584104)[0m [rank1]:W0215 03:27:42.603000 2584104 site-packages/torch/_inductor/codecache.py:1021] [0/0]   File "/home/jw4199/miniconda3/envs/ouro_vllm/lib/python3.10/site-packages/torch/_inductor/codecache.py", line 1019, in iterate_over_candidates
[36m(WorkerDict pid=2584104)[0m [rank1]:W0215 03:27:42.603000 2584104 site-packages/torch/_inductor/codecache.py:1021] [0/0]     yield pickle.loads(content), content
[36m(WorkerDict pid=2584104)[0m [rank1]:W0215 03:27:42.603000 2584104 site-packages/torch/_inductor/codecache.py:1021] [0/0] EOFError: Ran out of input
[36m(WorkerDict pid=2584105)[0m [rank2]:W0215 03:27:44.157000 2584105 site-packages/torch/_inductor/codecache.py:1021] [0/0] fx graph cache unable to load compiled graph
[36m(WorkerDict pid=2584105)[0m [rank2]:W0215 03:27:44.157000 2584105 site-packages/torch/_inductor/codecache.py:1021] [0/0] Traceback (most recent call last):
[36m(WorkerDict pid=2584105)[0m [rank2]:W0215 03:27:44.157000 2584105 site-packages/torch/_inductor/codecache.py:1021] [0/0]   File "/home/jw4199/miniconda3/envs/ouro_vllm/lib/python3.10/site-packages/torch/_inductor/codecache.py", line 1019, in iterate_over_candidates
[36m(WorkerDict pid=2584105)[0m [rank2]:W0215 03:27:44.157000 2584105 site-packages/torch/_inductor/codecache.py:1021] [0/0]     yield pickle.loads(content), content
[36m(WorkerDict pid=2584105)[0m [rank2]:W0215 03:27:44.157000 2584105 site-packages/torch/_inductor/codecache.py:1021] [0/0] EOFError: Ran out of input
[36m(WorkerDict pid=2584106)[0m [rank3]:W0215 03:27:44.952000 2584106 site-packages/torch/_inductor/codecache.py:1021] [0/0] fx graph cache unable to load compiled graph
[36m(WorkerDict pid=2584106)[0m [rank3]:W0215 03:27:44.952000 2584106 site-packages/torch/_inductor/codecache.py:1021] [0/0] Traceback (most recent call last):
[36m(WorkerDict pid=2584106)[0m [rank3]:W0215 03:27:44.952000 2584106 site-packages/torch/_inductor/codecache.py:1021] [0/0]   File "/home/jw4199/miniconda3/envs/ouro_vllm/lib/python3.10/site-packages/torch/_inductor/codecache.py", line 1019, in iterate_over_candidates
[36m(WorkerDict pid=2584106)[0m [rank3]:W0215 03:27:44.952000 2584106 site-packages/torch/_inductor/codecache.py:1021] [0/0]     yield pickle.loads(content), content
[36m(WorkerDict pid=2584106)[0m [rank3]:W0215 03:27:44.952000 2584106 site-packages/torch/_inductor/codecache.py:1021] [0/0] EOFError: Ran out of input
[36m(WorkerDict pid=2584106)[0m [rank3]:W0215 03:27:46.767000 2584106 site-packages/torch/_inductor/codecache.py:1021] [0/0] fx graph cache unable to load compiled graph
[36m(WorkerDict pid=2584106)[0m [rank3]:W0215 03:27:46.767000 2584106 site-packages/torch/_inductor/codecache.py:1021] [0/0] Traceback (most recent call last):
[36m(WorkerDict pid=2584106)[0m [rank3]:W0215 03:27:46.767000 2584106 site-packages/torch/_inductor/codecache.py:1021] [0/0]   File "/home/jw4199/miniconda3/envs/ouro_vllm/lib/python3.10/site-packages/torch/_inductor/codecache.py", line 1019, in iterate_over_candidates
[36m(WorkerDict pid=2584106)[0m [rank3]:W0215 03:27:46.767000 2584106 site-packages/torch/_inductor/codecache.py:1021] [0/0]     yield pickle.loads(content), content
[36m(WorkerDict pid=2584106)[0m [rank3]:W0215 03:27:46.767000 2584106 site-packages/torch/_inductor/codecache.py:1021] [0/0] EOFError: Ran out of input
[36m(WorkerDict pid=2584103)[0m [rank0]:W0215 03:27:47.711000 2584103 site-packages/torch/_inductor/codecache.py:1021] [0/0] fx graph cache unable to load compiled graph
[36m(WorkerDict pid=2584103)[0m [rank0]:W0215 03:27:47.711000 2584103 site-packages/torch/_inductor/codecache.py:1021] [0/0] Traceback (most recent call last):
[36m(WorkerDict pid=2584103)[0m [rank0]:W0215 03:27:47.711000 2584103 site-packages/torch/_inductor/codecache.py:1021] [0/0]   File "/home/jw4199/miniconda3/envs/ouro_vllm/lib/python3.10/site-packages/torch/_inductor/codecache.py", line 1019, in iterate_over_candidates
[36m(WorkerDict pid=2584103)[0m [rank0]:W0215 03:27:47.711000 2584103 site-packages/torch/_inductor/codecache.py:1021] [0/0]     yield pickle.loads(content), content
[36m(WorkerDict pid=2584103)[0m [rank0]:W0215 03:27:47.711000 2584103 site-packages/torch/_inductor/codecache.py:1021] [0/0] EOFError: Ran out of input
[36m(WorkerDict pid=2584106)[0m [rank3]:W0215 03:27:48.247000 2584106 site-packages/torch/_inductor/codecache.py:1021] [0/0] fx graph cache unable to load compiled graph
[36m(WorkerDict pid=2584106)[0m [rank3]:W0215 03:27:48.247000 2584106 site-packages/torch/_inductor/codecache.py:1021] [0/0] Traceback (most recent call last):
[36m(WorkerDict pid=2584106)[0m [rank3]:W0215 03:27:48.247000 2584106 site-packages/torch/_inductor/codecache.py:1021] [0/0]   File "/home/jw4199/miniconda3/envs/ouro_vllm/lib/python3.10/site-packages/torch/_inductor/codecache.py", line 1019, in iterate_over_candidates
[36m(WorkerDict pid=2584106)[0m [rank3]:W0215 03:27:48.247000 2584106 site-packages/torch/_inductor/codecache.py:1021] [0/0]     yield pickle.loads(content), content
[36m(WorkerDict pid=2584106)[0m [rank3]:W0215 03:27:48.247000 2584106 site-packages/torch/_inductor/codecache.py:1021] [0/0] EOFError: Ran out of input
[36m(WorkerDict pid=2584103)[0m [rank0]:W0215 03:27:50.047000 2584103 site-packages/torch/_inductor/codecache.py:1021] [0/0] fx graph cache unable to load compiled graph
[36m(WorkerDict pid=2584103)[0m [rank0]:W0215 03:27:50.047000 2584103 site-packages/torch/_inductor/codecache.py:1021] [0/0] Traceback (most recent call last):
[36m(WorkerDict pid=2584103)[0m [rank0]:W0215 03:27:50.047000 2584103 site-packages/torch/_inductor/codecache.py:1021] [0/0]   File "/home/jw4199/miniconda3/envs/ouro_vllm/lib/python3.10/site-packages/torch/_inductor/codecache.py", line 1019, in iterate_over_candidates
[36m(WorkerDict pid=2584103)[0m [rank0]:W0215 03:27:50.047000 2584103 site-packages/torch/_inductor/codecache.py:1021] [0/0]     yield pickle.loads(content), content
[36m(WorkerDict pid=2584103)[0m [rank0]:W0215 03:27:50.047000 2584103 site-packages/torch/_inductor/codecache.py:1021] [0/0] EOFError: Ran out of input
[36m(WorkerDict pid=2584106)[0m [rank3]:W0215 03:27:50.138000 2584106 site-packages/torch/_inductor/codecache.py:1021] [0/0] fx graph cache unable to load compiled graph
[36m(WorkerDict pid=2584106)[0m [rank3]:W0215 03:27:50.138000 2584106 site-packages/torch/_inductor/codecache.py:1021] [0/0] Traceback (most recent call last):
[36m(WorkerDict pid=2584106)[0m [rank3]:W0215 03:27:50.138000 2584106 site-packages/torch/_inductor/codecache.py:1021] [0/0]   File "/home/jw4199/miniconda3/envs/ouro_vllm/lib/python3.10/site-packages/torch/_inductor/codecache.py", line 1019, in iterate_over_candidates
[36m(WorkerDict pid=2584106)[0m [rank3]:W0215 03:27:50.138000 2584106 site-packages/torch/_inductor/codecache.py:1021] [0/0]     yield pickle.loads(content), content
[36m(WorkerDict pid=2584106)[0m [rank3]:W0215 03:27:50.138000 2584106 site-packages/torch/_inductor/codecache.py:1021] [0/0] EOFError: Ran out of input
[36m(WorkerDict pid=2584105)[0m [rank2]:W0215 03:27:50.484000 2584105 site-packages/torch/_inductor/codecache.py:1021] [0/0] fx graph cache unable to load compiled graph
[36m(WorkerDict pid=2584105)[0m [rank2]:W0215 03:27:50.484000 2584105 site-packages/torch/_inductor/codecache.py:1021] [0/0] Traceback (most recent call last):
[36m(WorkerDict pid=2584105)[0m [rank2]:W0215 03:27:50.484000 2584105 site-packages/torch/_inductor/codecache.py:1021] [0/0]   File "/home/jw4199/miniconda3/envs/ouro_vllm/lib/python3.10/site-packages/torch/_inductor/codecache.py", line 1019, in iterate_over_candidates
[36m(WorkerDict pid=2584105)[0m [rank2]:W0215 03:27:50.484000 2584105 site-packages/torch/_inductor/codecache.py:1021] [0/0]     yield pickle.loads(content), content
[36m(WorkerDict pid=2584105)[0m [rank2]:W0215 03:27:50.484000 2584105 site-packages/torch/_inductor/codecache.py:1021] [0/0] EOFError: Ran out of input
[36m(WorkerDict pid=2584104)[0m [rank1]:W0215 03:27:51.442000 2584104 site-packages/torch/_inductor/codecache.py:1021] [0/0] fx graph cache unable to load compiled graph
[36m(WorkerDict pid=2584104)[0m [rank1]:W0215 03:27:51.442000 2584104 site-packages/torch/_inductor/codecache.py:1021] [0/0] Traceback (most recent call last):
[36m(WorkerDict pid=2584104)[0m [rank1]:W0215 03:27:51.442000 2584104 site-packages/torch/_inductor/codecache.py:1021] [0/0]   File "/home/jw4199/miniconda3/envs/ouro_vllm/lib/python3.10/site-packages/torch/_inductor/codecache.py", line 1019, in iterate_over_candidates
[36m(WorkerDict pid=2584104)[0m [rank1]:W0215 03:27:51.442000 2584104 site-packages/torch/_inductor/codecache.py:1021] [0/0]     yield pickle.loads(content), content
[36m(WorkerDict pid=2584104)[0m [rank1]:W0215 03:27:51.442000 2584104 site-packages/torch/_inductor/codecache.py:1021] [0/0] EOFError: Ran out of input
[36m(WorkerDict pid=2584104)[0m [rank1]:W0215 03:27:51.507000 2584104 site-packages/torch/_inductor/codecache.py:1021] [0/0] fx graph cache unable to load compiled graph
[36m(WorkerDict pid=2584104)[0m [rank1]:W0215 03:27:51.507000 2584104 site-packages/torch/_inductor/codecache.py:1021] [0/0] Traceback (most recent call last):
[36m(WorkerDict pid=2584104)[0m [rank1]:W0215 03:27:51.507000 2584104 site-packages/torch/_inductor/codecache.py:1021] [0/0]   File "/home/jw4199/miniconda3/envs/ouro_vllm/lib/python3.10/site-packages/torch/_inductor/codecache.py", line 1019, in iterate_over_candidates
[36m(WorkerDict pid=2584104)[0m [rank1]:W0215 03:27:51.507000 2584104 site-packages/torch/_inductor/codecache.py:1021] [0/0]     yield pickle.loads(content), content
[36m(WorkerDict pid=2584104)[0m [rank1]:W0215 03:27:51.507000 2584104 site-packages/torch/_inductor/codecache.py:1021] [0/0] EOFError: Ran out of input
[36m(WorkerDict pid=2584103)[0m [rank0]:W0215 03:27:51.607000 2584103 site-packages/torch/_inductor/codecache.py:1021] [0/0] fx graph cache unable to load compiled graph
[36m(WorkerDict pid=2584103)[0m [rank0]:W0215 03:27:51.607000 2584103 site-packages/torch/_inductor/codecache.py:1021] [0/0] Traceback (most recent call last):
[36m(WorkerDict pid=2584103)[0m [rank0]:W0215 03:27:51.607000 2584103 site-packages/torch/_inductor/codecache.py:1021] [0/0]   File "/home/jw4199/miniconda3/envs/ouro_vllm/lib/python3.10/site-packages/torch/_inductor/codecache.py", line 1019, in iterate_over_candidates
[36m(WorkerDict pid=2584103)[0m [rank0]:W0215 03:27:51.607000 2584103 site-packages/torch/_inductor/codecache.py:1021] [0/0]     yield pickle.loads(content), content
[36m(WorkerDict pid=2584103)[0m [rank0]:W0215 03:27:51.607000 2584103 site-packages/torch/_inductor/codecache.py:1021] [0/0] EOFError: Ran out of input
[36m(WorkerDict pid=2584106)[0m [rank3]:W0215 03:27:52.032000 2584106 site-packages/torch/_inductor/codecache.py:1021] [0/0] fx graph cache unable to load compiled graph
[36m(WorkerDict pid=2584106)[0m [rank3]:W0215 03:27:52.032000 2584106 site-packages/torch/_inductor/codecache.py:1021] [0/0] Traceback (most recent call last):
[36m(WorkerDict pid=2584106)[0m [rank3]:W0215 03:27:52.032000 2584106 site-packages/torch/_inductor/codecache.py:1021] [0/0]   File "/home/jw4199/miniconda3/envs/ouro_vllm/lib/python3.10/site-packages/torch/_inductor/codecache.py", line 1019, in iterate_over_candidates
[36m(WorkerDict pid=2584106)[0m [rank3]:W0215 03:27:52.032000 2584106 site-packages/torch/_inductor/codecache.py:1021] [0/0]     yield pickle.loads(content), content
[36m(WorkerDict pid=2584106)[0m [rank3]:W0215 03:27:52.032000 2584106 site-packages/torch/_inductor/codecache.py:1021] [0/0] EOFError: Ran out of input
[36m(WorkerDict pid=2584104)[0m [rank1]:W0215 03:27:52.301000 2584104 site-packages/torch/_inductor/codecache.py:1021] [0/0] fx graph cache unable to load compiled graph
[36m(WorkerDict pid=2584104)[0m [rank1]:W0215 03:27:52.301000 2584104 site-packages/torch/_inductor/codecache.py:1021] [0/0] Traceback (most recent call last):
[36m(WorkerDict pid=2584104)[0m [rank1]:W0215 03:27:52.301000 2584104 site-packages/torch/_inductor/codecache.py:1021] [0/0]   File "/home/jw4199/miniconda3/envs/ouro_vllm/lib/python3.10/site-packages/torch/_inductor/codecache.py", line 1019, in iterate_over_candidates
[36m(WorkerDict pid=2584104)[0m [rank1]:W0215 03:27:52.301000 2584104 site-packages/torch/_inductor/codecache.py:1021] [0/0]     yield pickle.loads(content), content
[36m(WorkerDict pid=2584104)[0m [rank1]:W0215 03:27:52.301000 2584104 site-packages/torch/_inductor/codecache.py:1021] [0/0] EOFError: Ran out of input
[36m(WorkerDict pid=2584104)[0m 2026-02-15 03:28:02,498 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
[36m(WorkerDict pid=2584103)[0m 2026-02-15 03:28:02,486 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
[36m(WorkerDict pid=2584106)[0m 2026-02-15 03:28:02,498 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
[36m(WorkerDict pid=2584105)[0m 2026-02-15 03:28:02,500 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
[36m(WorkerDict pid=2584103)[0m 2026-02-15 03:28:02,637 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
[36m(WorkerDict pid=2584106)[0m 2026-02-15 03:28:02,645 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
[36m(WorkerDict pid=2584105)[0m 2026-02-15 03:28:02,648 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
[36m(WorkerDict pid=2584104)[0m 2026-02-15 03:28:02,650 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
[36m(WorkerDict pid=2584103)[0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/51 [00:00<?, ?it/s]
[36m(WorkerDict pid=2584103)[0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   2%|â–         | 1/51 [00:00<00:08,  5.91it/s]
[36m(WorkerDict pid=2584103)[0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   4%|â–         | 2/51 [00:00<00:07,  6.52it/s]
[36m(WorkerDict pid=2584103)[0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|â–Œ         | 3/51 [00:00<00:07,  6.51it/s]
[36m(WorkerDict pid=2584103)[0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   8%|â–Š         | 4/51 [00:00<00:07,  6.70it/s]
[36m(WorkerDict pid=2584103)[0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  10%|â–‰         | 5/51 [00:00<00:06,  6.71it/s]
[36m(WorkerDict pid=2584103)[0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  12%|â–ˆâ–        | 6/51 [00:00<00:06,  6.69it/s]
[36m(WorkerDict pid=2584103)[0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|â–ˆâ–Ž        | 7/51 [00:01<00:06,  6.70it/s]
[36m(WorkerDict pid=2584103)[0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  16%|â–ˆâ–Œ        | 8/51 [00:01<00:06,  6.71it/s]
[36m(WorkerDict pid=2584103)[0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|â–ˆâ–Š        | 9/51 [00:01<00:06,  6.67it/s]
[36m(WorkerDict pid=2584103)[0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|â–ˆâ–‰        | 10/51 [00:01<00:06,  6.75it/s]
[36m(WorkerDict pid=2584103)[0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  22%|â–ˆâ–ˆâ–       | 11/51 [00:01<00:05,  6.83it/s]
[36m(WorkerDict pid=2584103)[0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  24%|â–ˆâ–ˆâ–Ž       | 12/51 [00:01<00:05,  6.77it/s]
[36m(WorkerDict pid=2584103)[0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|â–ˆâ–ˆâ–Œ       | 13/51 [00:01<00:05,  6.58it/s]
[36m(WorkerDict pid=2584103)[0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|â–ˆâ–ˆâ–‹       | 14/51 [00:02<00:06,  5.85it/s]
[36m(WorkerDict pid=2584103)[0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|â–ˆâ–ˆâ–‰       | 15/51 [00:02<00:07,  5.02it/s]
[36m(WorkerDict pid=2584103)[0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|â–ˆâ–ˆâ–ˆâ–      | 16/51 [00:02<00:07,  4.76it/s]
[36m(WorkerDict pid=2584103)[0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 17/51 [00:02<00:07,  4.31it/s]
[36m(WorkerDict pid=2584103)[0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 18/51 [00:03<00:07,  4.69it/s]
[36m(WorkerDict pid=2584103)[0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|â–ˆâ–ˆâ–ˆâ–‹      | 19/51 [00:03<00:06,  5.09it/s]
[36m(WorkerDict pid=2584103)[0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  39%|â–ˆâ–ˆâ–ˆâ–‰      | 20/51 [00:03<00:05,  5.47it/s]
[36m(WorkerDict pid=2584103)[0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 21/51 [00:03<00:05,  5.67it/s]
[36m(WorkerDict pid=2584103)[0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 22/51 [00:03<00:05,  5.67it/s]
[36m(WorkerDict pid=2584103)[0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 23/51 [00:03<00:05,  5.30it/s]
[36m(WorkerDict pid=2584103)[0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 24/51 [00:04<00:05,  5.14it/s]
[36m(WorkerDict pid=2584103)[0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 25/51 [00:04<00:05,  5.06it/s]
[36m(WorkerDict pid=2584103)[0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 26/51 [00:04<00:04,  5.05it/s]
[36m(WorkerDict pid=2584103)[0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 27/51 [00:04<00:05,  4.66it/s]
[36m(WorkerDict pid=2584103)[0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 28/51 [00:05<00:04,  4.83it/s]
[36m(WorkerDict pid=2584103)[0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 29/51 [00:05<00:04,  4.88it/s]
[36m(WorkerDict pid=2584103)[0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 30/51 [00:05<00:04,  4.84it/s]
[36m(WorkerDict pid=2584103)[0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 31/51 [00:05<00:04,  4.89it/s]
[36m(WorkerDict pid=2584103)[0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 32/51 [00:05<00:03,  4.98it/s]
[36m(WorkerDict pid=2584103)[0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 33/51 [00:06<00:03,  4.91it/s]
[36m(WorkerDict pid=2584103)[0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 34/51 [00:06<00:03,  5.05it/s]
[36m(WorkerDict pid=2584103)[0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 35/51 [00:06<00:03,  5.17it/s]
[36m(WorkerDict pid=2584103)[0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 36/51 [00:06<00:02,  5.20it/s]
[36m(WorkerDict pid=2584103)[0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 37/51 [00:06<00:02,  5.24it/s]
[36m(WorkerDict pid=2584103)[0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 38/51 [00:06<00:02,  5.16it/s]
[36m(WorkerDict pid=2584103)[0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 39/51 [00:07<00:02,  5.14it/s]
[36m(WorkerDict pid=2584103)[0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 40/51 [00:07<00:02,  5.06it/s]
[36m(WorkerDict pid=2584103)[0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 41/51 [00:07<00:01,  5.01it/s]
[36m(WorkerDict pid=2584103)[0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 42/51 [00:07<00:01,  4.96it/s]
[36m(WorkerDict pid=2584103)[0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 43/51 [00:08<00:01,  4.96it/s]
[36m(WorkerDict pid=2584103)[0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 44/51 [00:08<00:01,  4.82it/s]
[36m(WorkerDict pid=2584103)[0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 45/51 [00:08<00:01,  4.44it/s]
[36m(WorkerDict pid=2584103)[0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 46/51 [00:08<00:01,  3.95it/s]
[36m(WorkerDict pid=2584103)[0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 47/51 [00:09<00:00,  4.19it/s]
[36m(WorkerDict pid=2584103)[0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 48/51 [00:09<00:00,  4.37it/s]
[36m(WorkerDict pid=2584103)[0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 49/51 [00:09<00:00,  4.56it/s]
[36m(WorkerDict pid=2584103)[0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 50/51 [00:09<00:00,  4.68it/s]
[36m(WorkerDict pid=2584103)[0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 51/51 [00:09<00:00,  4.75it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 51/51 [00:09<00:00,  5.19it/s]
[36m(WorkerDict pid=2584103)[0m Capturing CUDA graphs (decode, FULL):   0%|          | 0/51 [00:00<?, ?it/s]
[36m(WorkerDict pid=2584103)[0m Capturing CUDA graphs (decode, FULL):   2%|â–         | 1/51 [00:00<00:06,  7.47it/s]
[36m(WorkerDict pid=2584103)[0m Capturing CUDA graphs (decode, FULL):   4%|â–         | 2/51 [00:00<00:06,  7.80it/s]
[36m(WorkerDict pid=2584103)[0m Capturing CUDA graphs (decode, FULL):   6%|â–Œ         | 3/51 [00:00<00:06,  7.96it/s]
[36m(WorkerDict pid=2584103)[0m Capturing CUDA graphs (decode, FULL):   8%|â–Š         | 4/51 [00:00<00:05,  8.02it/s]
[36m(WorkerDict pid=2584103)[0m Capturing CUDA graphs (decode, FULL):  10%|â–‰         | 5/51 [00:00<00:05,  8.05it/s]
[36m(WorkerDict pid=2584103)[0m Capturing CUDA graphs (decode, FULL):  12%|â–ˆâ–        | 6/51 [00:00<00:05,  8.15it/s]
[36m(WorkerDict pid=2584103)[0m Capturing CUDA graphs (decode, FULL):  14%|â–ˆâ–Ž        | 7/51 [00:00<00:05,  8.25it/s]
[36m(WorkerDict pid=2584103)[0m Capturing CUDA graphs (decode, FULL):  16%|â–ˆâ–Œ        | 8/51 [00:00<00:05,  8.19it/s]
[36m(WorkerDict pid=2584103)[0m Capturing CUDA graphs (decode, FULL):  18%|â–ˆâ–Š        | 9/51 [00:01<00:05,  8.35it/s]
[36m(WorkerDict pid=2584103)[0m Capturing CUDA graphs (decode, FULL):  20%|â–ˆâ–‰        | 10/51 [00:01<00:04,  8.45it/s]
[36m(WorkerDict pid=2584103)[0m Capturing CUDA graphs (decode, FULL):  22%|â–ˆâ–ˆâ–       | 11/51 [00:01<00:04,  8.48it/s]
[36m(WorkerDict pid=2584103)[0m Capturing CUDA graphs (decode, FULL):  24%|â–ˆâ–ˆâ–Ž       | 12/51 [00:01<00:04,  8.52it/s]
[36m(WorkerDict pid=2584103)[0m Capturing CUDA graphs (decode, FULL):  25%|â–ˆâ–ˆâ–Œ       | 13/51 [00:01<00:04,  8.62it/s]
[36m(WorkerDict pid=2584103)[0m Capturing CUDA graphs (decode, FULL):  27%|â–ˆâ–ˆâ–‹       | 14/51 [00:01<00:04,  8.65it/s]
[36m(WorkerDict pid=2584103)[0m Capturing CUDA graphs (decode, FULL):  29%|â–ˆâ–ˆâ–‰       | 15/51 [00:01<00:04,  8.68it/s]
[36m(WorkerDict pid=2584103)[0m Capturing CUDA graphs (decode, FULL):  31%|â–ˆâ–ˆâ–ˆâ–      | 16/51 [00:01<00:04,  8.69it/s]
[36m(WorkerDict pid=2584103)[0m Capturing CUDA graphs (decode, FULL):  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 17/51 [00:02<00:03,  8.72it/s]
[36m(WorkerDict pid=2584103)[0m Capturing CUDA graphs (decode, FULL):  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 18/51 [00:02<00:03,  8.72it/s]
[36m(WorkerDict pid=2584103)[0m Capturing CUDA graphs (decode, FULL):  37%|â–ˆâ–ˆâ–ˆâ–‹      | 19/51 [00:02<00:03,  8.66it/s]
[36m(WorkerDict pid=2584103)[0m Capturing CUDA graphs (decode, FULL):  39%|â–ˆâ–ˆâ–ˆâ–‰      | 20/51 [00:02<00:03,  8.57it/s]
[36m(WorkerDict pid=2584103)[0m Capturing CUDA graphs (decode, FULL):  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 21/51 [00:02<00:03,  8.55it/s]
[36m(WorkerDict pid=2584103)[0m Capturing CUDA graphs (decode, FULL):  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 22/51 [00:02<00:03,  8.61it/s]
[36m(WorkerDict pid=2584103)[0m Capturing CUDA graphs (decode, FULL):  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 23/51 [00:02<00:03,  8.53it/s]
[36m(WorkerDict pid=2584103)[0m Capturing CUDA graphs (decode, FULL):  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 24/51 [00:02<00:03,  8.53it/s]
[36m(WorkerDict pid=2584103)[0m Capturing CUDA graphs (decode, FULL):  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 25/51 [00:02<00:03,  8.55it/s]
[36m(WorkerDict pid=2584103)[0m Capturing CUDA graphs (decode, FULL):  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 26/51 [00:03<00:02,  8.57it/s]
[36m(WorkerDict pid=2584103)[0m Capturing CUDA graphs (decode, FULL):  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 27/51 [00:03<00:02,  8.59it/s]
[36m(WorkerDict pid=2584103)[0m Capturing CUDA graphs (decode, FULL):  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 28/51 [00:03<00:02,  8.62it/s]
[36m(WorkerDict pid=2584103)[0m Capturing CUDA graphs (decode, FULL):  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 29/51 [00:03<00:02,  8.65it/s]
[36m(WorkerDict pid=2584103)[0m Capturing CUDA graphs (decode, FULL):  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 30/51 [00:03<00:02,  8.61it/s]
[36m(WorkerDict pid=2584103)[0m Capturing CUDA graphs (decode, FULL):  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 31/51 [00:03<00:02,  8.65it/s]
[36m(WorkerDict pid=2584103)[0m Capturing CUDA graphs (decode, FULL):  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 32/51 [00:03<00:02,  8.65it/s]
[36m(WorkerDict pid=2584103)[0m Capturing CUDA graphs (decode, FULL):  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 33/51 [00:03<00:02,  7.83it/s]
[36m(WorkerDict pid=2584103)[0m Capturing CUDA graphs (decode, FULL):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 34/51 [00:04<00:02,  5.97it/s]
[36m(WorkerDict pid=2584103)[0m Capturing CUDA graphs (decode, FULL):  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 35/51 [00:04<00:02,  6.57it/s]
[36m(WorkerDict pid=2584103)[0m Capturing CUDA graphs (decode, FULL):  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 36/51 [00:04<00:02,  7.09it/s]
[36m(WorkerDict pid=2584103)[0m Capturing CUDA graphs (decode, FULL):  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 37/51 [00:04<00:01,  7.56it/s]
[36m(WorkerDict pid=2584103)[0m Capturing CUDA graphs (decode, FULL):  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 38/51 [00:04<00:01,  7.80it/s]
[36m(WorkerDict pid=2584103)[0m Capturing CUDA graphs (decode, FULL):  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 39/51 [00:04<00:01,  7.89it/s]
[36m(WorkerDict pid=2584103)[0m Capturing CUDA graphs (decode, FULL):  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 40/51 [00:04<00:01,  7.93it/s]
[36m(WorkerDict pid=2584103)[0m Capturing CUDA graphs (decode, FULL):  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 41/51 [00:05<00:01,  7.93it/s]
[36m(WorkerDict pid=2584103)[0m Capturing CUDA graphs (decode, FULL):  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 42/51 [00:05<00:01,  7.87it/s]
[36m(WorkerDict pid=2584103)[0m Capturing CUDA graphs (decode, FULL):  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 43/51 [00:05<00:01,  7.90it/s]
[36m(WorkerDict pid=2584103)[0m Capturing CUDA graphs (decode, FULL):  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 44/51 [00:05<00:00,  8.06it/s]
[36m(WorkerDict pid=2584103)[0m Capturing CUDA graphs (decode, FULL):  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 45/51 [00:05<00:00,  8.26it/s]
[36m(WorkerDict pid=2584103)[0m Capturing CUDA graphs (decode, FULL):  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 46/51 [00:05<00:00,  8.22it/s]
[36m(WorkerDict pid=2584103)[0m Capturing CUDA graphs (decode, FULL):  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 47/51 [00:05<00:00,  8.27it/s]
[36m(WorkerDict pid=2584103)[0m Capturing CUDA graphs (decode, FULL):  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 48/51 [00:05<00:00,  8.26it/s]
[36m(WorkerDict pid=2584103)[0m Capturing CUDA graphs (decode, FULL):  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 49/51 [00:06<00:00,  8.01it/s]
[36m(WorkerDict pid=2584103)[0m Capturing CUDA graphs (decode, FULL):  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 50/51 [00:06<00:00,  7.56it/s]
[36m(WorkerDict pid=2584103)[0m Capturing CUDA graphs (decode, FULL): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 51/51 [00:06<00:00,  7.73it/s]Capturing CUDA graphs (decode, FULL): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 51/51 [00:06<00:00,  8.12it/s]
[36m(WorkerDict pid=2584103)[0m /home/jw4199/miniconda3/envs/ouro_vllm/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:675: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
[36m(WorkerDict pid=2584103)[0m   warnings.warn(
[36m(WorkerDict pid=2584104)[0m /home/jw4199/miniconda3/envs/ouro_vllm/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:675: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
[36m(WorkerDict pid=2584104)[0m   warnings.warn(
[36m(WorkerDict pid=2584105)[0m /home/jw4199/miniconda3/envs/ouro_vllm/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:675: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
[36m(WorkerDict pid=2584105)[0m   warnings.warn(
[36m(WorkerDict pid=2584106)[0m /home/jw4199/miniconda3/envs/ouro_vllm/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:675: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
[36m(WorkerDict pid=2584106)[0m   warnings.warn(
wandb: Tracking run with wandb version 0.23.1
wandb: W&B syncing is set to `offline` in this directory. Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
wandb: Run data is saved locally in /scratch/gpfs/OLGARUS/jw4199/model_weights_path/wandb/offline-run-20260215_032825-2m78zb68
2026-02-15 03:28:27,061 - INFO - Gradient accumulation enabled: accumulating 2 rollout batches per update
Training Progress:   0%|          | 0/3 [00:00<?, ?it/s]2026-02-15 03:29:21,521 - INFO - Step 1 (accum 1/2): rollout done in 54.3s
2026-02-15 03:29:24,557 - INFO - Truncated 13/32 responses at first \boxed{}. Mean tokens dropped: 130.6
Training Progress:   0%|          | 0/3 [01:00<?, ?it/s, rollout=54.3s, backprop=0.0s, accum=1/2]Training Progress:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [01:00<02:00, 60.41s/it, rollout=54.3s, backprop=0.0s, accum=1/2][36m(WorkerDict pid=2584104)[0m INFO:2026-02-15 03:29:27,433:[Rank 1] Saved model to /scratch/gpfs/OLGARUS/jw4199/distillation/LLOPSD/llopsd_experiments/llopsd_output/4753147/global_step_1/actor/model_world_size_4_rank_1.pt
[36m(WorkerDict pid=2584104)[0m INFO:2026-02-15 03:29:27,437:[Rank 1] Saved optim to /scratch/gpfs/OLGARUS/jw4199/distillation/LLOPSD/llopsd_experiments/llopsd_output/4753147/global_step_1/actor/optim_world_size_4_rank_1.pt
[36m(WorkerDict pid=2584104)[0m INFO:2026-02-15 03:29:27,438:[Rank 1] Saved extra_state to /scratch/gpfs/OLGARUS/jw4199/distillation/LLOPSD/llopsd_experiments/llopsd_output/4753147/global_step_1/actor/extra_state_world_size_4_rank_1.pt
[36m(WorkerDict pid=2584103)[0m INFO:2026-02-15 03:29:27,411:[Rank 0] Saved model to /scratch/gpfs/OLGARUS/jw4199/distillation/LLOPSD/llopsd_experiments/llopsd_output/4753147/global_step_1/actor/model_world_size_4_rank_0.pt
[36m(WorkerDict pid=2584103)[0m INFO:2026-02-15 03:29:27,414:[Rank 0] Saved optim to /scratch/gpfs/OLGARUS/jw4199/distillation/LLOPSD/llopsd_experiments/llopsd_output/4753147/global_step_1/actor/optim_world_size_4_rank_0.pt
[36m(WorkerDict pid=2584103)[0m INFO:2026-02-15 03:29:27,416:[Rank 0] Saved extra_state to /scratch/gpfs/OLGARUS/jw4199/distillation/LLOPSD/llopsd_experiments/llopsd_output/4753147/global_step_1/actor/extra_state_world_size_4_rank_0.pt
[36m(WorkerDict pid=2584103)[0m INFO:2026-02-15 03:29:27,460:[Rank 0] Saved model config and tokenizer class to /scratch/gpfs/OLGARUS/jw4199/distillation/LLOPSD/llopsd_experiments/llopsd_output/4753147/global_step_1/actor/huggingface
[36m(WorkerDict pid=2584106)[0m INFO:2026-02-15 03:29:27,428:[Rank 3] Saved model to /scratch/gpfs/OLGARUS/jw4199/distillation/LLOPSD/llopsd_experiments/llopsd_output/4753147/global_step_1/actor/model_world_size_4_rank_3.pt
[36m(WorkerDict pid=2584106)[0m INFO:2026-02-15 03:29:27,432:[Rank 3] Saved optim to /scratch/gpfs/OLGARUS/jw4199/distillation/LLOPSD/llopsd_experiments/llopsd_output/4753147/global_step_1/actor/optim_world_size_4_rank_3.pt
[36m(WorkerDict pid=2584106)[0m INFO:2026-02-15 03:29:27,434:[Rank 3] Saved extra_state to /scratch/gpfs/OLGARUS/jw4199/distillation/LLOPSD/llopsd_experiments/llopsd_output/4753147/global_step_1/actor/extra_state_world_size_4_rank_3.pt
[36m(WorkerDict pid=2584105)[0m INFO:2026-02-15 03:29:27,396:[Rank 2] Saved model to /scratch/gpfs/OLGARUS/jw4199/distillation/LLOPSD/llopsd_experiments/llopsd_output/4753147/global_step_1/actor/model_world_size_4_rank_2.pt
[36m(WorkerDict pid=2584105)[0m INFO:2026-02-15 03:29:27,400:[Rank 2] Saved optim to /scratch/gpfs/OLGARUS/jw4199/distillation/LLOPSD/llopsd_experiments/llopsd_output/4753147/global_step_1/actor/optim_world_size_4_rank_2.pt
[36m(WorkerDict pid=2584105)[0m INFO:2026-02-15 03:29:27,401:[Rank 2] Saved extra_state to /scratch/gpfs/OLGARUS/jw4199/distillation/LLOPSD/llopsd_experiments/llopsd_output/4753147/global_step_1/actor/extra_state_world_size_4_rank_2.pt
2026-02-15 03:30:06,351 - INFO - Step 2 (accum 2/2): rollout done in 38.9s
2026-02-15 03:30:08,991 - INFO - Truncated 16/32 responses at first \boxed{}. Mean tokens dropped: 185.8
2026-02-15 03:30:51,882 - INFO - Step 2: backprop done in 42.9s (accumulated 2 batches, 64 samples, total rollout time: 93.2s)
[36m(WorkerDict pid=2584104)[0m INFO:2026-02-15 03:30:54,699:[Rank 1] Saved model to /scratch/gpfs/OLGARUS/jw4199/distillation/LLOPSD/llopsd_experiments/llopsd_output/4753147/global_step_2/actor/model_world_size_4_rank_1.pt
[36m(WorkerDict pid=2584106)[0m INFO:2026-02-15 03:30:54,759:[Rank 3] Saved model to /scratch/gpfs/OLGARUS/jw4199/distillation/LLOPSD/llopsd_experiments/llopsd_output/4753147/global_step_2/actor/model_world_size_4_rank_3.pt
[36m(WorkerDict pid=2584105)[0m INFO:2026-02-15 03:30:54,677:[Rank 2] Saved model to /scratch/gpfs/OLGARUS/jw4199/distillation/LLOPSD/llopsd_experiments/llopsd_output/4753147/global_step_2/actor/model_world_size_4_rank_2.pt
[36m(WorkerDict pid=2584103)[0m INFO:2026-02-15 03:30:54,774:[Rank 0] Saved model to /scratch/gpfs/OLGARUS/jw4199/distillation/LLOPSD/llopsd_experiments/llopsd_output/4753147/global_step_2/actor/model_world_size_4_rank_0.pt
[36m(WorkerDict pid=2584105)[0m INFO:2026-02-15 03:30:55,872:[Rank 2] Saved optim to /scratch/gpfs/OLGARUS/jw4199/distillation/LLOPSD/llopsd_experiments/llopsd_output/4753147/global_step_2/actor/optim_world_size_4_rank_2.pt
[36m(WorkerDict pid=2584105)[0m INFO:2026-02-15 03:30:55,874:[Rank 2] Saved extra_state to /scratch/gpfs/OLGARUS/jw4199/distillation/LLOPSD/llopsd_experiments/llopsd_output/4753147/global_step_2/actor/extra_state_world_size_4_rank_2.pt
[36m(WorkerDict pid=2584104)[0m INFO:2026-02-15 03:30:55,913:[Rank 1] Saved optim to /scratch/gpfs/OLGARUS/jw4199/distillation/LLOPSD/llopsd_experiments/llopsd_output/4753147/global_step_2/actor/optim_world_size_4_rank_1.pt
[36m(WorkerDict pid=2584104)[0m INFO:2026-02-15 03:30:55,915:[Rank 1] Saved extra_state to /scratch/gpfs/OLGARUS/jw4199/distillation/LLOPSD/llopsd_experiments/llopsd_output/4753147/global_step_2/actor/extra_state_world_size_4_rank_1.pt
[36m(WorkerDict pid=2584103)[0m INFO:2026-02-15 03:30:55,983:[Rank 0] Saved optim to /scratch/gpfs/OLGARUS/jw4199/distillation/LLOPSD/llopsd_experiments/llopsd_output/4753147/global_step_2/actor/optim_world_size_4_rank_0.pt
[36m(WorkerDict pid=2584103)[0m INFO:2026-02-15 03:30:55,984:[Rank 0] Saved extra_state to /scratch/gpfs/OLGARUS/jw4199/distillation/LLOPSD/llopsd_experiments/llopsd_output/4753147/global_step_2/actor/extra_state_world_size_4_rank_0.pt
Training Progress:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [02:28<02:00, 60.41s/it, rollout=38.9s, backprop=42.9s, accum=0/2]Training Progress:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [02:28<01:16, 76.97s/it, rollout=38.9s, backprop=42.9s, accum=0/2][36m(WorkerDict pid=2584103)[0m INFO:2026-02-15 03:30:56,026:[Rank 0] Saved model config and tokenizer class to /scratch/gpfs/OLGARUS/jw4199/distillation/LLOPSD/llopsd_experiments/llopsd_output/4753147/global_step_2/actor/huggingface
[36m(WorkerDict pid=2584106)[0m INFO:2026-02-15 03:30:56,020:[Rank 3] Saved optim to /scratch/gpfs/OLGARUS/jw4199/distillation/LLOPSD/llopsd_experiments/llopsd_output/4753147/global_step_2/actor/optim_world_size_4_rank_3.pt
[36m(WorkerDict pid=2584106)[0m INFO:2026-02-15 03:30:56,022:[Rank 3] Saved extra_state to /scratch/gpfs/OLGARUS/jw4199/distillation/LLOPSD/llopsd_experiments/llopsd_output/4753147/global_step_2/actor/extra_state_world_size_4_rank_3.pt
2026-02-15 03:31:35,488 - INFO - Step 3 (accum 1/2): rollout done in 39.4s
2026-02-15 03:31:38,523 - INFO - Truncated 19/32 responses at first \boxed{}. Mean tokens dropped: 82.5
2026-02-15 03:31:59,423 - INFO - Step 3: backprop done in 20.9s (accumulated 1 batches, 32 samples, total rollout time: 39.4s)
[36m(WorkerDict pid=2584104)[0m INFO:2026-02-15 03:32:02,139:[Rank 1] Saved model to /scratch/gpfs/OLGARUS/jw4199/distillation/LLOPSD/llopsd_experiments/llopsd_output/4753147/global_step_3/actor/model_world_size_4_rank_1.pt
[36m(WorkerDict pid=2584106)[0m INFO:2026-02-15 03:32:02,187:[Rank 3] Saved model to /scratch/gpfs/OLGARUS/jw4199/distillation/LLOPSD/llopsd_experiments/llopsd_output/4753147/global_step_3/actor/model_world_size_4_rank_3.pt
[36m(WorkerDict pid=2584103)[0m INFO:2026-02-15 03:32:02,266:[Rank 0] Saved model to /scratch/gpfs/OLGARUS/jw4199/distillation/LLOPSD/llopsd_experiments/llopsd_output/4753147/global_step_3/actor/model_world_size_4_rank_0.pt
[36m(WorkerDict pid=2584105)[0m INFO:2026-02-15 03:32:02,288:[Rank 2] Saved model to /scratch/gpfs/OLGARUS/jw4199/distillation/LLOPSD/llopsd_experiments/llopsd_output/4753147/global_step_3/actor/model_world_size_4_rank_2.pt
[36m(WorkerDict pid=2584104)[0m INFO:2026-02-15 03:32:03,206:[Rank 1] Saved optim to /scratch/gpfs/OLGARUS/jw4199/distillation/LLOPSD/llopsd_experiments/llopsd_output/4753147/global_step_3/actor/optim_world_size_4_rank_1.pt
[36m(WorkerDict pid=2584104)[0m INFO:2026-02-15 03:32:03,208:[Rank 1] Saved extra_state to /scratch/gpfs/OLGARUS/jw4199/distillation/LLOPSD/llopsd_experiments/llopsd_output/4753147/global_step_3/actor/extra_state_world_size_4_rank_1.pt
[36m(WorkerDict pid=2584103)[0m INFO:2026-02-15 03:32:03,293:[Rank 0] Saved optim to /scratch/gpfs/OLGARUS/jw4199/distillation/LLOPSD/llopsd_experiments/llopsd_output/4753147/global_step_3/actor/optim_world_size_4_rank_0.pt
[36m(WorkerDict pid=2584103)[0m INFO:2026-02-15 03:32:03,294:[Rank 0] Saved extra_state to /scratch/gpfs/OLGARUS/jw4199/distillation/LLOPSD/llopsd_experiments/llopsd_output/4753147/global_step_3/actor/extra_state_world_size_4_rank_0.pt
[36m(WorkerDict pid=2584103)[0m INFO:2026-02-15 03:32:03,338:[Rank 0] Saved model config and tokenizer class to /scratch/gpfs/OLGARUS/jw4199/distillation/LLOPSD/llopsd_experiments/llopsd_output/4753147/global_step_3/actor/huggingface
[36m(WorkerDict pid=2584105)[0m INFO:2026-02-15 03:32:03,315:[Rank 2] Saved optim to /scratch/gpfs/OLGARUS/jw4199/distillation/LLOPSD/llopsd_experiments/llopsd_output/4753147/global_step_3/actor/optim_world_size_4_rank_2.pt
[36m(WorkerDict pid=2584105)[0m INFO:2026-02-15 03:32:03,317:[Rank 2] Saved extra_state to /scratch/gpfs/OLGARUS/jw4199/distillation/LLOPSD/llopsd_experiments/llopsd_output/4753147/global_step_3/actor/extra_state_world_size_4_rank_2.pt
Training Progress:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [03:36<01:16, 76.97s/it, rollout=39.4s, backprop=20.9s, accum=0/2]Training Progress: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [03:36<00:00, 72.63s/it, rollout=39.4s, backprop=20.9s, accum=0/2]2026-02-15 03:32:03,511 - INFO - Epoch 1 Timing Summary:
2026-02-15 03:32:03,511 - INFO -   Total rollout time: 132.6s
2026-02-15 03:32:03,511 - INFO -   Total backprop time: 63.8s
2026-02-15 03:32:03,512 - INFO - Final validation metrics: None
Training Progress: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [03:36<00:00, 72.15s/it, rollout=39.4s, backprop=20.9s, accum=0/2]
2026-02-15 03:32:03,512 - INFO - LLOPSD verl training completed successfully!
2026-02-15 03:32:03,513 - INFO - Output directory: ./llopsd_output/4753147
Exception ignored in: <function Tracking.__del__ at 0x14aaf4f33910>
2026-02-15 03:32:03,571 - WARNING - socket.send() raised exception.
Traceback (most recent call last):
  File "/home/jw4199/miniconda3/envs/ouro_vllm/lib/python3.10/site-packages/verl/utils/tracking.py", line 160, in __del__
2026-02-15 03:32:03,571 - WARNING - socket.send() raised exception.
[36m(WorkerDict pid=2584106)[0m INFO:2026-02-15 03:32:03,491:[Rank 3] Saved optim to /scratch/gpfs/OLGARUS/jw4199/distillation/LLOPSD/llopsd_experiments/llopsd_output/4753147/global_step_3/actor/optim_world_size_4_rank_3.pt
[36m(WorkerDict pid=2584106)[0m INFO:2026-02-15 03:32:03,493:[Rank 3] Saved extra_state to /scratch/gpfs/OLGARUS/jw4199/distillation/LLOPSD/llopsd_experiments/llopsd_output/4753147/global_step_3/actor/extra_state_world_size_4_rank_3.pt
    self.logger["wandb"].finish(exit_code=0)
  File "/home/jw4199/miniconda3/envs/ouro_vllm/lib/python3.10/site-packages/wandb/sdk/wandb_run.py", line 4097, in finish
    wandb.run.finish(exit_code=exit_code, quiet=quiet)
  File "/home/jw4199/miniconda3/envs/ouro_vllm/lib/python3.10/site-packages/wandb/sdk/wandb_run.py", line 400, in wrapper
    return func(self, *args, **kwargs)
  File "/home/jw4199/miniconda3/envs/ouro_vllm/lib/python3.10/site-packages/wandb/sdk/wandb_run.py", line 445, in wrapper
    return func(self, *args, **kwargs)
  File "/home/jw4199/miniconda3/envs/ouro_vllm/lib/python3.10/site-packages/wandb/sdk/wandb_run.py", line 2275, in finish
    return self._finish(exit_code)
  File "/home/jw4199/miniconda3/envs/ouro_vllm/lib/python3.10/site-packages/wandb/sdk/wandb_run.py", line 400, in wrapper
    return func(self, *args, **kwargs)
  File "/home/jw4199/miniconda3/envs/ouro_vllm/lib/python3.10/site-packages/wandb/sdk/wandb_run.py", line 2288, in _finish
    with telemetry.context(run=self) as tel:
  File "/home/jw4199/miniconda3/envs/ouro_vllm/lib/python3.10/site-packages/wandb/sdk/lib/telemetry.py", line 42, in __exit__
    self._run._telemetry_callback(self._obj)
  File "/home/jw4199/miniconda3/envs/ouro_vllm/lib/python3.10/site-packages/wandb/sdk/wandb_run.py", line 752, in _telemetry_callback
    self._telemetry_flush()
  File "/home/jw4199/miniconda3/envs/ouro_vllm/lib/python3.10/site-packages/wandb/sdk/wandb_run.py", line 765, in _telemetry_flush
    self._backend.interface._publish_telemetry(self._telemetry_obj)
  File "/home/jw4199/miniconda3/envs/ouro_vllm/lib/python3.10/site-packages/wandb/sdk/interface/interface_shared.py", line 103, in _publish_telemetry
    self._publish(rec)
  File "/home/jw4199/miniconda3/envs/ouro_vllm/lib/python3.10/site-packages/wandb/sdk/interface/interface_sock.py", line 46, in _publish
    self._asyncer.run(lambda: self._client.publish(request))
  File "/home/jw4199/miniconda3/envs/ouro_vllm/lib/python3.10/site-packages/wandb/sdk/lib/asyncio_manager.py", line 136, in run
    return future.result()
  File "/home/jw4199/miniconda3/envs/ouro_vllm/lib/python3.10/concurrent/futures/_base.py", line 458, in result
2026-02-15 03:32:03,621 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,621 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,621 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,621 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,621 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,621 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,621 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,621 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,622 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,622 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,622 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,622 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,633 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,633 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,633 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,633 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,633 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,633 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,633 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,633 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,633 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,634 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,634 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,634 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,634 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,634 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,634 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,634 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,634 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,634 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,634 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,634 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,634 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,634 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,634 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,634 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,634 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,634 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,634 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,634 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,634 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,634 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,634 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,634 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,634 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,635 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,635 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,635 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,635 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,635 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,635 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,635 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,635 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,635 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,635 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,635 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,635 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,635 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,635 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,635 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,635 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,635 - WARNING - socket.send() raised exception.
    return self.__get_result()
  File "/home/jw4199/miniconda3/envs/ouro_vllm/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
2026-02-15 03:32:03,635 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,635 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,635 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,635 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,635 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,635 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,635 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,635 - WARNING - socket.send() raised exception.
    raise self._exception
  File "/home/jw4199/miniconda3/envs/ouro_vllm/lib/python3.10/site-packages/wandb/sdk/lib/asyncio_manager.py", line 219, in _wrap
2026-02-15 03:32:03,636 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,636 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,636 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,636 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,636 - WARNING - socket.send() raised exception.
    return await fn()
  File "/home/jw4199/miniconda3/envs/ouro_vllm/lib/python3.10/site-packages/wandb/sdk/lib/service/service_client.py", line 38, in publish
2026-02-15 03:32:03,636 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,636 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,636 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,636 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,636 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,637 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,637 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,637 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,637 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,637 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,637 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,637 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,637 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,637 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,637 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,637 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,637 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,637 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,637 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,637 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,637 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,637 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,637 - WARNING - socket.send() raised exception.
    await self._send_server_request(request)
  File "/home/jw4199/miniconda3/envs/ouro_vllm/lib/python3.10/site-packages/wandb/sdk/lib/service/service_client.py", line 64, in _send_server_request
    await self._writer.drain()
  File "/home/jw4199/miniconda3/envs/ouro_vllm/lib/python3.10/asyncio/streams.py", line 371, in drain
    await self._protocol._drain_helper()
2026-02-15 03:32:03,638 - WARNING - socket.send() raised exception.
  File "/home/jw4199/miniconda3/envs/ouro_vllm/lib/python3.10/asyncio/streams.py", line 167, in _drain_helper
    raise ConnectionResetError('Connection lost')
2026-02-15 03:32:03,638 - WARNING - socket.send() raised exception.
ConnectionResetError: Connection lost
2026-02-15 03:32:03,639 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,639 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,639 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,639 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,639 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,639 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,639 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,639 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,639 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,639 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,639 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,639 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,639 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,639 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,639 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,639 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,647 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,647 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,649 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,649 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,649 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,649 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,649 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,649 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,649 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,649 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,649 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,649 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,649 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,649 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,649 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,649 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,649 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,649 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,649 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,649 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,649 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,649 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,649 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,649 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,649 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,649 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,650 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,650 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,650 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,650 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,650 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,650 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,650 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,650 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,650 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,650 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,650 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,650 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,650 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,650 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,650 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,650 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,650 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,650 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,650 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,650 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,650 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,650 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,650 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,650 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,650 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,650 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,650 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,650 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,650 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,650 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,650 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,650 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,650 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,651 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,651 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,651 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,651 - WARNING - socket.send() raised exception.
2026-02-15 03:32:03,651 - WARNING - socket.send() raised exception.
