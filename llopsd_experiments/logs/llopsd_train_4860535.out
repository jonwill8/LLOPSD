==========================================
LLOPSD Training - Loop-to-Loop On-Policy Self-Distillation
==========================================
Job ID: 4860535
Node: della-i22g2
Start time: Wed Feb 18 02:50:34 AM EST 2026

==========================================
*** TRAINING MODE: FULL PARAMETER FINE-TUNING ***
==========================================

Python: /home/jw4199/miniconda3/envs/ouro_vllm/bin/python
PyTorch version: 2.9.0+cu128
Transformers version: 4.57.3
CUDA available: True
GPU count: 4
GPU name: NVIDIA H200

vLLM: available
verl: available

==========================================
LLOPSD Configuration
==========================================

LLOPSD-Specific Settings:
  Teacher loops (R): 4
  Student loops (R~): 2
  Loop mapping: shift
  Weight schedule: uniform
  Weight gamma: 1.0
  Divergence: jsd
  Teacher context: opsd
  Teacher mode: ema
  EMA decay: 0.99
  Total UT steps: 4

Model Settings:
  Model path: /scratch/gpfs/OLGARUS/jw4199/model_weights_path/Ouro-2.6B-Thinking

Data Settings:
  Train file: /scratch/gpfs/OLGARUS/jw4199/datasets/MATH/math_train.jsonl
  Output dir: ./llopsd_output/4860535

RL Hyperparameters:
  Num generations per prompt: 1
  Num prompts per batch: 32
  Temperature: 1

Training Hyperparameters:
  Learning rate: 1e-6
  LR scheduler: constant
  Warmup ratio: 0.1
  Weight decay: 0.1
  Max grad norm: 0.1
  Epochs: 1
  Total batch size: 32 (32 prompts x 1 gens)
  Dynamic batch sizing: enabled (ppo_max_token_len=4096)
  Log-prob max token len: 4096

Sequence Lengths:
  Max prompt length: 1024
  Max completion length: 2048

==========================================
Training Mode Configuration:
==========================================
  *** TRAINING MODE: FULL PARAMETER FINE-TUNING ***
  All model parameters will be updated
  Note: Requires more GPU memory than LoRA

Multi-GPU Configuration:
  Number of GPUs: 4
  Number of nodes: 1
  vLLM TP size: 1

vLLM Configuration:
  Use vLLM: true
  GPU memory utilization: 0.45

SFT Checkpoint:
  Use SFT checkpoint: false
  Starting from fresh model weights (no SFT initialization)

==========================================

LLOPSD Loop Mapping & Weight Schedule:
=======================================

Loop Mapping (shift):
  Using SHIFT mapping: student loop s maps to teacher loop s+1
  Student R~=2 loops learn from teacher R=4 loops
  Each student loop distills from one loop ahead in the teacher

Weight Schedule (uniform):
  Using UNIFORM weights: all loop pairs contribute equally

Divergence (jsd):
  Using Jensen-Shannon Divergence: symmetric combination

Training Progress Guide:
========================
  Dataset size: 7500 prompts
  Prompts per step: 32
  Steps per epoch: 234
  Total steps: 234 (over 1 epoch(s))

Starting LLOPSD training...

Using dataset class: RLHFDataset
dataset len: 7500
filter dataset len: 7481
Using dataset class: RLHFDataset
dataset len: 7500
filter dataset len: 7481
Size of train dataloader: 233, Size of val dataloader: 234
Total training steps: 250
colocated worker base class <class 'verl.single_controller.base.worker.Worker'>
[36m(WorkerDict pid=4158405)[0m [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[36m(WorkerDict pid=4158407)[0m [Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[36m(WorkerDict pid=4158404)[0m [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[36m(WorkerDict pid=4158406)[0m [Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[36m(WorkerDict pid=4158405)[0m [LLOPSD WORKER DEBUG] init_model() called on rank 1
[36m(WorkerDict pid=4158407)[0m [LLOPSD WORKER DEBUG] init_model() called on rank 3
[36m(WorkerDict pid=4158404)[0m LLOPSD Worker initialized:
[36m(WorkerDict pid=4158404)[0m   teacher_loops: 4
[36m(WorkerDict pid=4158404)[0m   student_loops: 2
[36m(WorkerDict pid=4158404)[0m   loop_mapping_strategy: shift
[36m(WorkerDict pid=4158404)[0m   weight_schedule: uniform
[36m(WorkerDict pid=4158404)[0m   weight_gamma: 1.0
[36m(WorkerDict pid=4158404)[0m   divergence_type: forward_kl
[36m(WorkerDict pid=4158404)[0m   teacher_context: opsd
[36m(WorkerDict pid=4158404)[0m   teacher_mode: ema
[36m(WorkerDict pid=4158404)[0m   ema_decay: 0.99
[36m(WorkerDict pid=4158404)[0m [LLOPSD WORKER DEBUG] init_model() called on rank 0
[36m(WorkerDict pid=4158406)[0m [LLOPSD WORKER DEBUG] init_model() called on rank 2
[36m(WorkerDict pid=4158404)[0m Model config after override: OuroConfig {
[36m(WorkerDict pid=4158404)[0m   "architectures": [
[36m(WorkerDict pid=4158404)[0m     "OuroForCausalLM"
[36m(WorkerDict pid=4158404)[0m   ],
[36m(WorkerDict pid=4158404)[0m   "attention_dropout": 0.0,
[36m(WorkerDict pid=4158404)[0m   "auto_map": {
[36m(WorkerDict pid=4158404)[0m     "AutoConfig": "configuration_ouro.OuroConfig",
[36m(WorkerDict pid=4158404)[0m     "AutoModel": "modeling_ouro.OuroModel",
[36m(WorkerDict pid=4158404)[0m     "AutoModelForCausalLM": "modeling_ouro.OuroForCausalLM"
[36m(WorkerDict pid=4158404)[0m   },
[36m(WorkerDict pid=4158404)[0m   "bos_token_id": 0,
[36m(WorkerDict pid=4158404)[0m   "dtype": "bfloat16",
[36m(WorkerDict pid=4158404)[0m   "early_exit_threshold": 1.0,
[36m(WorkerDict pid=4158404)[0m   "eos_token_id": 0,
[36m(WorkerDict pid=4158404)[0m   "head_dim": 128,
[36m(WorkerDict pid=4158404)[0m   "hidden_act": "silu",
[36m(WorkerDict pid=4158404)[0m   "hidden_size": 2048,
[36m(WorkerDict pid=4158404)[0m   "initializer_range": 0.02,
[36m(WorkerDict pid=4158404)[0m   "intermediate_size": 5632,
[36m(WorkerDict pid=4158404)[0m   "layer_types": [
[36m(WorkerDict pid=4158404)[0m     "full_attention",
[36m(WorkerDict pid=4158404)[0m     "full_attention",
[36m(WorkerDict pid=4158404)[0m     "full_attention",
[36m(WorkerDict pid=4158404)[0m     "full_attention",
[36m(WorkerDict pid=4158404)[0m     "full_attention",
[36m(WorkerDict pid=4158404)[0m     "full_attention",
[36m(WorkerDict pid=4158404)[0m     "full_attention",
[36m(WorkerDict pid=4158404)[0m     "full_attention",
[36m(WorkerDict pid=4158404)[0m     "full_attention",
[36m(WorkerDict pid=4158404)[0m     "full_attention",
[36m(WorkerDict pid=4158404)[0m     "full_attention",
[36m(WorkerDict pid=4158404)[0m     "full_attention",
[36m(WorkerDict pid=4158404)[0m     "full_attention",
[36m(WorkerDict pid=4158404)[0m     "full_attention",
[36m(WorkerDict pid=4158404)[0m     "full_attention",
[36m(WorkerDict pid=4158404)[0m     "full_attention",
[36m(WorkerDict pid=4158404)[0m     "full_attention",
[36m(WorkerDict pid=4158404)[0m     "full_attention",
[36m(WorkerDict pid=4158404)[0m     "full_attention",
[36m(WorkerDict pid=4158404)[0m     "full_attention",
[36m(WorkerDict pid=4158404)[0m     "full_attention",
[36m(WorkerDict pid=4158404)[0m     "full_attention",
[36m(WorkerDict pid=4158404)[0m     "full_attention",
[36m(WorkerDict pid=4158404)[0m     "full_attention",
[36m(WorkerDict pid=4158404)[0m     "full_attention",
[36m(WorkerDict pid=4158404)[0m     "full_attention",
[36m(WorkerDict pid=4158404)[0m     "full_attention",
[36m(WorkerDict pid=4158404)[0m     "full_attention",
[36m(WorkerDict pid=4158404)[0m     "full_attention",
[36m(WorkerDict pid=4158404)[0m     "full_attention",
[36m(WorkerDict pid=4158404)[0m     "full_attention",
[36m(WorkerDict pid=4158404)[0m     "full_attention",
[36m(WorkerDict pid=4158404)[0m     "full_attention",
[36m(WorkerDict pid=4158404)[0m     "full_attention",
[36m(WorkerDict pid=4158404)[0m     "full_attention",
[36m(WorkerDict pid=4158404)[0m     "full_attention",
[36m(WorkerDict pid=4158404)[0m     "full_attention",
[36m(WorkerDict pid=4158404)[0m     "full_attention",
[36m(WorkerDict pid=4158404)[0m     "full_attention",
[36m(WorkerDict pid=4158404)[0m     "full_attention",
[36m(WorkerDict pid=4158404)[0m     "full_attention",
[36m(WorkerDict pid=4158404)[0m     "full_attention",
[36m(WorkerDict pid=4158404)[0m     "full_attention",
[36m(WorkerDict pid=4158404)[0m     "full_attention",
[36m(WorkerDict pid=4158404)[0m     "full_attention",
[36m(WorkerDict pid=4158404)[0m     "full_attention",
[36m(WorkerDict pid=4158404)[0m     "full_attention",
[36m(WorkerDict pid=4158404)[0m     "full_attention"
[36m(WorkerDict pid=4158404)[0m   ],
[36m(WorkerDict pid=4158404)[0m   "max_position_embeddings": 65536,
[36m(WorkerDict pid=4158404)[0m   "max_window_layers": 48,
[36m(WorkerDict pid=4158404)[0m   "model_type": "ouro",
[36m(WorkerDict pid=4158404)[0m   "num_attention_heads": 16,
[36m(WorkerDict pid=4158404)[0m   "num_hidden_layers": 48,
[36m(WorkerDict pid=4158404)[0m   "num_key_value_heads": 16,
[36m(WorkerDict pid=4158404)[0m   "pad_token_id": 0,
[36m(WorkerDict pid=4158404)[0m   "rms_norm_eps": 1e-06,
[36m(WorkerDict pid=4158404)[0m   "rope_scaling": null,
[36m(WorkerDict pid=4158404)[0m   "rope_theta": 1000000.0,
[36m(WorkerDict pid=4158404)[0m   "sliding_window": null,
[36m(WorkerDict pid=4158404)[0m   "tie_word_embeddings": false,
[36m(WorkerDict pid=4158404)[0m   "total_ut_steps": 4,
[36m(WorkerDict pid=4158404)[0m   "transformers_version": "4.57.3",
[36m(WorkerDict pid=4158404)[0m   "use_cache": true,
[36m(WorkerDict pid=4158404)[0m   "use_sliding_window": false,
[36m(WorkerDict pid=4158404)[0m   "vocab_size": 49152
[36m(WorkerDict pid=4158404)[0m }
[36m(WorkerDict pid=4158404)[0m 
[36m(WorkerDict pid=4158405)[0m Monkey patch _flash_attention_forward in transformers.integrations.flash_attention
[36m(WorkerDict pid=4158405)[0m Skipping monkey patch for OuroForCausalLM as use_fused_kernels is False or fused_kernels_backend is None
[36m(WorkerDict pid=4158407)[0m Monkey patch _flash_attention_forward in transformers.integrations.flash_attention
[36m(WorkerDict pid=4158407)[0m Skipping monkey patch for OuroForCausalLM as use_fused_kernels is False or fused_kernels_backend is None
[36m(WorkerDict pid=4158404)[0m Monkey patch _flash_attention_forward in transformers.integrations.flash_attention
[36m(WorkerDict pid=4158404)[0m Skipping monkey patch for OuroForCausalLM as use_fused_kernels is False or fused_kernels_backend is None
[36m(WorkerDict pid=4158406)[0m Monkey patch _flash_attention_forward in transformers.integrations.flash_attention
[36m(WorkerDict pid=4158406)[0m Skipping monkey patch for OuroForCausalLM as use_fused_kernels is False or fused_kernels_backend is None
[36m(WorkerDict pid=4158404)[0m OuroForCausalLM contains 2.67B parameters
[36m(WorkerDict pid=4158404)[0m wrap_policy: functools.partial(<function _or_policy at 0x1523a935a8c0>, policies=[functools.partial(<function transformer_auto_wrap_policy at 0x1523a935a7a0>, transformer_layer_cls={<class 'transformers_modules.Ouro_hyphen_2_dot_6B_hyphen_Thinking.modeling_ouro.OuroDecoderLayer'>})])
[36m(WorkerDict pid=4158404)[0m Total steps: 250, num_warmup_steps: 25
[36m(WorkerDict pid=4158404)[0m Actor use_remove_padding=False
[36m(WorkerDict pid=4158404)[0m Actor use_fused_kernels=False
[36m(WorkerDict pid=4158405)[0m [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[36m(WorkerDict pid=4158405)[0m [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[36m(WorkerDict pid=4158405)[0m [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[36m(WorkerDict pid=4158407)[0m [Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[36m(WorkerDict pid=4158407)[0m [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[36m(WorkerDict pid=4158407)[0m [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[36m(WorkerDict pid=4158404)[0m [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[36m(WorkerDict pid=4158404)[0m [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[36m(WorkerDict pid=4158404)[0m [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[36m(WorkerDict pid=4158406)[0m [Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[36m(WorkerDict pid=4158406)[0m [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[36m(WorkerDict pid=4158406)[0m [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[36m(WorkerDict pid=4158405)[0m INFO 02-18 02:52:42 [utils.py:253] non-default args: {'trust_remote_code': True, 'dtype': 'bfloat16', 'max_model_len': 3072, 'distributed_executor_backend': 'external_launcher', 'enable_prefix_caching': True, 'gpu_memory_utilization': 0.45, 'max_num_batched_tokens': 8192, 'max_num_seqs': 1024, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'model': '/scratch/gpfs/OLGARUS/jw4199/model_weights_path/Ouro-2.6B-Thinking'}
[36m(WorkerDict pid=4158405)[0m INFO 02-18 02:52:42 [model.py:637] Resolved architecture: OuroForCausalLM
[36m(WorkerDict pid=4158405)[0m INFO 02-18 02:52:42 [model.py:1750] Using max model len 3072
[36m(WorkerDict pid=4158405)[0m INFO 02-18 02:52:42 [arg_utils.py:1427] Using ray runtime env (env vars redacted): {'env_vars': {'MASTER_ADDR': '***', 'MASTER_PORT': '***', 'RANK': '***', 'RAY_LOCAL_WORLD_SIZE': '***', 'WG_BACKEND': '***', 'WG_PREFIX': '***', 'WORLD_SIZE': '***'}}
[36m(WorkerDict pid=4158405)[0m INFO 02-18 02:52:42 [parallel.py:511] Using external launcher for distributed inference.
[36m(WorkerDict pid=4158405)[0m INFO 02-18 02:52:42 [parallel.py:545] Disabling V1 multiprocessing for external launcher.
[36m(WorkerDict pid=4158405)[0m INFO 02-18 02:52:42 [scheduler.py:228] Chunked prefill is enabled with max_num_batched_tokens=8192.
[36m(WorkerDict pid=4158407)[0m INFO 02-18 02:52:42 [utils.py:253] non-default args: {'trust_remote_code': True, 'dtype': 'bfloat16', 'max_model_len': 3072, 'distributed_executor_backend': 'external_launcher', 'enable_prefix_caching': True, 'gpu_memory_utilization': 0.45, 'max_num_batched_tokens': 8192, 'max_num_seqs': 1024, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'model': '/scratch/gpfs/OLGARUS/jw4199/model_weights_path/Ouro-2.6B-Thinking'}
[36m(WorkerDict pid=4158407)[0m INFO 02-18 02:52:42 [model.py:637] Resolved architecture: OuroForCausalLM
[36m(WorkerDict pid=4158407)[0m INFO 02-18 02:52:42 [model.py:1750] Using max model len 3072
[36m(WorkerDict pid=4158407)[0m INFO 02-18 02:52:42 [arg_utils.py:1427] Using ray runtime env (env vars redacted): {'env_vars': {'MASTER_ADDR': '***', 'MASTER_PORT': '***', 'RANK': '***', 'RAY_LOCAL_WORLD_SIZE': '***', 'WG_BACKEND': '***', 'WG_PREFIX': '***', 'WORLD_SIZE': '***'}}
[36m(WorkerDict pid=4158407)[0m INFO 02-18 02:52:42 [parallel.py:511] Using external launcher for distributed inference.
[36m(WorkerDict pid=4158407)[0m INFO 02-18 02:52:42 [parallel.py:545] Disabling V1 multiprocessing for external launcher.
[36m(WorkerDict pid=4158407)[0m INFO 02-18 02:52:42 [scheduler.py:228] Chunked prefill is enabled with max_num_batched_tokens=8192.
[36m(WorkerDict pid=4158404)[0m INFO 02-18 02:52:42 [utils.py:253] non-default args: {'trust_remote_code': True, 'dtype': 'bfloat16', 'max_model_len': 3072, 'distributed_executor_backend': 'external_launcher', 'enable_prefix_caching': True, 'gpu_memory_utilization': 0.45, 'max_num_batched_tokens': 8192, 'max_num_seqs': 1024, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'model': '/scratch/gpfs/OLGARUS/jw4199/model_weights_path/Ouro-2.6B-Thinking'}
[36m(WorkerDict pid=4158404)[0m INFO 02-18 02:52:42 [model.py:637] Resolved architecture: OuroForCausalLM
[36m(WorkerDict pid=4158404)[0m INFO 02-18 02:52:42 [model.py:1750] Using max model len 3072
[36m(WorkerDict pid=4158404)[0m INFO 02-18 02:52:42 [arg_utils.py:1427] Using ray runtime env (env vars redacted): {'env_vars': {'MASTER_ADDR': '***', 'MASTER_PORT': '***', 'RANK': '***', 'RAY_LOCAL_WORLD_SIZE': '***', 'WG_BACKEND': '***', 'WG_PREFIX': '***', 'WORLD_SIZE': '***'}}
[36m(WorkerDict pid=4158404)[0m INFO 02-18 02:52:42 [parallel.py:511] Using external launcher for distributed inference.
[36m(WorkerDict pid=4158404)[0m INFO 02-18 02:52:42 [parallel.py:545] Disabling V1 multiprocessing for external launcher.
[36m(WorkerDict pid=4158404)[0m INFO 02-18 02:52:42 [scheduler.py:228] Chunked prefill is enabled with max_num_batched_tokens=8192.
[36m(WorkerDict pid=4158406)[0m INFO 02-18 02:52:42 [utils.py:253] non-default args: {'trust_remote_code': True, 'dtype': 'bfloat16', 'max_model_len': 3072, 'distributed_executor_backend': 'external_launcher', 'enable_prefix_caching': True, 'gpu_memory_utilization': 0.45, 'max_num_batched_tokens': 8192, 'max_num_seqs': 1024, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'model': '/scratch/gpfs/OLGARUS/jw4199/model_weights_path/Ouro-2.6B-Thinking'}
[36m(WorkerDict pid=4158406)[0m INFO 02-18 02:52:42 [model.py:637] Resolved architecture: OuroForCausalLM
[36m(WorkerDict pid=4158406)[0m INFO 02-18 02:52:42 [model.py:1750] Using max model len 3072
[36m(WorkerDict pid=4158406)[0m INFO 02-18 02:52:42 [arg_utils.py:1427] Using ray runtime env (env vars redacted): {'env_vars': {'MASTER_ADDR': '***', 'MASTER_PORT': '***', 'RANK': '***', 'RAY_LOCAL_WORLD_SIZE': '***', 'WG_BACKEND': '***', 'WG_PREFIX': '***', 'WORLD_SIZE': '***'}}
[36m(WorkerDict pid=4158406)[0m INFO 02-18 02:52:42 [parallel.py:511] Using external launcher for distributed inference.
[36m(WorkerDict pid=4158406)[0m INFO 02-18 02:52:42 [parallel.py:545] Disabling V1 multiprocessing for external launcher.
[36m(WorkerDict pid=4158406)[0m INFO 02-18 02:52:42 [scheduler.py:228] Chunked prefill is enabled with max_num_batched_tokens=8192.
[36m(WorkerDict pid=4158405)[0m INFO 02-18 02:52:42 [core.py:93] Initializing a V1 LLM engine (v0.12.0) with config: model='/scratch/gpfs/OLGARUS/jw4199/model_weights_path/Ouro-2.6B-Thinking', speculative_config=None, tokenizer='/scratch/gpfs/OLGARUS/jw4199/model_weights_path/Ouro-2.6B-Thinking', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=3072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01), seed=0, served_model_name=/scratch/gpfs/OLGARUS/jw4199/model_weights_path/Ouro-2.6B-Thinking, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>}, 'local_cache_dir': None}
[36m(WorkerDict pid=4158407)[0m INFO 02-18 02:52:42 [core.py:93] Initializing a V1 LLM engine (v0.12.0) with config: model='/scratch/gpfs/OLGARUS/jw4199/model_weights_path/Ouro-2.6B-Thinking', speculative_config=None, tokenizer='/scratch/gpfs/OLGARUS/jw4199/model_weights_path/Ouro-2.6B-Thinking', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=3072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01), seed=0, served_model_name=/scratch/gpfs/OLGARUS/jw4199/model_weights_path/Ouro-2.6B-Thinking, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>}, 'local_cache_dir': None}
[36m(WorkerDict pid=4158404)[0m INFO 02-18 02:52:42 [core.py:93] Initializing a V1 LLM engine (v0.12.0) with config: model='/scratch/gpfs/OLGARUS/jw4199/model_weights_path/Ouro-2.6B-Thinking', speculative_config=None, tokenizer='/scratch/gpfs/OLGARUS/jw4199/model_weights_path/Ouro-2.6B-Thinking', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=3072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01), seed=0, served_model_name=/scratch/gpfs/OLGARUS/jw4199/model_weights_path/Ouro-2.6B-Thinking, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>}, 'local_cache_dir': None}
[36m(WorkerDict pid=4158406)[0m INFO 02-18 02:52:42 [core.py:93] Initializing a V1 LLM engine (v0.12.0) with config: model='/scratch/gpfs/OLGARUS/jw4199/model_weights_path/Ouro-2.6B-Thinking', speculative_config=None, tokenizer='/scratch/gpfs/OLGARUS/jw4199/model_weights_path/Ouro-2.6B-Thinking', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=3072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01), seed=0, served_model_name=/scratch/gpfs/OLGARUS/jw4199/model_weights_path/Ouro-2.6B-Thinking, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>}, 'local_cache_dir': None}
[36m(WorkerDict pid=4158405)[0m INFO 02-18 02:52:46 [parallel_state.py:1408] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
[36m(WorkerDict pid=4158407)[0m INFO 02-18 02:52:46 [parallel_state.py:1408] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
[36m(WorkerDict pid=4158404)[0m INFO 02-18 02:52:46 [parallel_state.py:1408] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
[36m(WorkerDict pid=4158406)[0m INFO 02-18 02:52:46 [parallel_state.py:1408] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
[36m(WorkerDict pid=4158404)[0m INFO 02-18 02:52:46 [gpu_model_runner.py:3467] Starting to load model /scratch/gpfs/OLGARUS/jw4199/model_weights_path/Ouro-2.6B-Thinking...
[36m(WorkerDict pid=4158405)[0m INFO 02-18 02:52:48 [cuda.py:411] Using FLASH_ATTN attention backend out of potential backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[36m(WorkerDict pid=4158407)[0m INFO 02-18 02:52:48 [cuda.py:411] Using FLASH_ATTN attention backend out of potential backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[36m(WorkerDict pid=4158404)[0m INFO 02-18 02:52:48 [cuda.py:411] Using FLASH_ATTN attention backend out of potential backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[36m(WorkerDict pid=4158406)[0m INFO 02-18 02:52:48 [cuda.py:411] Using FLASH_ATTN attention backend out of potential backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[36m(WorkerDict pid=4158405)[0m [DEBUG OuroModel.load_weights] early_exit params in model: ['early_exit_gate.weight', 'early_exit_gate.bias']
[36m(WorkerDict pid=4158405)[0m [DEBUG OuroModel.load_weights] Processing weight: 'early_exit_gate.bias' shape=torch.Size([1])
[36m(WorkerDict pid=4158405)[0m [DEBUG OuroModel.load_weights] Processing weight: 'early_exit_gate.weight' shape=torch.Size([1, 2048])
[36m(WorkerDict pid=4158407)[0m [DEBUG OuroModel.load_weights] early_exit params in model: ['early_exit_gate.weight', 'early_exit_gate.bias']
[36m(WorkerDict pid=4158407)[0m [DEBUG OuroModel.load_weights] Processing weight: 'early_exit_gate.bias' shape=torch.Size([1])
[36m(WorkerDict pid=4158407)[0m [DEBUG OuroModel.load_weights] Processing weight: 'early_exit_gate.weight' shape=torch.Size([1, 2048])
[36m(WorkerDict pid=4158404)[0m [DEBUG OuroModel.load_weights] early_exit params in model: ['early_exit_gate.weight', 'early_exit_gate.bias']
[36m(WorkerDict pid=4158404)[0m [DEBUG OuroModel.load_weights] Processing weight: 'early_exit_gate.bias' shape=torch.Size([1])
[36m(WorkerDict pid=4158404)[0m [DEBUG OuroModel.load_weights] Processing weight: 'early_exit_gate.weight' shape=torch.Size([1, 2048])
[36m(WorkerDict pid=4158406)[0m [DEBUG OuroModel.load_weights] early_exit params in model: ['early_exit_gate.weight', 'early_exit_gate.bias']
[36m(WorkerDict pid=4158406)[0m [DEBUG OuroModel.load_weights] Processing weight: 'early_exit_gate.bias' shape=torch.Size([1])
[36m(WorkerDict pid=4158406)[0m [DEBUG OuroModel.load_weights] Processing weight: 'early_exit_gate.weight' shape=torch.Size([1, 2048])
[36m(WorkerDict pid=4158405)[0m INFO 02-18 02:52:57 [default_loader.py:308] Loading weights took 8.83 seconds
[36m(WorkerDict pid=4158407)[0m INFO 02-18 02:52:57 [default_loader.py:308] Loading weights took 8.80 seconds
[36m(WorkerDict pid=4158404)[0m INFO 02-18 02:52:57 [default_loader.py:308] Loading weights took 8.79 seconds
[36m(WorkerDict pid=4158406)[0m INFO 02-18 02:52:57 [default_loader.py:308] Loading weights took 8.83 seconds
[36m(WorkerDict pid=4158407)[0m INFO 02-18 02:52:57 [gpu_model_runner.py:3549] Model loading took 4.9858 GiB memory and 9.941156 seconds
[36m(WorkerDict pid=4158404)[0m INFO 02-18 02:52:57 [gpu_model_runner.py:3549] Model loading took 4.9858 GiB memory and 9.933857 seconds
[36m(WorkerDict pid=4158405)[0m INFO 02-18 02:52:57 [gpu_model_runner.py:3549] Model loading took 4.9858 GiB memory and 9.961609 seconds
[36m(WorkerDict pid=4158406)[0m INFO 02-18 02:52:57 [gpu_model_runner.py:3549] Model loading took 4.9858 GiB memory and 9.990352 seconds
[36m(WorkerDict pid=4158405)[0m INFO 02-18 02:53:24 [backends.py:655] Using cache directory: /scratch/gpfs/OLGARUS/jw4199/model_weights_path/.xdg_cache/vllm/torch_compile_cache/ab8c3de0aa/rank_1_0/backbone for vLLM's torch.compile
[36m(WorkerDict pid=4158405)[0m INFO 02-18 02:53:24 [backends.py:715] Dynamo bytecode transform time: 26.71 s
[36m(WorkerDict pid=4158407)[0m INFO 02-18 02:53:24 [backends.py:655] Using cache directory: /scratch/gpfs/OLGARUS/jw4199/model_weights_path/.xdg_cache/vllm/torch_compile_cache/ab8c3de0aa/rank_3_0/backbone for vLLM's torch.compile
[36m(WorkerDict pid=4158406)[0m INFO 02-18 02:53:24 [backends.py:655] Using cache directory: /scratch/gpfs/OLGARUS/jw4199/model_weights_path/.xdg_cache/vllm/torch_compile_cache/ab8c3de0aa/rank_2_0/backbone for vLLM's torch.compile
[36m(WorkerDict pid=4158407)[0m INFO 02-18 02:53:24 [backends.py:715] Dynamo bytecode transform time: 26.73 s
[36m(WorkerDict pid=4158406)[0m INFO 02-18 02:53:24 [backends.py:715] Dynamo bytecode transform time: 26.71 s
[36m(WorkerDict pid=4158404)[0m INFO 02-18 02:53:25 [backends.py:655] Using cache directory: /scratch/gpfs/OLGARUS/jw4199/model_weights_path/.xdg_cache/vllm/torch_compile_cache/ab8c3de0aa/rank_0_0/backbone for vLLM's torch.compile
[36m(WorkerDict pid=4158404)[0m INFO 02-18 02:53:25 [backends.py:715] Dynamo bytecode transform time: 27.18 s
[36m(WorkerDict pid=4158407)[0m INFO 02-18 02:53:43 [backends.py:216] Directly load the compiled graph(s) for dynamic shape from the cache, took 16.615 s
[36m(WorkerDict pid=4158406)[0m INFO 02-18 02:53:43 [backends.py:216] Directly load the compiled graph(s) for dynamic shape from the cache, took 16.604 s
[36m(WorkerDict pid=4158405)[0m INFO 02-18 02:53:43 [backends.py:216] Directly load the compiled graph(s) for dynamic shape from the cache, took 16.941 s
[36m(WorkerDict pid=4158404)[0m INFO 02-18 02:53:43 [backends.py:216] Directly load the compiled graph(s) for dynamic shape from the cache, took 16.497 s
[36m(WorkerDict pid=4158406)[0m INFO 02-18 02:53:50 [monitor.py:34] torch.compile takes 43.31 s in total
[36m(WorkerDict pid=4158405)[0m INFO 02-18 02:53:50 [monitor.py:34] torch.compile takes 43.65 s in total
[36m(WorkerDict pid=4158404)[0m INFO 02-18 02:53:50 [monitor.py:34] torch.compile takes 43.67 s in total
[36m(WorkerDict pid=4158407)[0m INFO 02-18 02:53:50 [monitor.py:34] torch.compile takes 43.34 s in total
[36m(WorkerDict pid=4158405)[0m INFO 02-18 02:53:53 [gpu_worker.py:359] Available KV cache memory: 56.00 GiB
[36m(WorkerDict pid=4158407)[0m INFO 02-18 02:53:53 [gpu_worker.py:359] Available KV cache memory: 56.00 GiB
[36m(WorkerDict pid=4158404)[0m INFO 02-18 02:53:53 [gpu_worker.py:359] Available KV cache memory: 56.00 GiB
[36m(WorkerDict pid=4158406)[0m INFO 02-18 02:53:53 [gpu_worker.py:359] Available KV cache memory: 56.00 GiB
[36m(WorkerDict pid=4158405)[0m INFO 02-18 02:53:53 [kv_cache_utils.py:1286] GPU KV cache size: 38,224 tokens
[36m(WorkerDict pid=4158405)[0m INFO 02-18 02:53:53 [kv_cache_utils.py:1291] Maximum concurrency for 3,072 tokens per request: 12.44x
[36m(WorkerDict pid=4158407)[0m INFO 02-18 02:53:53 [kv_cache_utils.py:1286] GPU KV cache size: 38,224 tokens
[36m(WorkerDict pid=4158407)[0m INFO 02-18 02:53:53 [kv_cache_utils.py:1291] Maximum concurrency for 3,072 tokens per request: 12.44x
[36m(WorkerDict pid=4158404)[0m INFO 02-18 02:53:53 [kv_cache_utils.py:1286] GPU KV cache size: 38,224 tokens
[36m(WorkerDict pid=4158404)[0m INFO 02-18 02:53:53 [kv_cache_utils.py:1291] Maximum concurrency for 3,072 tokens per request: 12.44x
[36m(WorkerDict pid=4158406)[0m INFO 02-18 02:53:53 [kv_cache_utils.py:1286] GPU KV cache size: 38,224 tokens
[36m(WorkerDict pid=4158406)[0m INFO 02-18 02:53:53 [kv_cache_utils.py:1291] Maximum concurrency for 3,072 tokens per request: 12.44x
[36m(WorkerDict pid=4158407)[0m INFO 02-18 02:54:11 [gpu_model_runner.py:4466] Graph capturing finished in 18 secs, took 1.34 GiB
[36m(WorkerDict pid=4158407)[0m INFO 02-18 02:54:11 [core.py:254] init engine (profile, create kv cache, warmup model) took 74.16 seconds
[36m(WorkerDict pid=4158406)[0m INFO 02-18 02:54:11 [gpu_model_runner.py:4466] Graph capturing finished in 18 secs, took 1.34 GiB
[36m(WorkerDict pid=4158406)[0m INFO 02-18 02:54:11 [core.py:254] init engine (profile, create kv cache, warmup model) took 74.14 seconds
[36m(WorkerDict pid=4158405)[0m INFO 02-18 02:54:12 [gpu_model_runner.py:4466] Graph capturing finished in 18 secs, took 1.34 GiB
[36m(WorkerDict pid=4158405)[0m INFO 02-18 02:54:12 [core.py:254] init engine (profile, create kv cache, warmup model) took 74.46 seconds
[36m(WorkerDict pid=4158404)[0m INFO 02-18 02:54:12 [gpu_model_runner.py:4466] Graph capturing finished in 18 secs, took 1.34 GiB
[36m(WorkerDict pid=4158404)[0m INFO 02-18 02:54:12 [core.py:254] init engine (profile, create kv cache, warmup model) took 74.69 seconds
[36m(WorkerDict pid=4158407)[0m INFO 02-18 02:54:12 [llm.py:343] Supported tasks: ('generate',)
[36m(WorkerDict pid=4158407)[0m kwargs: {'n': 1, 'logprobs': 0, 'max_tokens': 2048, 'repetition_penalty': 1.0, 'detokenize': False, 'temperature': 1.0, 'top_k': -1, 'top_p': 1.0, 'ignore_eos': False}
[36m(WorkerDict pid=4158407)[0m Only support config type of {'llama', 'minicpmv', 'qwen2_vl', 'glm4v', 'qwen2_5_vl', 'apertus', 'qwen3_vl_moe', 'deepseek_v3', 'seed_oss', 'qwen3', 'minicpmo', 'qwen3_moe', 'qwen2_moe', 'gemma3_text', 'mistral', 'qwen3_vl', 'qwen2'}, but got ouro. MFU will always be zero.
[36m(WorkerDict pid=4158407)[0m [LLOPSD WORKER DEBUG] Replacing actor with LLOPSDDataParallelPPOActor on rank 3
[36m(WorkerDict pid=4158406)[0m INFO 02-18 02:54:12 [llm.py:343] Supported tasks: ('generate',)
[36m(WorkerDict pid=4158406)[0m kwargs: {'n': 1, 'logprobs': 0, 'max_tokens': 2048, 'repetition_penalty': 1.0, 'detokenize': False, 'temperature': 1.0, 'top_k': -1, 'top_p': 1.0, 'ignore_eos': False}
[36m(WorkerDict pid=4158406)[0m Only support config type of {'deepseek_v3', 'qwen3_moe', 'minicpmo', 'llama', 'qwen3_vl_moe', 'seed_oss', 'qwen2_vl', 'apertus', 'qwen2_5_vl', 'qwen2', 'mistral', 'glm4v', 'gemma3_text', 'qwen3_vl', 'minicpmv', 'qwen3', 'qwen2_moe'}, but got ouro. MFU will always be zero.
[36m(WorkerDict pid=4158406)[0m [LLOPSD WORKER DEBUG] Replacing actor with LLOPSDDataParallelPPOActor on rank 2
[36m(WorkerDict pid=4158405)[0m INFO 02-18 02:54:13 [llm.py:343] Supported tasks: ('generate',)
[36m(WorkerDict pid=4158405)[0m kwargs: {'n': 1, 'logprobs': 0, 'max_tokens': 2048, 'repetition_penalty': 1.0, 'detokenize': False, 'temperature': 1.0, 'top_k': -1, 'top_p': 1.0, 'ignore_eos': False}
[36m(WorkerDict pid=4158405)[0m Only support config type of {'apertus', 'qwen3_vl_moe', 'seed_oss', 'qwen3', 'qwen2', 'minicpmo', 'gemma3_text', 'qwen3_vl', 'qwen2_moe', 'deepseek_v3', 'glm4v', 'mistral', 'qwen2_5_vl', 'minicpmv', 'qwen2_vl', 'llama', 'qwen3_moe'}, but got ouro. MFU will always be zero.
[36m(WorkerDict pid=4158405)[0m [LLOPSD WORKER DEBUG] Replacing actor with LLOPSDDataParallelPPOActor on rank 1
[36m(WorkerDict pid=4158404)[0m INFO 02-18 02:54:13 [llm.py:343] Supported tasks: ('generate',)
[36m(WorkerDict pid=4158404)[0m kwargs: {'n': 1, 'logprobs': 0, 'max_tokens': 2048, 'repetition_penalty': 1.0, 'detokenize': False, 'temperature': 1.0, 'top_k': -1, 'top_p': 1.0, 'ignore_eos': False}
[36m(WorkerDict pid=4158404)[0m Only support config type of {'qwen2_vl', 'minicpmv', 'apertus', 'qwen3', 'qwen2', 'qwen3_moe', 'minicpmo', 'gemma3_text', 'qwen2_5_vl', 'qwen2_moe', 'seed_oss', 'qwen3_vl_moe', 'mistral', 'glm4v', 'qwen3_vl', 'deepseek_v3', 'llama'}, but got ouro. MFU will always be zero.
[36m(WorkerDict pid=4158404)[0m Set actor model.config.total_ut_steps = 4
[36m(WorkerDict pid=4158404)[0m [LLOPSD WORKER DEBUG] Replacing actor with LLOPSDDataParallelPPOActor on rank 0
[36m(WorkerDict pid=4158404)[0m [LLOPSD] Building ema teacher model from: /scratch/gpfs/OLGARUS/jw4199/model_weights_path/Ouro-2.6B-Thinking
[36m(WorkerDict pid=4158404)[0m Model config after override: OuroConfig {
[36m(WorkerDict pid=4158404)[0m   "architectures": [
[36m(WorkerDict pid=4158404)[0m     "OuroForCausalLM"
[36m(WorkerDict pid=4158404)[0m   ],
[36m(WorkerDict pid=4158404)[0m   "attention_dropout": 0.0,
[36m(WorkerDict pid=4158404)[0m   "auto_map": {
[36m(WorkerDict pid=4158404)[0m     "AutoConfig": "configuration_ouro.OuroConfig",
[36m(WorkerDict pid=4158404)[0m     "AutoModel": "modeling_ouro.OuroModel",
[36m(WorkerDict pid=4158404)[0m     "AutoModelForCausalLM": "modeling_ouro.OuroForCausalLM"
[36m(WorkerDict pid=4158404)[0m   },
[36m(WorkerDict pid=4158404)[0m   "bos_token_id": 0,
[36m(WorkerDict pid=4158404)[0m   "dtype": "bfloat16",
[36m(WorkerDict pid=4158404)[0m   "early_exit_threshold": 1.0,
[36m(WorkerDict pid=4158404)[0m   "eos_token_id": 0,
[36m(WorkerDict pid=4158404)[0m   "head_dim": 128,
[36m(WorkerDict pid=4158404)[0m   "hidden_act": "silu",
[36m(WorkerDict pid=4158404)[0m   "hidden_size": 2048,
[36m(WorkerDict pid=4158404)[0m   "initializer_range": 0.02,
[36m(WorkerDict pid=4158404)[0m   "intermediate_size": 5632,
[36m(WorkerDict pid=4158404)[0m   "layer_types": [
[36m(WorkerDict pid=4158404)[0m     "full_attention",
[36m(WorkerDict pid=4158404)[0m     "full_attention",
[36m(WorkerDict pid=4158404)[0m     "full_attention",
[36m(WorkerDict pid=4158404)[0m     "full_attention",
[36m(WorkerDict pid=4158404)[0m     "full_attention",
[36m(WorkerDict pid=4158404)[0m     "full_attention",
[36m(WorkerDict pid=4158404)[0m     "full_attention",
[36m(WorkerDict pid=4158404)[0m     "full_attention",
[36m(WorkerDict pid=4158404)[0m     "full_attention",
[36m(WorkerDict pid=4158404)[0m     "full_attention",
[36m(WorkerDict pid=4158404)[0m     "full_attention",
[36m(WorkerDict pid=4158404)[0m     "full_attention",
[36m(WorkerDict pid=4158404)[0m     "full_attention",
[36m(WorkerDict pid=4158404)[0m     "full_attention",
[36m(WorkerDict pid=4158404)[0m     "full_attention",
[36m(WorkerDict pid=4158404)[0m     "full_attention",
[36m(WorkerDict pid=4158404)[0m     "full_attention",
[36m(WorkerDict pid=4158404)[0m     "full_attention",
[36m(WorkerDict pid=4158404)[0m     "full_attention",
[36m(WorkerDict pid=4158404)[0m     "full_attention",
[36m(WorkerDict pid=4158404)[0m     "full_attention",
[36m(WorkerDict pid=4158404)[0m     "full_attention",
[36m(WorkerDict pid=4158404)[0m     "full_attention",
[36m(WorkerDict pid=4158404)[0m     "full_attention",
[36m(WorkerDict pid=4158404)[0m     "full_attention",
[36m(WorkerDict pid=4158404)[0m     "full_attention",
[36m(WorkerDict pid=4158404)[0m     "full_attention",
[36m(WorkerDict pid=4158404)[0m     "full_attention",
[36m(WorkerDict pid=4158404)[0m     "full_attention",
[36m(WorkerDict pid=4158404)[0m     "full_attention",
[36m(WorkerDict pid=4158404)[0m     "full_attention",
[36m(WorkerDict pid=4158404)[0m     "full_attention",
[36m(WorkerDict pid=4158404)[0m     "full_attention",
[36m(WorkerDict pid=4158404)[0m     "full_attention",
[36m(WorkerDict pid=4158404)[0m     "full_attention",
[36m(WorkerDict pid=4158404)[0m     "full_attention",
[36m(WorkerDict pid=4158404)[0m     "full_attention",
[36m(WorkerDict pid=4158404)[0m     "full_attention",
[36m(WorkerDict pid=4158404)[0m     "full_attention",
[36m(WorkerDict pid=4158404)[0m     "full_attention",
[36m(WorkerDict pid=4158404)[0m     "full_attention",
[36m(WorkerDict pid=4158404)[0m     "full_attention",
[36m(WorkerDict pid=4158404)[0m     "full_attention",
[36m(WorkerDict pid=4158404)[0m     "full_attention",
[36m(WorkerDict pid=4158404)[0m     "full_attention",
[36m(WorkerDict pid=4158404)[0m     "full_attention",
[36m(WorkerDict pid=4158404)[0m     "full_attention",
[36m(WorkerDict pid=4158404)[0m     "full_attention"
[36m(WorkerDict pid=4158404)[0m   ],
[36m(WorkerDict pid=4158404)[0m   "max_position_embeddings": 65536,
[36m(WorkerDict pid=4158404)[0m   "max_window_layers": 48,
[36m(WorkerDict pid=4158404)[0m   "model_type": "ouro",
[36m(WorkerDict pid=4158404)[0m   "num_attention_heads": 16,
[36m(WorkerDict pid=4158404)[0m   "num_hidden_layers": 48,
[36m(WorkerDict pid=4158404)[0m   "num_key_value_heads": 16,
[36m(WorkerDict pid=4158404)[0m   "pad_token_id": 0,
[36m(WorkerDict pid=4158404)[0m   "rms_norm_eps": 1e-06,
[36m(WorkerDict pid=4158404)[0m   "rope_scaling": null,
[36m(WorkerDict pid=4158404)[0m   "rope_theta": 1000000.0,
[36m(WorkerDict pid=4158404)[0m   "sliding_window": null,
[36m(WorkerDict pid=4158404)[0m   "tie_word_embeddings": false,
[36m(WorkerDict pid=4158404)[0m   "total_ut_steps": 4,
[36m(WorkerDict pid=4158404)[0m   "transformers_version": "4.57.3",
[36m(WorkerDict pid=4158404)[0m   "use_cache": true,
[36m(WorkerDict pid=4158404)[0m   "use_sliding_window": false,
[36m(WorkerDict pid=4158404)[0m   "vocab_size": 49152
[36m(WorkerDict pid=4158404)[0m }
[36m(WorkerDict pid=4158404)[0m 
[36m(WorkerDict pid=4158405)[0m Monkey patch _flash_attention_forward in transformers.integrations.flash_attention
[36m(WorkerDict pid=4158405)[0m Skipping monkey patch for OuroForCausalLM as use_fused_kernels is False or fused_kernels_backend is None
[36m(WorkerDict pid=4158407)[0m Monkey patch _flash_attention_forward in transformers.integrations.flash_attention
[36m(WorkerDict pid=4158407)[0m Skipping monkey patch for OuroForCausalLM as use_fused_kernels is False or fused_kernels_backend is None
[36m(WorkerDict pid=4158404)[0m Monkey patch _flash_attention_forward in transformers.integrations.flash_attention
[36m(WorkerDict pid=4158404)[0m Skipping monkey patch for OuroForCausalLM as use_fused_kernels is False or fused_kernels_backend is None
[36m(WorkerDict pid=4158404)[0m OuroForCausalLM contains 2.67B parameters
[36m(WorkerDict pid=4158404)[0m wrap_policy: functools.partial(<function _or_policy at 0x1523a935a8c0>, policies=[functools.partial(<function transformer_auto_wrap_policy at 0x1523a935a7a0>, transformer_layer_cls={<class 'transformers_modules.Ouro_hyphen_2_dot_6B_hyphen_Thinking.modeling_ouro.OuroDecoderLayer'>})])
[36m(WorkerDict pid=4158406)[0m Monkey patch _flash_attention_forward in transformers.integrations.flash_attention
[36m(WorkerDict pid=4158406)[0m Skipping monkey patch for OuroForCausalLM as use_fused_kernels is False or fused_kernels_backend is None
[36m(WorkerDict pid=4158405)[0m [LLOPSD WORKER DEBUG] init_model() completed on rank 1
[36m(WorkerDict pid=4158407)[0m [LLOPSD WORKER DEBUG] init_model() completed on rank 3
[36m(WorkerDict pid=4158404)[0m [LLOPSD] Set teacher model.config.total_ut_steps = 4
[36m(WorkerDict pid=4158404)[0m [LLOPSD] EMA teacher model built successfully (FSDP with CPU offload)
[36m(WorkerDict pid=4158404)[0m Actor use_remove_padding=False
[36m(WorkerDict pid=4158404)[0m Actor use_fused_kernels=False
[36m(WorkerDict pid=4158404)[0m [LLOPSD] _check_model_rltt_support: model_type=OuroForCausalLM, return_per_loop_logits=False, return_exit_pdf=False -> False
[36m(WorkerDict pid=4158404)[0m LLOPSD Actor initialized:
[36m(WorkerDict pid=4158404)[0m   teacher_loops (R)       = 4
[36m(WorkerDict pid=4158404)[0m   student_loops (R~)      = 2
[36m(WorkerDict pid=4158404)[0m   loop_mapping_strategy   = shift
[36m(WorkerDict pid=4158404)[0m   loop_mapping            = [2, 3]
[36m(WorkerDict pid=4158404)[0m   weight_schedule         = uniform
[36m(WorkerDict pid=4158404)[0m   weight_gamma            = 1.0
[36m(WorkerDict pid=4158404)[0m   student_weights         = [0.5, 0.5]
[36m(WorkerDict pid=4158404)[0m   divergence_type         = forward_kl
[36m(WorkerDict pid=4158404)[0m   teacher_mode            = ema
[36m(WorkerDict pid=4158404)[0m   teacher_context         = opsd
[36m(WorkerDict pid=4158404)[0m   ema_decay               = 0.99
[36m(WorkerDict pid=4158404)[0m   model_supports_per_loop = False
[36m(WorkerDict pid=4158404)[0m Created LLOPSDDataParallelPPOActor with:
[36m(WorkerDict pid=4158404)[0m   teacher_module: provided
[36m(WorkerDict pid=4158404)[0m   teacher_loops: 4
[36m(WorkerDict pid=4158404)[0m   student_loops: 2
[36m(WorkerDict pid=4158404)[0m   loop_mapping_strategy: shift
[36m(WorkerDict pid=4158404)[0m   weight_schedule: uniform
[36m(WorkerDict pid=4158404)[0m   weight_gamma: 1.0
[36m(WorkerDict pid=4158404)[0m   divergence_type: forward_kl
[36m(WorkerDict pid=4158404)[0m   teacher_context: opsd
[36m(WorkerDict pid=4158404)[0m   teacher_mode: ema
[36m(WorkerDict pid=4158404)[0m   ema_decay: 0.99
[36m(WorkerDict pid=4158404)[0m Parameter summary (summed across 4 FSDP shards):
[36m(WorkerDict pid=4158404)[0m   Total params:     2,667,974,660
[36m(WorkerDict pid=4158404)[0m   Trainable params: 2,667,974,660 (100.00%)
[36m(WorkerDict pid=4158404)[0m   Frozen params:    0 (0.00%)
[36m(WorkerDict pid=4158404)[0m [LLOPSD WORKER DEBUG] init_model() completed on rank 0
[36m(WorkerDict pid=4158406)[0m [LLOPSD WORKER DEBUG] init_model() completed on rank 2
