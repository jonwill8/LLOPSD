==========================================
RLTT Training - Reward Latent Thought Trajectories
==========================================
Job ID: 4753520
Node: della-i24g2
Start time: Sun Feb 15 05:07:43 AM EST 2026

==========================================
*** TRAINING MODE: FULL PARAMETER FINE-TUNING ***
==========================================

DISABLE_LOOP_CHECKPOINTING=1 (exit_pdf mode with FSDP)
Python: /home/jw4199/miniconda3/envs/ouro_vllm/bin/python
PyTorch version: 2.9.0+cu128
Transformers version: 4.57.3
CUDA available: True
GPU count: 4
GPU name: NVIDIA H200

vLLM: available
verl: available

==========================================
RLTT Configuration
==========================================

RLTT-Specific Settings:
  Loop weighting strategy: exit_pdf
  Progressive alpha: 1.0
  Total UT steps: 4
  Log-prob chunk size: 2048
  Reference model CPU offload: true
  Verl gradient checkpointing: enabled

Model Settings:
  Model path: /scratch/gpfs/OLGARUS/jw4199/model_weights_path/Ouro-2.6B-Thinking

Data Settings:
  Train file: /scratch/gpfs/OLGARUS/jw4199/datasets/MATH/math_train.jsonl
  Output dir: ./rltt_output/4753520

RL Hyperparameters:
  Beta (KL coef): 0.001
  Num generations per prompt: 8
  Num prompts per batch: 32
  Temperature: 1

Training Hyperparameters:
  Learning rate: 1e-6
  LR scheduler: constant
  Warmup ratio: 0.1
  Weight decay: 0.1
  Max grad norm: 0.1
  Epochs: 1
  Total batch size: 256 (32 prompts x 8 gens)
  Dynamic batch sizing: enabled (ppo_max_token_len=8192)
  Log-prob max token len: 8192

Sequence Lengths:
  Max prompt length: 1024
  Max completion length: 2048

==========================================
Training Mode Configuration:
==========================================
  *** TRAINING MODE: FULL PARAMETER FINE-TUNING ***
  All model parameters will be updated
  Note: Requires more GPU memory than LoRA

Multi-GPU Configuration:
  Number of GPUs: 4
  Number of nodes: 1
  vLLM TP size: 1

vLLM Configuration:
  Use vLLM: true
  GPU memory utilization: 0.45

SFT Checkpoint:
  Use SFT checkpoint: false
  Starting from fresh model weights (no SFT initialization)

==========================================

RLTT Loop Weighting Explanation:
================================
  Using EXIT_PDF weighting (Option C - model-native)
  Ï‰_t = P(exit at loop t) from model's exit gate
  Dynamic per-token weighting based on model's exit probability distribution
  Requires RLTT-modified Ouro model (USE_RLTT_MODEL=true)

RLTT Model Configuration:
========================
  Using RLTT-modified Ouro model: /scratch/gpfs/OLGARUS/jw4199/model_weights_path/Ouro-2.6B-Thinking-RLTT
  This model exposes per-loop hidden states and exit PDF

Training Progress Guide:
========================
  Dataset size: 7500 prompts
  Prompts per step: 32
  Steps per epoch: 234
  Total steps: 234 (over 1 epoch(s))

Starting RLTT training...

Using dataset class: RLHFDataset
dataset len: 7500
filter dataset len: 7481
Using dataset class: RLHFDataset
dataset len: 7500
filter dataset len: 7481
Size of train dataloader: 233, Size of val dataloader: 234
Total training steps: 3
colocated worker base class <class 'verl.single_controller.base.worker.Worker'>
[36m(WorkerDict pid=2000109)[0m [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[36m(WorkerDict pid=2000110)[0m [Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[36m(WorkerDict pid=2000111)[0m [Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[36m(WorkerDict pid=2000108)[0m [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[36m(WorkerDict pid=2000109)[0m [RLTT WORKER DEBUG] init_model() called on rank 1
[36m(WorkerDict pid=2000110)[0m [RLTT WORKER DEBUG] init_model() called on rank 2
[36m(WorkerDict pid=2000111)[0m [RLTT WORKER DEBUG] init_model() called on rank 3
[36m(WorkerDict pid=2000108)[0m RLTT Worker initialized:
[36m(WorkerDict pid=2000108)[0m   loop_weighting: exit_pdf
[36m(WorkerDict pid=2000108)[0m   progressive_alpha: 1.0
[36m(WorkerDict pid=2000108)[0m   total_ut_steps: 4
[36m(WorkerDict pid=2000108)[0m [RLTT WORKER DEBUG] init_model() called on rank 0
[36m(WorkerDict pid=2000108)[0m Model config after override: OuroConfig {
[36m(WorkerDict pid=2000108)[0m   "architectures": [
[36m(WorkerDict pid=2000108)[0m     "OuroForCausalLM"
[36m(WorkerDict pid=2000108)[0m   ],
[36m(WorkerDict pid=2000108)[0m   "attention_dropout": 0.0,
[36m(WorkerDict pid=2000108)[0m   "auto_map": {
[36m(WorkerDict pid=2000108)[0m     "AutoConfig": "configuration_ouro.OuroConfig",
[36m(WorkerDict pid=2000108)[0m     "AutoModel": "modeling_ouro.OuroModel",
[36m(WorkerDict pid=2000108)[0m     "AutoModelForCausalLM": "modeling_ouro.OuroForCausalLM"
[36m(WorkerDict pid=2000108)[0m   },
[36m(WorkerDict pid=2000108)[0m   "bos_token_id": 0,
[36m(WorkerDict pid=2000108)[0m   "dtype": "bfloat16",
[36m(WorkerDict pid=2000108)[0m   "early_exit_threshold": 1.0,
[36m(WorkerDict pid=2000108)[0m   "eos_token_id": 0,
[36m(WorkerDict pid=2000108)[0m   "head_dim": 128,
[36m(WorkerDict pid=2000108)[0m   "hidden_act": "silu",
[36m(WorkerDict pid=2000108)[0m   "hidden_size": 2048,
[36m(WorkerDict pid=2000108)[0m   "initializer_range": 0.02,
[36m(WorkerDict pid=2000108)[0m   "intermediate_size": 5632,
[36m(WorkerDict pid=2000108)[0m   "layer_types": [
[36m(WorkerDict pid=2000108)[0m     "full_attention",
[36m(WorkerDict pid=2000108)[0m     "full_attention",
[36m(WorkerDict pid=2000108)[0m     "full_attention",
[36m(WorkerDict pid=2000108)[0m     "full_attention",
[36m(WorkerDict pid=2000108)[0m     "full_attention",
[36m(WorkerDict pid=2000108)[0m     "full_attention",
[36m(WorkerDict pid=2000108)[0m     "full_attention",
[36m(WorkerDict pid=2000108)[0m     "full_attention",
[36m(WorkerDict pid=2000108)[0m     "full_attention",
[36m(WorkerDict pid=2000108)[0m     "full_attention",
[36m(WorkerDict pid=2000108)[0m     "full_attention",
[36m(WorkerDict pid=2000108)[0m     "full_attention",
[36m(WorkerDict pid=2000108)[0m     "full_attention",
[36m(WorkerDict pid=2000108)[0m     "full_attention",
[36m(WorkerDict pid=2000108)[0m     "full_attention",
[36m(WorkerDict pid=2000108)[0m     "full_attention",
[36m(WorkerDict pid=2000108)[0m     "full_attention",
[36m(WorkerDict pid=2000108)[0m     "full_attention",
[36m(WorkerDict pid=2000108)[0m     "full_attention",
[36m(WorkerDict pid=2000108)[0m     "full_attention",
[36m(WorkerDict pid=2000108)[0m     "full_attention",
[36m(WorkerDict pid=2000108)[0m     "full_attention",
[36m(WorkerDict pid=2000108)[0m     "full_attention",
[36m(WorkerDict pid=2000108)[0m     "full_attention",
[36m(WorkerDict pid=2000108)[0m     "full_attention",
[36m(WorkerDict pid=2000108)[0m     "full_attention",
[36m(WorkerDict pid=2000108)[0m     "full_attention",
[36m(WorkerDict pid=2000108)[0m     "full_attention",
[36m(WorkerDict pid=2000108)[0m     "full_attention",
[36m(WorkerDict pid=2000108)[0m     "full_attention",
[36m(WorkerDict pid=2000108)[0m     "full_attention",
[36m(WorkerDict pid=2000108)[0m     "full_attention",
[36m(WorkerDict pid=2000108)[0m     "full_attention",
[36m(WorkerDict pid=2000108)[0m     "full_attention",
[36m(WorkerDict pid=2000108)[0m     "full_attention",
[36m(WorkerDict pid=2000108)[0m     "full_attention",
[36m(WorkerDict pid=2000108)[0m     "full_attention",
[36m(WorkerDict pid=2000108)[0m     "full_attention",
[36m(WorkerDict pid=2000108)[0m     "full_attention",
[36m(WorkerDict pid=2000108)[0m     "full_attention",
[36m(WorkerDict pid=2000108)[0m     "full_attention",
[36m(WorkerDict pid=2000108)[0m     "full_attention",
[36m(WorkerDict pid=2000108)[0m     "full_attention",
[36m(WorkerDict pid=2000108)[0m     "full_attention",
[36m(WorkerDict pid=2000108)[0m     "full_attention",
[36m(WorkerDict pid=2000108)[0m     "full_attention",
[36m(WorkerDict pid=2000108)[0m     "full_attention",
[36m(WorkerDict pid=2000108)[0m     "full_attention"
[36m(WorkerDict pid=2000108)[0m   ],
[36m(WorkerDict pid=2000108)[0m   "max_position_embeddings": 65536,
[36m(WorkerDict pid=2000108)[0m   "max_window_layers": 48,
[36m(WorkerDict pid=2000108)[0m   "model_type": "ouro",
[36m(WorkerDict pid=2000108)[0m   "num_attention_heads": 16,
[36m(WorkerDict pid=2000108)[0m   "num_hidden_layers": 48,
[36m(WorkerDict pid=2000108)[0m   "num_key_value_heads": 16,
[36m(WorkerDict pid=2000108)[0m   "pad_token_id": 0,
[36m(WorkerDict pid=2000108)[0m   "rltt_logprob_chunk_size": 2048,
[36m(WorkerDict pid=2000108)[0m   "rltt_loop_level_checkpointing": true,
[36m(WorkerDict pid=2000108)[0m   "rms_norm_eps": 1e-06,
[36m(WorkerDict pid=2000108)[0m   "rope_scaling": null,
[36m(WorkerDict pid=2000108)[0m   "rope_theta": 1000000.0,
[36m(WorkerDict pid=2000108)[0m   "sliding_window": null,
[36m(WorkerDict pid=2000108)[0m   "tie_word_embeddings": false,
[36m(WorkerDict pid=2000108)[0m   "total_ut_steps": 4,
[36m(WorkerDict pid=2000108)[0m   "transformers_version": "4.57.3",
[36m(WorkerDict pid=2000108)[0m   "use_cache": true,
[36m(WorkerDict pid=2000108)[0m   "use_sliding_window": false,
[36m(WorkerDict pid=2000108)[0m   "vocab_size": 49152
[36m(WorkerDict pid=2000108)[0m }
[36m(WorkerDict pid=2000108)[0m 
[36m(WorkerDict pid=2000109)[0m ðŸš¨ `use_weighted_exit` is part of OuroForCausalLM.forward's signature, but not documented. Make sure to add it to the docstring of the function in /scratch/gpfs/OLGARUS/jw4199/model_weights_path/transformers_modules/Ouro_hyphen_2_dot_6B_hyphen_Thinking_hyphen_RLTT/modeling_ouro.py.
[36m(WorkerDict pid=2000109)[0m ðŸš¨ `exit_at_step` is part of OuroForCausalLM.forward's signature, but not documented. Make sure to add it to the docstring of the function in /scratch/gpfs/OLGARUS/jw4199/model_weights_path/transformers_modules/Ouro_hyphen_2_dot_6B_hyphen_Thinking_hyphen_RLTT/modeling_ouro.py.
[36m(WorkerDict pid=2000109)[0m ðŸš¨ `exit_threshold` is part of OuroForCausalLM.forward's signature, but not documented. Make sure to add it to the docstring of the function in /scratch/gpfs/OLGARUS/jw4199/model_weights_path/transformers_modules/Ouro_hyphen_2_dot_6B_hyphen_Thinking_hyphen_RLTT/modeling_ouro.py.
[36m(WorkerDict pid=2000110)[0m ðŸš¨ `use_weighted_exit` is part of OuroForCausalLM.forward's signature, but not documented. Make sure to add it to the docstring of the function in /scratch/gpfs/OLGARUS/jw4199/model_weights_path/transformers_modules/Ouro_hyphen_2_dot_6B_hyphen_Thinking_hyphen_RLTT/modeling_ouro.py.
[36m(WorkerDict pid=2000110)[0m ðŸš¨ `exit_at_step` is part of OuroForCausalLM.forward's signature, but not documented. Make sure to add it to the docstring of the function in /scratch/gpfs/OLGARUS/jw4199/model_weights_path/transformers_modules/Ouro_hyphen_2_dot_6B_hyphen_Thinking_hyphen_RLTT/modeling_ouro.py.
[36m(WorkerDict pid=2000110)[0m ðŸš¨ `exit_threshold` is part of OuroForCausalLM.forward's signature, but not documented. Make sure to add it to the docstring of the function in /scratch/gpfs/OLGARUS/jw4199/model_weights_path/transformers_modules/Ouro_hyphen_2_dot_6B_hyphen_Thinking_hyphen_RLTT/modeling_ouro.py.
[36m(WorkerDict pid=2000111)[0m ðŸš¨ `use_weighted_exit` is part of OuroForCausalLM.forward's signature, but not documented. Make sure to add it to the docstring of the function in /scratch/gpfs/OLGARUS/jw4199/model_weights_path/transformers_modules/Ouro_hyphen_2_dot_6B_hyphen_Thinking_hyphen_RLTT/modeling_ouro.py.
[36m(WorkerDict pid=2000111)[0m ðŸš¨ `exit_at_step` is part of OuroForCausalLM.forward's signature, but not documented. Make sure to add it to the docstring of the function in /scratch/gpfs/OLGARUS/jw4199/model_weights_path/transformers_modules/Ouro_hyphen_2_dot_6B_hyphen_Thinking_hyphen_RLTT/modeling_ouro.py.
[36m(WorkerDict pid=2000111)[0m ðŸš¨ `exit_threshold` is part of OuroForCausalLM.forward's signature, but not documented. Make sure to add it to the docstring of the function in /scratch/gpfs/OLGARUS/jw4199/model_weights_path/transformers_modules/Ouro_hyphen_2_dot_6B_hyphen_Thinking_hyphen_RLTT/modeling_ouro.py.
[36m(WorkerDict pid=2000108)[0m ðŸš¨ `use_weighted_exit` is part of OuroForCausalLM.forward's signature, but not documented. Make sure to add it to the docstring of the function in /scratch/gpfs/OLGARUS/jw4199/model_weights_path/transformers_modules/Ouro_hyphen_2_dot_6B_hyphen_Thinking_hyphen_RLTT/modeling_ouro.py.
[36m(WorkerDict pid=2000108)[0m ðŸš¨ `exit_at_step` is part of OuroForCausalLM.forward's signature, but not documented. Make sure to add it to the docstring of the function in /scratch/gpfs/OLGARUS/jw4199/model_weights_path/transformers_modules/Ouro_hyphen_2_dot_6B_hyphen_Thinking_hyphen_RLTT/modeling_ouro.py.
[36m(WorkerDict pid=2000108)[0m ðŸš¨ `exit_threshold` is part of OuroForCausalLM.forward's signature, but not documented. Make sure to add it to the docstring of the function in /scratch/gpfs/OLGARUS/jw4199/model_weights_path/transformers_modules/Ouro_hyphen_2_dot_6B_hyphen_Thinking_hyphen_RLTT/modeling_ouro.py.
[36m(WorkerDict pid=2000109)[0m Monkey patch _flash_attention_forward in transformers.integrations.flash_attention
[36m(WorkerDict pid=2000109)[0m Skipping monkey patch for OuroForCausalLM as use_fused_kernels is False or fused_kernels_backend is None
[36m(WorkerDict pid=2000110)[0m Monkey patch _flash_attention_forward in transformers.integrations.flash_attention
[36m(WorkerDict pid=2000110)[0m Skipping monkey patch for OuroForCausalLM as use_fused_kernels is False or fused_kernels_backend is None
[36m(WorkerDict pid=2000111)[0m Monkey patch _flash_attention_forward in transformers.integrations.flash_attention
[36m(WorkerDict pid=2000111)[0m Skipping monkey patch for OuroForCausalLM as use_fused_kernels is False or fused_kernels_backend is None
[36m(WorkerDict pid=2000108)[0m Monkey patch _flash_attention_forward in transformers.integrations.flash_attention
[36m(WorkerDict pid=2000108)[0m Skipping monkey patch for OuroForCausalLM as use_fused_kernels is False or fused_kernels_backend is None
[36m(WorkerDict pid=2000108)[0m OuroForCausalLM contains 2.67B parameters
[36m(WorkerDict pid=2000108)[0m wrap_policy: functools.partial(<function _or_policy at 0x1460ed70e8c0>, policies=[functools.partial(<function transformer_auto_wrap_policy at 0x1460ed70e7a0>, transformer_layer_cls={<class 'transformers_modules.Ouro_hyphen_2_dot_6B_hyphen_Thinking_hyphen_RLTT.modeling_ouro.OuroDecoderLayer'>})])
[36m(WorkerDict pid=2000108)[0m Total steps: 3, num_warmup_steps: 0
[36m(WorkerDict pid=2000108)[0m Actor use_remove_padding=False
[36m(WorkerDict pid=2000108)[0m Actor use_fused_kernels=False
[36m(WorkerDict pid=2000109)[0m [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[36m(WorkerDict pid=2000109)[0m [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[36m(WorkerDict pid=2000109)[0m [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[36m(WorkerDict pid=2000110)[0m [Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[36m(WorkerDict pid=2000110)[0m [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[36m(WorkerDict pid=2000110)[0m [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[36m(WorkerDict pid=2000111)[0m [Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[36m(WorkerDict pid=2000111)[0m [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[36m(WorkerDict pid=2000111)[0m [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[36m(WorkerDict pid=2000108)[0m [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[36m(WorkerDict pid=2000108)[0m [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[36m(WorkerDict pid=2000108)[0m [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[36m(WorkerDict pid=2000109)[0m INFO 02-15 05:09:24 [utils.py:253] non-default args: {'trust_remote_code': True, 'dtype': 'bfloat16', 'max_model_len': 3072, 'distributed_executor_backend': 'external_launcher', 'enable_prefix_caching': True, 'gpu_memory_utilization': 0.45, 'max_num_batched_tokens': 8192, 'max_num_seqs': 1024, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'model': '/scratch/gpfs/OLGARUS/jw4199/model_weights_path/Ouro-2.6B-Thinking-RLTT'}
[36m(WorkerDict pid=2000110)[0m INFO 02-15 05:09:24 [utils.py:253] non-default args: {'trust_remote_code': True, 'dtype': 'bfloat16', 'max_model_len': 3072, 'distributed_executor_backend': 'external_launcher', 'enable_prefix_caching': True, 'gpu_memory_utilization': 0.45, 'max_num_batched_tokens': 8192, 'max_num_seqs': 1024, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'model': '/scratch/gpfs/OLGARUS/jw4199/model_weights_path/Ouro-2.6B-Thinking-RLTT'}
[36m(WorkerDict pid=2000111)[0m INFO 02-15 05:09:24 [utils.py:253] non-default args: {'trust_remote_code': True, 'dtype': 'bfloat16', 'max_model_len': 3072, 'distributed_executor_backend': 'external_launcher', 'enable_prefix_caching': True, 'gpu_memory_utilization': 0.45, 'max_num_batched_tokens': 8192, 'max_num_seqs': 1024, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'model': '/scratch/gpfs/OLGARUS/jw4199/model_weights_path/Ouro-2.6B-Thinking-RLTT'}
[36m(WorkerDict pid=2000108)[0m INFO 02-15 05:09:24 [utils.py:253] non-default args: {'trust_remote_code': True, 'dtype': 'bfloat16', 'max_model_len': 3072, 'distributed_executor_backend': 'external_launcher', 'enable_prefix_caching': True, 'gpu_memory_utilization': 0.45, 'max_num_batched_tokens': 8192, 'max_num_seqs': 1024, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'model': '/scratch/gpfs/OLGARUS/jw4199/model_weights_path/Ouro-2.6B-Thinking-RLTT'}
[36m(WorkerDict pid=2000109)[0m INFO 02-15 05:09:24 [model.py:637] Resolved architecture: OuroForCausalLM
[36m(WorkerDict pid=2000109)[0m INFO 02-15 05:09:24 [model.py:1750] Using max model len 3072
[36m(WorkerDict pid=2000109)[0m INFO 02-15 05:09:24 [arg_utils.py:1427] Using ray runtime env (env vars redacted): {'env_vars': {'MASTER_ADDR': '***', 'MASTER_PORT': '***', 'RANK': '***', 'RAY_LOCAL_WORLD_SIZE': '***', 'WG_BACKEND': '***', 'WG_PREFIX': '***', 'WORLD_SIZE': '***'}}
[36m(WorkerDict pid=2000109)[0m INFO 02-15 05:09:24 [parallel.py:511] Using external launcher for distributed inference.
[36m(WorkerDict pid=2000109)[0m INFO 02-15 05:09:24 [parallel.py:545] Disabling V1 multiprocessing for external launcher.
[36m(WorkerDict pid=2000109)[0m INFO 02-15 05:09:24 [scheduler.py:228] Chunked prefill is enabled with max_num_batched_tokens=8192.
[36m(WorkerDict pid=2000110)[0m INFO 02-15 05:09:24 [model.py:637] Resolved architecture: OuroForCausalLM
[36m(WorkerDict pid=2000110)[0m INFO 02-15 05:09:24 [model.py:1750] Using max model len 3072
[36m(WorkerDict pid=2000110)[0m INFO 02-15 05:09:24 [arg_utils.py:1427] Using ray runtime env (env vars redacted): {'env_vars': {'MASTER_ADDR': '***', 'MASTER_PORT': '***', 'RANK': '***', 'RAY_LOCAL_WORLD_SIZE': '***', 'WG_BACKEND': '***', 'WG_PREFIX': '***', 'WORLD_SIZE': '***'}}
[36m(WorkerDict pid=2000110)[0m INFO 02-15 05:09:24 [parallel.py:511] Using external launcher for distributed inference.
[36m(WorkerDict pid=2000110)[0m INFO 02-15 05:09:24 [parallel.py:545] Disabling V1 multiprocessing for external launcher.
[36m(WorkerDict pid=2000110)[0m INFO 02-15 05:09:24 [scheduler.py:228] Chunked prefill is enabled with max_num_batched_tokens=8192.
[36m(WorkerDict pid=2000111)[0m INFO 02-15 05:09:24 [model.py:637] Resolved architecture: OuroForCausalLM
[36m(WorkerDict pid=2000111)[0m INFO 02-15 05:09:24 [model.py:1750] Using max model len 3072
[36m(WorkerDict pid=2000111)[0m INFO 02-15 05:09:24 [arg_utils.py:1427] Using ray runtime env (env vars redacted): {'env_vars': {'MASTER_ADDR': '***', 'MASTER_PORT': '***', 'RANK': '***', 'RAY_LOCAL_WORLD_SIZE': '***', 'WG_BACKEND': '***', 'WG_PREFIX': '***', 'WORLD_SIZE': '***'}}
[36m(WorkerDict pid=2000111)[0m INFO 02-15 05:09:24 [parallel.py:511] Using external launcher for distributed inference.
[36m(WorkerDict pid=2000111)[0m INFO 02-15 05:09:24 [parallel.py:545] Disabling V1 multiprocessing for external launcher.
[36m(WorkerDict pid=2000111)[0m INFO 02-15 05:09:24 [scheduler.py:228] Chunked prefill is enabled with max_num_batched_tokens=8192.
[36m(WorkerDict pid=2000108)[0m INFO 02-15 05:09:24 [model.py:637] Resolved architecture: OuroForCausalLM
[36m(WorkerDict pid=2000108)[0m INFO 02-15 05:09:24 [model.py:1750] Using max model len 3072
[36m(WorkerDict pid=2000108)[0m INFO 02-15 05:09:24 [arg_utils.py:1427] Using ray runtime env (env vars redacted): {'env_vars': {'MASTER_ADDR': '***', 'MASTER_PORT': '***', 'RANK': '***', 'RAY_LOCAL_WORLD_SIZE': '***', 'WG_BACKEND': '***', 'WG_PREFIX': '***', 'WORLD_SIZE': '***'}}
[36m(WorkerDict pid=2000108)[0m INFO 02-15 05:09:24 [parallel.py:511] Using external launcher for distributed inference.
[36m(WorkerDict pid=2000108)[0m INFO 02-15 05:09:24 [parallel.py:545] Disabling V1 multiprocessing for external launcher.
[36m(WorkerDict pid=2000108)[0m INFO 02-15 05:09:24 [scheduler.py:228] Chunked prefill is enabled with max_num_batched_tokens=8192.
[36m(WorkerDict pid=2000109)[0m INFO 02-15 05:09:24 [core.py:93] Initializing a V1 LLM engine (v0.12.0) with config: model='/scratch/gpfs/OLGARUS/jw4199/model_weights_path/Ouro-2.6B-Thinking-RLTT', speculative_config=None, tokenizer='/scratch/gpfs/OLGARUS/jw4199/model_weights_path/Ouro-2.6B-Thinking-RLTT', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=3072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01), seed=0, served_model_name=/scratch/gpfs/OLGARUS/jw4199/model_weights_path/Ouro-2.6B-Thinking-RLTT, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>}, 'local_cache_dir': None}
[36m(WorkerDict pid=2000110)[0m INFO 02-15 05:09:24 [core.py:93] Initializing a V1 LLM engine (v0.12.0) with config: model='/scratch/gpfs/OLGARUS/jw4199/model_weights_path/Ouro-2.6B-Thinking-RLTT', speculative_config=None, tokenizer='/scratch/gpfs/OLGARUS/jw4199/model_weights_path/Ouro-2.6B-Thinking-RLTT', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=3072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01), seed=0, served_model_name=/scratch/gpfs/OLGARUS/jw4199/model_weights_path/Ouro-2.6B-Thinking-RLTT, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>}, 'local_cache_dir': None}
[36m(WorkerDict pid=2000111)[0m INFO 02-15 05:09:24 [core.py:93] Initializing a V1 LLM engine (v0.12.0) with config: model='/scratch/gpfs/OLGARUS/jw4199/model_weights_path/Ouro-2.6B-Thinking-RLTT', speculative_config=None, tokenizer='/scratch/gpfs/OLGARUS/jw4199/model_weights_path/Ouro-2.6B-Thinking-RLTT', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=3072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01), seed=0, served_model_name=/scratch/gpfs/OLGARUS/jw4199/model_weights_path/Ouro-2.6B-Thinking-RLTT, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>}, 'local_cache_dir': None}
[36m(WorkerDict pid=2000108)[0m INFO 02-15 05:09:24 [core.py:93] Initializing a V1 LLM engine (v0.12.0) with config: model='/scratch/gpfs/OLGARUS/jw4199/model_weights_path/Ouro-2.6B-Thinking-RLTT', speculative_config=None, tokenizer='/scratch/gpfs/OLGARUS/jw4199/model_weights_path/Ouro-2.6B-Thinking-RLTT', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=3072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01), seed=0, served_model_name=/scratch/gpfs/OLGARUS/jw4199/model_weights_path/Ouro-2.6B-Thinking-RLTT, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>}, 'local_cache_dir': None}
[36m(WorkerDict pid=2000109)[0m INFO 02-15 05:09:29 [parallel_state.py:1408] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
[36m(WorkerDict pid=2000110)[0m INFO 02-15 05:09:29 [parallel_state.py:1408] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
[36m(WorkerDict pid=2000111)[0m INFO 02-15 05:09:29 [parallel_state.py:1408] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
[36m(WorkerDict pid=2000108)[0m INFO 02-15 05:09:29 [parallel_state.py:1408] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
[36m(WorkerDict pid=2000108)[0m INFO 02-15 05:09:29 [gpu_model_runner.py:3467] Starting to load model /scratch/gpfs/OLGARUS/jw4199/model_weights_path/Ouro-2.6B-Thinking-RLTT...
[36m(WorkerDict pid=2000109)[0m INFO 02-15 05:09:30 [cuda.py:411] Using FLASH_ATTN attention backend out of potential backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[36m(WorkerDict pid=2000110)[0m INFO 02-15 05:09:30 [cuda.py:411] Using FLASH_ATTN attention backend out of potential backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[36m(WorkerDict pid=2000111)[0m INFO 02-15 05:09:30 [cuda.py:411] Using FLASH_ATTN attention backend out of potential backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[36m(WorkerDict pid=2000108)[0m INFO 02-15 05:09:30 [cuda.py:411] Using FLASH_ATTN attention backend out of potential backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[36m(WorkerDict pid=2000109)[0m [DEBUG OuroModel.load_weights] early_exit params in model: ['early_exit_gate.weight', 'early_exit_gate.bias']
[36m(WorkerDict pid=2000109)[0m [DEBUG OuroModel.load_weights] Processing weight: 'early_exit_gate.bias' shape=torch.Size([1])
[36m(WorkerDict pid=2000109)[0m [DEBUG OuroModel.load_weights] Processing weight: 'early_exit_gate.weight' shape=torch.Size([1, 2048])
[36m(WorkerDict pid=2000110)[0m [DEBUG OuroModel.load_weights] early_exit params in model: ['early_exit_gate.weight', 'early_exit_gate.bias']
[36m(WorkerDict pid=2000110)[0m [DEBUG OuroModel.load_weights] Processing weight: 'early_exit_gate.bias' shape=torch.Size([1])
[36m(WorkerDict pid=2000110)[0m [DEBUG OuroModel.load_weights] Processing weight: 'early_exit_gate.weight' shape=torch.Size([1, 2048])
[36m(WorkerDict pid=2000111)[0m [DEBUG OuroModel.load_weights] early_exit params in model: ['early_exit_gate.weight', 'early_exit_gate.bias']
[36m(WorkerDict pid=2000111)[0m [DEBUG OuroModel.load_weights] Processing weight: 'early_exit_gate.bias' shape=torch.Size([1])
[36m(WorkerDict pid=2000111)[0m [DEBUG OuroModel.load_weights] Processing weight: 'early_exit_gate.weight' shape=torch.Size([1, 2048])
[36m(WorkerDict pid=2000108)[0m [DEBUG OuroModel.load_weights] early_exit params in model: ['early_exit_gate.weight', 'early_exit_gate.bias']
[36m(WorkerDict pid=2000108)[0m [DEBUG OuroModel.load_weights] Processing weight: 'early_exit_gate.bias' shape=torch.Size([1])
[36m(WorkerDict pid=2000108)[0m [DEBUG OuroModel.load_weights] Processing weight: 'early_exit_gate.weight' shape=torch.Size([1, 2048])
[36m(WorkerDict pid=2000109)[0m INFO 02-15 05:09:33 [default_loader.py:308] Loading weights took 2.77 seconds
[36m(WorkerDict pid=2000110)[0m INFO 02-15 05:09:33 [default_loader.py:308] Loading weights took 2.77 seconds
[36m(WorkerDict pid=2000111)[0m INFO 02-15 05:09:33 [default_loader.py:308] Loading weights took 2.72 seconds
[36m(WorkerDict pid=2000108)[0m INFO 02-15 05:09:33 [default_loader.py:308] Loading weights took 2.84 seconds
[36m(WorkerDict pid=2000111)[0m INFO 02-15 05:09:34 [gpu_model_runner.py:3549] Model loading took 4.9858 GiB memory and 3.421177 seconds
[36m(WorkerDict pid=2000109)[0m INFO 02-15 05:09:34 [gpu_model_runner.py:3549] Model loading took 4.9858 GiB memory and 3.541487 seconds
[36m(WorkerDict pid=2000110)[0m INFO 02-15 05:09:34 [gpu_model_runner.py:3549] Model loading took 4.9858 GiB memory and 3.523114 seconds
[36m(WorkerDict pid=2000108)[0m INFO 02-15 05:09:34 [gpu_model_runner.py:3549] Model loading took 4.9858 GiB memory and 3.569024 seconds
[36m(WorkerDict pid=2000109)[0m INFO 02-15 05:10:06 [backends.py:655] Using cache directory: /scratch/gpfs/OLGARUS/jw4199/model_weights_path/.xdg_cache/vllm/torch_compile_cache/046aa6253d/rank_1_0/backbone for vLLM's torch.compile
[36m(WorkerDict pid=2000109)[0m INFO 02-15 05:10:06 [backends.py:715] Dynamo bytecode transform time: 32.14 s
[36m(WorkerDict pid=2000110)[0m INFO 02-15 05:10:06 [backends.py:655] Using cache directory: /scratch/gpfs/OLGARUS/jw4199/model_weights_path/.xdg_cache/vllm/torch_compile_cache/046aa6253d/rank_2_0/backbone for vLLM's torch.compile
[36m(WorkerDict pid=2000110)[0m INFO 02-15 05:10:06 [backends.py:715] Dynamo bytecode transform time: 32.14 s
[36m(WorkerDict pid=2000111)[0m INFO 02-15 05:10:06 [backends.py:655] Using cache directory: /scratch/gpfs/OLGARUS/jw4199/model_weights_path/.xdg_cache/vllm/torch_compile_cache/046aa6253d/rank_3_0/backbone for vLLM's torch.compile
[36m(WorkerDict pid=2000111)[0m INFO 02-15 05:10:06 [backends.py:715] Dynamo bytecode transform time: 32.22 s
[36m(WorkerDict pid=2000108)[0m INFO 02-15 05:10:06 [backends.py:655] Using cache directory: /scratch/gpfs/OLGARUS/jw4199/model_weights_path/.xdg_cache/vllm/torch_compile_cache/046aa6253d/rank_0_0/backbone for vLLM's torch.compile
[36m(WorkerDict pid=2000108)[0m INFO 02-15 05:10:06 [backends.py:715] Dynamo bytecode transform time: 32.10 s
[36m(WorkerDict pid=2000111)[0m INFO 02-15 05:10:26 [backends.py:216] Directly load the compiled graph(s) for dynamic shape from the cache, took 17.478 s
[36m(WorkerDict pid=2000108)[0m INFO 02-15 05:10:26 [backends.py:216] Directly load the compiled graph(s) for dynamic shape from the cache, took 17.629 s
[36m(WorkerDict pid=2000109)[0m INFO 02-15 05:10:26 [backends.py:216] Directly load the compiled graph(s) for dynamic shape from the cache, took 17.687 s
[36m(WorkerDict pid=2000110)[0m INFO 02-15 05:10:26 [backends.py:216] Directly load the compiled graph(s) for dynamic shape from the cache, took 17.751 s
[36m(WorkerDict pid=2000111)[0m INFO 02-15 05:10:32 [monitor.py:34] torch.compile takes 49.70 s in total
[36m(WorkerDict pid=2000109)[0m INFO 02-15 05:10:33 [monitor.py:34] torch.compile takes 49.83 s in total
[36m(WorkerDict pid=2000110)[0m INFO 02-15 05:10:33 [monitor.py:34] torch.compile takes 49.89 s in total
[36m(WorkerDict pid=2000108)[0m INFO 02-15 05:10:33 [monitor.py:34] torch.compile takes 49.73 s in total
[36m(WorkerDict pid=2000111)[0m INFO 02-15 05:10:34 [gpu_worker.py:359] Available KV cache memory: 56.00 GiB
[36m(WorkerDict pid=2000109)[0m INFO 02-15 05:10:34 [gpu_worker.py:359] Available KV cache memory: 56.00 GiB
[36m(WorkerDict pid=2000110)[0m INFO 02-15 05:10:34 [gpu_worker.py:359] Available KV cache memory: 56.00 GiB
[36m(WorkerDict pid=2000108)[0m INFO 02-15 05:10:34 [gpu_worker.py:359] Available KV cache memory: 56.00 GiB
[36m(WorkerDict pid=2000109)[0m INFO 02-15 05:10:35 [kv_cache_utils.py:1286] GPU KV cache size: 38,224 tokens
[36m(WorkerDict pid=2000109)[0m INFO 02-15 05:10:35 [kv_cache_utils.py:1291] Maximum concurrency for 3,072 tokens per request: 12.44x
[36m(WorkerDict pid=2000110)[0m INFO 02-15 05:10:35 [kv_cache_utils.py:1286] GPU KV cache size: 38,224 tokens
[36m(WorkerDict pid=2000110)[0m INFO 02-15 05:10:35 [kv_cache_utils.py:1291] Maximum concurrency for 3,072 tokens per request: 12.44x
[36m(WorkerDict pid=2000111)[0m INFO 02-15 05:10:35 [kv_cache_utils.py:1286] GPU KV cache size: 38,224 tokens
[36m(WorkerDict pid=2000111)[0m INFO 02-15 05:10:35 [kv_cache_utils.py:1291] Maximum concurrency for 3,072 tokens per request: 12.44x
[36m(WorkerDict pid=2000108)[0m INFO 02-15 05:10:35 [kv_cache_utils.py:1286] GPU KV cache size: 38,224 tokens
[36m(WorkerDict pid=2000108)[0m INFO 02-15 05:10:35 [kv_cache_utils.py:1291] Maximum concurrency for 3,072 tokens per request: 12.44x
[36m(WorkerDict pid=2000108)[0m INFO 02-15 05:10:53 [gpu_model_runner.py:4466] Graph capturing finished in 18 secs, took 1.34 GiB
[36m(WorkerDict pid=2000108)[0m INFO 02-15 05:10:53 [core.py:254] init engine (profile, create kv cache, warmup model) took 79.25 seconds
[36m(WorkerDict pid=2000108)[0m INFO 02-15 05:10:54 [llm.py:343] Supported tasks: ('generate',)
[36m(WorkerDict pid=2000108)[0m kwargs: {'n': 1, 'logprobs': 0, 'max_tokens': 2048, 'repetition_penalty': 1.0, 'detokenize': False, 'temperature': 1.0, 'top_k': -1, 'top_p': 1.0, 'ignore_eos': False}
[36m(WorkerDict pid=2000108)[0m Only support config type of {'seed_oss', 'qwen2_moe', 'mistral', 'apertus', 'minicpmv', 'gemma3_text', 'qwen3_vl', 'minicpmo', 'qwen3_moe', 'qwen3', 'qwen2', 'qwen3_vl_moe', 'glm4v', 'llama', 'qwen2_vl', 'deepseek_v3', 'qwen2_5_vl'}, but got ouro. MFU will always be zero.
[36m(WorkerDict pid=2000108)[0m [RLTT WORKER DEBUG] Replacing actor with RLTTDataParallelPPOActor on rank 0
[36m(WorkerDict pid=2000108)[0m Actor use_remove_padding=False
[36m(WorkerDict pid=2000108)[0m Actor use_fused_kernels=False
[36m(WorkerDict pid=2000108)[0m [RLTT DEBUG] _check_model_rltt_support: actor_module type = <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
[36m(WorkerDict pid=2000108)[0m [RLTT DEBUG] hasattr checks: _fsdp_wrapped_module=True, _orig_mod=False, module=True
[36m(WorkerDict pid=2000108)[0m [RLTT DEBUG] Unwrapped _fsdp_wrapped_module -> <class 'transformers_modules.Ouro_hyphen_2_dot_6B_hyphen_Thinking_hyphen_RLTT.modeling_ouro.OuroForCausalLM'>
[36m(WorkerDict pid=2000108)[0m [RLTT DEBUG] Final unwrapped model type = <class 'transformers_modules.Ouro_hyphen_2_dot_6B_hyphen_Thinking_hyphen_RLTT.modeling_ouro.OuroForCausalLM'>
[36m(WorkerDict pid=2000108)[0m [RLTT DEBUG] Forward params (first 10): ['input_ids', 'attention_mask', 'position_ids', 'past_key_values', 'inputs_embeds', 'labels', 'use_cache', 'cache_position', 'logits_to_keep', 'use_weighted_exit']
[36m(WorkerDict pid=2000108)[0m [RLTT DEBUG] Forward params check: log_probs=True, logits=True, exit_pdf=True -> True
[36m(WorkerDict pid=2000108)[0m RLTT Actor: Model supports native per-loop logits
[36m(WorkerDict pid=2000108)[0m RLTT Actor: loop_weighting=exit_pdf, alpha=1.0, T_max=4
[36m(WorkerDict pid=2000108)[0m Created RLTTDataParallelPPOActor with:
[36m(WorkerDict pid=2000108)[0m   loop_weighting=exit_pdf
[36m(WorkerDict pid=2000108)[0m   progressive_alpha=1.0
[36m(WorkerDict pid=2000108)[0m   total_ut_steps=4
[36m(WorkerDict pid=2000109)[0m INFO 02-15 05:10:54 [gpu_model_runner.py:4466] Graph capturing finished in 19 secs, took 1.34 GiB
[36m(WorkerDict pid=2000109)[0m INFO 02-15 05:10:54 [core.py:254] init engine (profile, create kv cache, warmup model) took 80.53 seconds
[36m(WorkerDict pid=2000111)[0m INFO 02-15 05:10:54 [gpu_model_runner.py:4466] Graph capturing finished in 19 secs, took 1.34 GiB
[36m(WorkerDict pid=2000111)[0m INFO 02-15 05:10:54 [core.py:254] init engine (profile, create kv cache, warmup model) took 80.63 seconds
[36m(WorkerDict pid=2000110)[0m INFO 02-15 05:10:55 [gpu_model_runner.py:4466] Graph capturing finished in 19 secs, took 1.34 GiB
[36m(WorkerDict pid=2000110)[0m INFO 02-15 05:10:55 [core.py:254] init engine (profile, create kv cache, warmup model) took 80.76 seconds
[36m(WorkerDict pid=2000109)[0m INFO 02-15 05:10:55 [llm.py:343] Supported tasks: ('generate',)
[36m(WorkerDict pid=2000109)[0m kwargs: {'n': 1, 'logprobs': 0, 'max_tokens': 2048, 'repetition_penalty': 1.0, 'detokenize': False, 'temperature': 1.0, 'top_k': -1, 'top_p': 1.0, 'ignore_eos': False}
[36m(WorkerDict pid=2000109)[0m Only support config type of {'qwen3_moe', 'minicpmv', 'qwen2_5_vl', 'qwen3_vl_moe', 'llama', 'qwen2_moe', 'glm4v', 'qwen3_vl', 'qwen2_vl', 'minicpmo', 'mistral', 'seed_oss', 'qwen3', 'deepseek_v3', 'gemma3_text', 'apertus', 'qwen2'}, but got ouro. MFU will always be zero.
[36m(WorkerDict pid=2000109)[0m [RLTT WORKER DEBUG] Replacing actor with RLTTDataParallelPPOActor on rank 1
[36m(WorkerDict pid=2000109)[0m [RLTT DEBUG] _check_model_rltt_support: actor_module type = <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
[36m(WorkerDict pid=2000109)[0m [RLTT DEBUG] hasattr checks: _fsdp_wrapped_module=True, _orig_mod=False, module=True
[36m(WorkerDict pid=2000109)[0m [RLTT DEBUG] Unwrapped _fsdp_wrapped_module -> <class 'transformers_modules.Ouro_hyphen_2_dot_6B_hyphen_Thinking_hyphen_RLTT.modeling_ouro.OuroForCausalLM'>
[36m(WorkerDict pid=2000109)[0m [RLTT DEBUG] Final unwrapped model type = <class 'transformers_modules.Ouro_hyphen_2_dot_6B_hyphen_Thinking_hyphen_RLTT.modeling_ouro.OuroForCausalLM'>
[36m(WorkerDict pid=2000109)[0m [RLTT DEBUG] Forward params (first 10): ['input_ids', 'attention_mask', 'position_ids', 'past_key_values', 'inputs_embeds', 'labels', 'use_cache', 'cache_position', 'logits_to_keep', 'use_weighted_exit']
[36m(WorkerDict pid=2000109)[0m [RLTT DEBUG] Forward params check: log_probs=True, logits=True, exit_pdf=True -> True
[36m(WorkerDict pid=2000109)[0m RLTT Actor: Model supports native per-loop logits
[36m(WorkerDict pid=2000109)[0m RLTT Actor: loop_weighting=exit_pdf, alpha=1.0, T_max=4
[36m(WorkerDict pid=2000109)[0m [RLTT WORKER DEBUG] init_model() completed on rank 1
[36m(WorkerDict pid=2000111)[0m INFO 02-15 05:10:55 [llm.py:343] Supported tasks: ('generate',)
[36m(WorkerDict pid=2000111)[0m kwargs: {'n': 1, 'logprobs': 0, 'max_tokens': 2048, 'repetition_penalty': 1.0, 'detokenize': False, 'temperature': 1.0, 'top_k': -1, 'top_p': 1.0, 'ignore_eos': False}
[36m(WorkerDict pid=2000111)[0m Only support config type of {'qwen3_vl_moe', 'gemma3_text', 'qwen2', 'deepseek_v3', 'apertus', 'qwen3', 'mistral', 'seed_oss', 'qwen2_5_vl', 'qwen2_moe', 'glm4v', 'qwen3_moe', 'qwen2_vl', 'llama', 'qwen3_vl', 'minicpmo', 'minicpmv'}, but got ouro. MFU will always be zero.
[36m(WorkerDict pid=2000111)[0m [RLTT WORKER DEBUG] Replacing actor with RLTTDataParallelPPOActor on rank 3
[36m(WorkerDict pid=2000111)[0m [RLTT DEBUG] _check_model_rltt_support: actor_module type = <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
[36m(WorkerDict pid=2000111)[0m [RLTT DEBUG] hasattr checks: _fsdp_wrapped_module=True, _orig_mod=False, module=True
[36m(WorkerDict pid=2000111)[0m [RLTT DEBUG] Unwrapped _fsdp_wrapped_module -> <class 'transformers_modules.Ouro_hyphen_2_dot_6B_hyphen_Thinking_hyphen_RLTT.modeling_ouro.OuroForCausalLM'>
[36m(WorkerDict pid=2000111)[0m [RLTT DEBUG] Final unwrapped model type = <class 'transformers_modules.Ouro_hyphen_2_dot_6B_hyphen_Thinking_hyphen_RLTT.modeling_ouro.OuroForCausalLM'>
[36m(WorkerDict pid=2000111)[0m [RLTT DEBUG] Forward params (first 10): ['input_ids', 'attention_mask', 'position_ids', 'past_key_values', 'inputs_embeds', 'labels', 'use_cache', 'cache_position', 'logits_to_keep', 'use_weighted_exit']
[36m(WorkerDict pid=2000111)[0m [RLTT DEBUG] Forward params check: log_probs=True, logits=True, exit_pdf=True -> True
[36m(WorkerDict pid=2000111)[0m RLTT Actor: Model supports native per-loop logits
[36m(WorkerDict pid=2000111)[0m RLTT Actor: loop_weighting=exit_pdf, alpha=1.0, T_max=4
[36m(WorkerDict pid=2000111)[0m [RLTT WORKER DEBUG] init_model() completed on rank 3
[36m(WorkerDict pid=2000110)[0m INFO 02-15 05:10:55 [llm.py:343] Supported tasks: ('generate',)
[36m(WorkerDict pid=2000110)[0m kwargs: {'n': 1, 'logprobs': 0, 'max_tokens': 2048, 'repetition_penalty': 1.0, 'detokenize': False, 'temperature': 1.0, 'top_k': -1, 'top_p': 1.0, 'ignore_eos': False}
[36m(WorkerDict pid=2000110)[0m Only support config type of {'deepseek_v3', 'llama', 'qwen3', 'qwen2', 'gemma3_text', 'seed_oss', 'apertus', 'qwen2_vl', 'minicpmv', 'qwen2_moe', 'qwen3_moe', 'mistral', 'qwen3_vl_moe', 'qwen2_5_vl', 'glm4v', 'qwen3_vl', 'minicpmo'}, but got ouro. MFU will always be zero.
[36m(WorkerDict pid=2000110)[0m [RLTT WORKER DEBUG] Replacing actor with RLTTDataParallelPPOActor on rank 2
[36m(WorkerDict pid=2000110)[0m [RLTT DEBUG] _check_model_rltt_support: actor_module type = <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>
[36m(WorkerDict pid=2000110)[0m [RLTT DEBUG] hasattr checks: _fsdp_wrapped_module=True, _orig_mod=False, module=True
[36m(WorkerDict pid=2000110)[0m [RLTT DEBUG] Unwrapped _fsdp_wrapped_module -> <class 'transformers_modules.Ouro_hyphen_2_dot_6B_hyphen_Thinking_hyphen_RLTT.modeling_ouro.OuroForCausalLM'>
[36m(WorkerDict pid=2000110)[0m [RLTT DEBUG] Final unwrapped model type = <class 'transformers_modules.Ouro_hyphen_2_dot_6B_hyphen_Thinking_hyphen_RLTT.modeling_ouro.OuroForCausalLM'>
[36m(WorkerDict pid=2000110)[0m [RLTT DEBUG] Forward params (first 10): ['input_ids', 'attention_mask', 'position_ids', 'past_key_values', 'inputs_embeds', 'labels', 'use_cache', 'cache_position', 'logits_to_keep', 'use_weighted_exit']
[36m(WorkerDict pid=2000110)[0m [RLTT DEBUG] Forward params check: log_probs=True, logits=True, exit_pdf=True -> True
[36m(WorkerDict pid=2000110)[0m RLTT Actor: Model supports native per-loop logits
[36m(WorkerDict pid=2000110)[0m RLTT Actor: loop_weighting=exit_pdf, alpha=1.0, T_max=4
[36m(WorkerDict pid=2000110)[0m [RLTT WORKER DEBUG] init_model() completed on rank 2
[36m(WorkerDict pid=2000108)[0m Parameter summary (summed across 4 FSDP shards):
[36m(WorkerDict pid=2000108)[0m   Total params:     2,667,974,660
[36m(WorkerDict pid=2000108)[0m   Trainable params: 2,667,974,660 (100.00%)
[36m(WorkerDict pid=2000108)[0m   Frozen params:    0 (0.00%)
[36m(WorkerDict pid=2000108)[0m [RLTT WORKER DEBUG] init_model() completed on rank 0
[36m(WorkerDict pid=2000109)[0m [DEBUG OuroModel.load_weights] early_exit params in model: ['early_exit_gate.weight', 'early_exit_gate.bias']
[36m(WorkerDict pid=2000110)[0m [DEBUG OuroModel.load_weights] early_exit params in model: ['early_exit_gate.weight', 'early_exit_gate.bias']
[36m(WorkerDict pid=2000111)[0m [DEBUG OuroModel.load_weights] early_exit params in model: ['early_exit_gate.weight', 'early_exit_gate.bias']
[36m(WorkerDict pid=2000108)[0m [DEBUG OuroModel.load_weights] early_exit params in model: ['early_exit_gate.weight', 'early_exit_gate.bias']
[36m(WorkerDict pid=2000109)[0m [DEBUG OuroModel.load_weights] Processing weight: 'early_exit_gate.weight' shape=torch.Size([1, 2048])
[36m(WorkerDict pid=2000109)[0m [DEBUG OuroModel.load_weights] Processing weight: 'early_exit_gate.bias' shape=torch.Size([1])
[36m(WorkerDict pid=2000110)[0m [DEBUG OuroModel.load_weights] Processing weight: 'early_exit_gate.weight' shape=torch.Size([1, 2048])
[36m(WorkerDict pid=2000110)[0m [DEBUG OuroModel.load_weights] Processing weight: 'early_exit_gate.bias' shape=torch.Size([1])
[36m(WorkerDict pid=2000111)[0m [DEBUG OuroModel.load_weights] Processing weight: 'early_exit_gate.weight' shape=torch.Size([1, 2048])
[36m(WorkerDict pid=2000111)[0m [DEBUG OuroModel.load_weights] Processing weight: 'early_exit_gate.bias' shape=torch.Size([1])
[36m(WorkerDict pid=2000108)[0m [DEBUG OuroModel.load_weights] Processing weight: 'early_exit_gate.weight' shape=torch.Size([1, 2048])
[36m(WorkerDict pid=2000108)[0m [DEBUG OuroModel.load_weights] Processing weight: 'early_exit_gate.bias' shape=torch.Size([1])
[36m(WorkerDict pid=2000109)[0m [RLTT DEBUG] compute_log_prob() called, calculate_entropy=True
[36m(WorkerDict pid=2000110)[0m [RLTT DEBUG] compute_log_prob() called, calculate_entropy=True
[36m(WorkerDict pid=2000111)[0m [RLTT DEBUG] compute_log_prob() called, calculate_entropy=True
[36m(WorkerDict pid=2000108)[0m [RLTT DEBUG] compute_log_prob() called, calculate_entropy=True
[36m(WorkerDict pid=2000109)[0m [RLTT DEBUG] compute_log_prob() completed
[36m(WorkerDict pid=2000110)[0m [RLTT DEBUG] compute_log_prob() completed
[36m(WorkerDict pid=2000111)[0m [RLTT DEBUG] compute_log_prob() completed
[36m(WorkerDict pid=2000108)[0m [RLTT DEBUG] compute_log_prob() completed
local_global_step_folder: ./rltt_output/4753520/global_step_1
[36m(WorkerDict pid=2000109)[0m [DEBUG OuroModel.load_weights] early_exit params in model: ['early_exit_gate.weight', 'early_exit_gate.bias']
[36m(WorkerDict pid=2000109)[0m [DEBUG OuroModel.load_weights] Processing weight: 'early_exit_gate.weight' shape=torch.Size([1, 2048])
[36m(WorkerDict pid=2000109)[0m [DEBUG OuroModel.load_weights] Processing weight: 'early_exit_gate.bias' shape=torch.Size([1])
[36m(WorkerDict pid=2000110)[0m [DEBUG OuroModel.load_weights] early_exit params in model: ['early_exit_gate.weight', 'early_exit_gate.bias']
[36m(WorkerDict pid=2000110)[0m [DEBUG OuroModel.load_weights] Processing weight: 'early_exit_gate.weight' shape=torch.Size([1, 2048])
[36m(WorkerDict pid=2000110)[0m [DEBUG OuroModel.load_weights] Processing weight: 'early_exit_gate.bias' shape=torch.Size([1])
[36m(WorkerDict pid=2000111)[0m [DEBUG OuroModel.load_weights] early_exit params in model: ['early_exit_gate.weight', 'early_exit_gate.bias']
[36m(WorkerDict pid=2000111)[0m [DEBUG OuroModel.load_weights] Processing weight: 'early_exit_gate.weight' shape=torch.Size([1, 2048])
[36m(WorkerDict pid=2000111)[0m [DEBUG OuroModel.load_weights] Processing weight: 'early_exit_gate.bias' shape=torch.Size([1])
[36m(WorkerDict pid=2000108)[0m [DEBUG OuroModel.load_weights] early_exit params in model: ['early_exit_gate.weight', 'early_exit_gate.bias']
[36m(WorkerDict pid=2000108)[0m [DEBUG OuroModel.load_weights] Processing weight: 'early_exit_gate.weight' shape=torch.Size([1, 2048])
[36m(WorkerDict pid=2000108)[0m [DEBUG OuroModel.load_weights] Processing weight: 'early_exit_gate.bias' shape=torch.Size([1])
[36m(WorkerDict pid=2000109)[0m [RLTT DEBUG] compute_log_prob() called, calculate_entropy=True
[36m(WorkerDict pid=2000110)[0m [RLTT DEBUG] compute_log_prob() called, calculate_entropy=True
[36m(WorkerDict pid=2000111)[0m [RLTT DEBUG] compute_log_prob() called, calculate_entropy=True
[36m(WorkerDict pid=2000108)[0m [RLTT DEBUG] compute_log_prob() called, calculate_entropy=True
[36m(WorkerDict pid=2000109)[0m [RLTT DEBUG] compute_log_prob() completed
[36m(WorkerDict pid=2000110)[0m [RLTT DEBUG] compute_log_prob() completed
[36m(WorkerDict pid=2000111)[0m [RLTT DEBUG] compute_log_prob() completed
[36m(WorkerDict pid=2000108)[0m [RLTT DEBUG] compute_log_prob() completed
[36m(WorkerDict pid=2000109)[0m [RLTT DEBUG] update_policy() called, batch_size=torch.Size([128])
[36m(WorkerDict pid=2000109)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000109)[0m [RLTT MODEL] _forward_streaming_log_probs: DISABLE_LOOP_CHECKPOINTING=True, use_loop_checkpointing=False, training=True, loop_level_checkpointing=True
[36m(WorkerDict pid=2000110)[0m [RLTT DEBUG] update_policy() called, batch_size=torch.Size([128])
[36m(WorkerDict pid=2000110)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000110)[0m [RLTT MODEL] _forward_streaming_log_probs: DISABLE_LOOP_CHECKPOINTING=True, use_loop_checkpointing=False, training=True, loop_level_checkpointing=True
[36m(WorkerDict pid=2000111)[0m [RLTT DEBUG] update_policy() called, batch_size=torch.Size([128])
[36m(WorkerDict pid=2000111)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000111)[0m [RLTT MODEL] _forward_streaming_log_probs: DISABLE_LOOP_CHECKPOINTING=True, use_loop_checkpointing=False, training=True, loop_level_checkpointing=True
[36m(WorkerDict pid=2000108)[0m [RLTT DEBUG] update_policy() called, batch_size=torch.Size([128])
[36m(WorkerDict pid=2000108)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000108)[0m [RLTT MODEL] _forward_streaming_log_probs: DISABLE_LOOP_CHECKPOINTING=True, use_loop_checkpointing=False, training=True, loop_level_checkpointing=True
[36m(WorkerDict pid=2000109)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000110)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000111)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000108)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000109)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000110)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000111)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000108)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000109)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000110)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000111)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000108)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000109)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000110)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000111)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000108)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000109)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000110)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000111)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000108)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000109)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000110)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000111)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000108)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000109)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000110)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000111)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000108)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000109)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000110)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000111)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000108)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000109)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000110)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000111)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000108)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000109)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000110)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000111)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000108)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000109)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000110)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000111)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000108)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000109)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000110)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000111)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000108)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000109)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000110)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000111)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000108)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000109)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000110)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000111)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000108)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000109)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000110)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000111)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000108)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000109)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000110)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000111)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000108)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000109)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000110)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000111)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000108)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000109)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000110)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000111)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000108)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000109)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000110)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000111)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000108)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000109)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000110)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000111)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000108)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000109)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000110)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000111)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000108)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000109)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000110)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000111)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000108)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000109)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000110)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000111)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000108)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000109)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000110)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000111)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000108)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000109)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000110)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000111)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000108)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000109)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000110)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000111)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000108)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000109)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000110)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000111)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000108)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000109)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000110)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000111)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000108)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000109)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000110)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000111)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000108)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000109)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000110)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000111)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000108)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000109)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000111)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000108)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000110)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
local_global_step_folder: ./rltt_output/4753520/global_step_2
[36m(WorkerDict pid=2000109)[0m [DEBUG OuroModel.load_weights] early_exit params in model: ['early_exit_gate.weight', 'early_exit_gate.bias']
[36m(WorkerDict pid=2000109)[0m [DEBUG OuroModel.load_weights] Processing weight: 'early_exit_gate.weight' shape=torch.Size([1, 2048])
[36m(WorkerDict pid=2000109)[0m [DEBUG OuroModel.load_weights] Processing weight: 'early_exit_gate.bias' shape=torch.Size([1])
[36m(WorkerDict pid=2000110)[0m [DEBUG OuroModel.load_weights] early_exit params in model: ['early_exit_gate.weight', 'early_exit_gate.bias']
[36m(WorkerDict pid=2000110)[0m [DEBUG OuroModel.load_weights] Processing weight: 'early_exit_gate.weight' shape=torch.Size([1, 2048])
[36m(WorkerDict pid=2000110)[0m [DEBUG OuroModel.load_weights] Processing weight: 'early_exit_gate.bias' shape=torch.Size([1])
[36m(WorkerDict pid=2000111)[0m [DEBUG OuroModel.load_weights] early_exit params in model: ['early_exit_gate.weight', 'early_exit_gate.bias']
[36m(WorkerDict pid=2000111)[0m [DEBUG OuroModel.load_weights] Processing weight: 'early_exit_gate.weight' shape=torch.Size([1, 2048])
[36m(WorkerDict pid=2000111)[0m [DEBUG OuroModel.load_weights] Processing weight: 'early_exit_gate.bias' shape=torch.Size([1])
[36m(WorkerDict pid=2000108)[0m [DEBUG OuroModel.load_weights] early_exit params in model: ['early_exit_gate.weight', 'early_exit_gate.bias']
[36m(WorkerDict pid=2000108)[0m [DEBUG OuroModel.load_weights] Processing weight: 'early_exit_gate.weight' shape=torch.Size([1, 2048])
[36m(WorkerDict pid=2000108)[0m [DEBUG OuroModel.load_weights] Processing weight: 'early_exit_gate.bias' shape=torch.Size([1])
[36m(WorkerDict pid=2000109)[0m [RLTT DEBUG] compute_log_prob() called, calculate_entropy=True
[36m(WorkerDict pid=2000110)[0m [RLTT DEBUG] compute_log_prob() called, calculate_entropy=True
[36m(WorkerDict pid=2000111)[0m [RLTT DEBUG] compute_log_prob() called, calculate_entropy=True
[36m(WorkerDict pid=2000108)[0m [RLTT DEBUG] compute_log_prob() called, calculate_entropy=True
[36m(WorkerDict pid=2000109)[0m [RLTT DEBUG] compute_log_prob() completed
[36m(WorkerDict pid=2000110)[0m [RLTT DEBUG] compute_log_prob() completed
[36m(WorkerDict pid=2000111)[0m [RLTT DEBUG] compute_log_prob() completed
[36m(WorkerDict pid=2000108)[0m [RLTT DEBUG] compute_log_prob() completed
[36m(WorkerDict pid=2000109)[0m [RLTT DEBUG] update_policy() called, batch_size=torch.Size([64])
[36m(WorkerDict pid=2000109)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000110)[0m [RLTT DEBUG] update_policy() called, batch_size=torch.Size([64])
[36m(WorkerDict pid=2000110)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000111)[0m [RLTT DEBUG] update_policy() called, batch_size=torch.Size([64])
[36m(WorkerDict pid=2000111)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000108)[0m [RLTT DEBUG] update_policy() called, batch_size=torch.Size([64])
[36m(WorkerDict pid=2000108)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000109)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000110)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000111)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000108)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000109)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000110)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000111)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000108)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000109)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000110)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000111)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000108)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000109)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000110)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000111)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000108)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000109)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000110)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000111)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000108)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000109)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000110)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000111)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000108)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000109)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000110)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000111)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000108)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000109)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000110)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000111)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000108)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000109)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000110)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000111)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000108)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000109)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000110)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000111)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000108)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000109)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000110)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000111)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000108)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000109)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000110)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000111)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000108)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000109)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000110)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000111)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000108)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000109)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000110)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000111)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000108)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000109)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000110)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000111)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
[36m(WorkerDict pid=2000108)[0m [RLTT DEBUG] _model_supports_per_loop_log_probs: model_type=OuroForCausalLM, result=True
local_global_step_folder: ./rltt_output/4753520/global_step_3
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /scratch/gpfs/OLGARUS/jw4199/model_weights_path/wandb/offline-run-20260215_051057-51bq5xlo[0m
[1;34mwandb[0m: Find logs at: [1;35m../../../model_weights_path/wandb/offline-run-20260215_051057-51bq5xlo/logs[0m

==========================================
Training complete!
End time: Sun Feb 15 05:22:00 AM EST 2026

Output files:
  Model checkpoint: ./rltt_output/4753520/
  Rollout logs: ./rltt_output/4753520/rollout_logs/
  wandb logs: ./rltt_output/4753520/wandb/

To sync wandb (when internet available):
  wandb sync ./rltt_output/4753520/wandb/offline-run-*

==========================================
